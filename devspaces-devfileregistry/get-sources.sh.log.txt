======= BOOTSTRAP DOCKERFILE =======>
#
# Copyright (c) 2018-2023 Red Hat, Inc.
# This program and the accompanying materials are made
# available under the terms of the Eclipse Public License 2.0
# which is available at https://www.eclipse.org/legal/epl-2.0/
#
# SPDX-License-Identifier: EPL-2.0
#
# Contributors:
#   Red Hat, Inc. - initial API and implementation
#   IBM Corporation - implementation
#

# Builder: check meta.yamls and create index.json
# https://registry.access.redhat.com/ubi8/python-38
FROM registry.access.redhat.com/ubi8/python-38:1-129 as builder
#FROM registry-proxy.engineering.redhat.com/ubi8/python-38:1 as builder
USER 0

ARG BOOTSTRAP=true
ENV BOOTSTRAP=${BOOTSTRAP}
# if not defined or string is null, allow all registries/tags in list_referenced_images
# otherwise restrict to only those space-separated registries/tags; if others found, build will fail
# useful for failing build if quay images in an RC, or wrong devspaces image tag (3.2 in 3.1 build)
ARG ALLOWED_REGISTRIES=""
ENV ALLOWED_REGISTRIES=${ALLOWED_REGISTRIES}
ARG ALLOWED_TAGS=""
ENV ALLOWED_TAGS=${ALLOWED_TAGS}

COPY ./build/dockerfiles/content_sets_rhel8.repo /etc/yum.repos.d/
COPY ./build/dockerfiles/rhel.install.sh /tmp
RUN /tmp/rhel.install.sh && rm -f /tmp/rhel.install.sh

COPY ./build/scripts ./versions.json /build/
COPY ./build/scripts/clone_and_zip.sh /build/build/scripts/
COPY ./VERSION /
COPY ./devfiles /build/devfiles
WORKDIR /build/

RUN ./generate_devworkspace_templates.sh
RUN chmod -R g+rwX /build/resources

# validate devfile content
RUN ./check_referenced_images.sh devfiles --registries "${ALLOWED_REGISTRIES}" --tags "${ALLOWED_TAGS}"
RUN ./check_mandatory_fields.sh devfiles

# Cache projects in DS 
COPY ./build/dockerfiles/rhel.cache_projects.sh /tmp/ 
RUN /tmp/rhel.cache_projects.sh /build/ && rm -rf /tmp/rhel.cache_projects.sh /tmp/resources.tgz 

# don't do swaps, or we end up with missing content if built on s390x or ppc64le worker
# RUN ./swap_yamlfiles.sh devfiles
# RUN ./swap_images.sh devfiles
RUN ./index.sh > /build/devfiles/index.json && \
    ./list_referenced_images.sh devfiles > /build/devfiles/external_images.txt && \
    ./list_referenced_images_by_file.sh devfiles > /build/devfiles/external_images_by_devfile.txt && \
    chmod -R g+rwX /build/devfiles

<======= BOOTSTRAP DOCKERFILE =======
======= START BOOTSTRAP BUILD =======>
STEP 1/23: FROM registry.access.redhat.com/ubi8/python-38:1-129 AS builder
STEP 2/23: USER 0
--> 3a8fd864b93
STEP 3/23: ARG BOOTSTRAP=true
--> b8986c2fb8c
STEP 4/23: ENV BOOTSTRAP=${BOOTSTRAP}
--> 62a5670850b
STEP 5/23: ARG ALLOWED_REGISTRIES=""
--> 77df0f7fc36
STEP 6/23: ENV ALLOWED_REGISTRIES=${ALLOWED_REGISTRIES}
--> 1d766367ca1
STEP 7/23: ARG ALLOWED_TAGS=""
--> 182e126974e
STEP 8/23: ENV ALLOWED_TAGS=${ALLOWED_TAGS}
--> 07a3b9a39c4
STEP 9/23: COPY ./build/dockerfiles/content_sets_rhel8.repo /etc/yum.repos.d/
--> 84aad0a854c
STEP 10/23: COPY ./build/dockerfiles/rhel.install.sh /tmp
--> bd75cfad355
STEP 11/23: RUN /tmp/rhel.install.sh && rm -f /tmp/rhel.install.sh

Installed:
  containers-common-2:1-64.module+el8.8.0+18571+eed59fc4.x86_64                 
  criu-3.15-4.module+el8.8.0+19044+f9982fd8.x86_64                              
  fuse-common-3.3.0-16.el8.x86_64                                               
  fuse-overlayfs-1.11-1.module+el8.8.0+18634+9a268292.x86_64                    
  fuse3-3.3.0-16.el8.x86_64                                                     
  fuse3-libs-3.3.0-16.el8.x86_64                                                
  iptables-libs-1.8.4-24.el8.x86_64                                             
  jansson-2.14-1.el8.x86_64                                                     
  jq-1.6-6.el8.x86_64                                                           
  kmod-25-19.el8.x86_64                                                         
  libibverbs-44.0-2.el8.1.x86_64                                                
  libmnl-1.0.4-6.el8.x86_64                                                     
  libnet-1.1.6-15.el8.x86_64                                                    
  libnftnl-1.1.5-5.el8.x86_64                                                   
  libpcap-14:1.9.1-5.el8.x86_64                                                 
  libslirp-4.4.0-1.module+el8.8.0+18060+3f21f2cc.x86_64                         
  mpdecimal-2.5.1-3.el8.x86_64                                                  
  nftables-1:0.9.3-26.el8.x86_64                                                
  oniguruma-6.8.2-2.el8.x86_64                                                  
  protobuf-c-1.3.0-6.el8.x86_64                                                 
  python3.11-3.11.2-2.el8_8.1.x86_64                                            
  python3.11-devel-3.11.2-2.el8_8.1.x86_64                                      
  python3.11-libs-3.11.2-2.el8_8.1.x86_64                                       
  python3.11-pip-22.3.1-2.el8.noarch                                            
  python3.11-pip-wheel-22.3.1-2.el8.noarch                                      
  python3.11-setuptools-65.5.1-2.el8.noarch                                     
  python3.11-setuptools-wheel-65.5.1-2.el8.noarch                               
  runc-1:1.1.4-1.module+el8.8.0+18060+3f21f2cc.x86_64                           
  skopeo-2:1.11.2-0.2.module+el8.8.0+18251+ad5b274c.x86_64                      
  slirp4netns-1.2.0-2.module+el8.8.0+18060+3f21f2cc.x86_64                      

Collecting yq
  Downloading yq-3.2.2-py3-none-any.whl (17 kB)
Collecting argcomplete
  Downloading argcomplete-3.1.1-py3-none-any.whl (41 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.5/41.5 kB 4.1 MB/s eta 0:00:00
Requirement already satisfied: pip in /usr/lib/python3.11/site-packages (22.3.1)
Collecting pip
  Downloading pip-23.2-py3-none-any.whl (2.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 49.3 MB/s eta 0:00:00
Collecting PyYAML>=5.3.1
  Downloading PyYAML-6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 757.9/757.9 kB 357.7 MB/s eta 0:00:00
Collecting xmltodict>=0.11.0
  Downloading xmltodict-0.13.0-py2.py3-none-any.whl (10.0 kB)
Collecting tomlkit>=0.11.6
  Downloading tomlkit-0.11.8-py3-none-any.whl (35 kB)
Installing collected packages: xmltodict, tomlkit, PyYAML, pip, argcomplete, yq
Successfully installed PyYAML-6.0 argcomplete-3.1.1 pip-23.2 tomlkit-0.11.8 xmltodict-0.13.0 yq-3.2.2
python: Python 3.8.16
yq: yq 3.2.2
jq: jq-1.6
--> a38db468654
STEP 12/23: COPY ./build/scripts ./versions.json /build/
--> a0384dbf96f
STEP 13/23: COPY ./build/scripts/clone_and_zip.sh /build/build/scripts/
--> f3c07c3e218
STEP 14/23: COPY ./VERSION /
--> 67329521057
STEP 15/23: COPY ./devfiles /build/devfiles
--> 61bb38c17cf
STEP 16/23: WORKDIR /build/
--> 574f4409c80
STEP 17/23: RUN ./generate_devworkspace_templates.sh

> core-js@2.6.12 postinstall /build/node_modules/core-js
> node -e "try{require('./postinstall')}catch(e){}"

[96mThank you for using core-js ([94m https://github.com/zloirock/core-js [96m) for polyfilling JavaScript standard library![0m

[96mThe project needs your help! Please consider supporting of core-js on Open Collective or Patreon: [0m
[96m>[94m https://opencollective.com/core-js [0m
[96m>[94m https://www.patreon.com/zloirock [0m

[96mAlso, the author of core-js ([94m https://github.com/zloirock [96m) is looking for a good job -)[0m

+ @eclipse-che/che-devworkspace-generator@0.0.1-99986b8
added 119 packages from 183 contributors and audited 119 packages in 9.074s

5 packages are looking for funding
  run `npm fund` for details

found 3 vulnerabilities (2 moderate, 1 high)
  run `npm audit fix` to fix them, or `npm audit` for details
DevWorkspace che-code-ansible-demo was generated.
DevWorkspace che-code-ansible-demo was generated.
DevWorkspace che-idea-ansible-demo was generated.
DevWorkspace che-code-java-lombok was generated.
DevWorkspace che-code-java-lombok was generated.
DevWorkspace che-idea-java-lombok was generated.
DevWorkspace che-code-quarkus-quickstart was generated.
DevWorkspace che-code-quarkus-quickstart was generated.
DevWorkspace che-idea-quarkus-quickstart was generated.
DevWorkspace che-code-nodejs-mongodb was generated.
DevWorkspace che-code-nodejs-mongodb was generated.
DevWorkspace che-idea-nodejs-mongodb was generated.
DevWorkspace che-code-nodejs-web-app was generated.
DevWorkspace che-code-nodejs-web-app was generated.
DevWorkspace che-idea-nodejs-web-app was generated.
DevWorkspace che-code-python-hello-world was generated.
DevWorkspace che-code-python-hello-world was generated.
DevWorkspace che-idea-python-hello-world was generated.
DevWorkspace che-code-cpp was generated.
DevWorkspace che-code-cpp was generated.
DevWorkspace che-idea-cpp was generated.
DevWorkspace che-code-dotnet was generated.
DevWorkspace che-code-dotnet was generated.
DevWorkspace che-idea-dotnet was generated.
DevWorkspace che-code-golang was generated.
DevWorkspace che-code-golang was generated.
DevWorkspace che-idea-golang was generated.
DevWorkspace che-code-php-hello-world was generated.
DevWorkspace che-code-php-hello-world was generated.
DevWorkspace che-idea-php-hello-world was generated.
--> a48ad889a41
STEP 18/23: RUN chmod -R g+rwX /build/resources
--> 9865cb151bf
STEP 19/23: RUN ./check_referenced_images.sh devfiles --registries "${ALLOWED_REGISTRIES}" --tags "${ALLOWED_TAGS}"
 = quay.io/devspaces/ansible-creator-ee@sha256:3ff5d2d5f17c9c1e4a352d9922e27be09641647ac028a56845aaab6f6e3c7958 PASS
 + registry.redhat.io/devspaces/code-rhel8:3.8 PASS - registry.redhat.io allowed
 + registry.redhat.io/devspaces/idea-rhel8:3.8 PASS - registry.redhat.io allowed
 + registry.redhat.io/devspaces/udi-rhel8:3.8 PASS - registry.redhat.io allowed
 + registry.redhat.io/rhscl/mongodb-36-rhel7:1-50 PASS - registry.redhat.io allowed
 = quay.io/devspaces/ansible-creator-ee@sha256:3ff5d2d5f17c9c1e4a352d9922e27be09641647ac028a56845aaab6f6e3c7958 PASS
 + registry.redhat.io/devspaces/code-rhel8:3.8 PASS - 3.8 allowed
 + registry.redhat.io/devspaces/idea-rhel8:3.8 PASS - 3.8 allowed
 + registry.redhat.io/devspaces/udi-rhel8:3.8 PASS - 3.8 allowed
 = registry.redhat.io/rhscl/mongodb-36-rhel7:1-50 PASS
--> 2d71fa76da3
STEP 20/23: RUN ./check_mandatory_fields.sh devfiles
Checking devfile 'devfiles/TP__cpp__c-plus-plus/meta.yaml'
Checking devfile 'devfiles/TP__dotnet__dotnet-web-simple/meta.yaml'
Checking devfile 'devfiles/TP__go__golang-health-check/meta.yaml'
Checking devfile 'devfiles/TP__php__php-hello-world/meta.yaml'
Checking devfile 'devfiles/ansible__ansible-demo/meta.yaml'
Checking devfile 'devfiles/java11-maven-lombok__lombok-project-sample/meta.yaml'
Checking devfile 'devfiles/java11-maven-quarkus__quarkus-quickstarts/meta.yaml'
Checking devfile 'devfiles/nodejs__nodejs-mongodb-sample/meta.yaml'
Checking devfile 'devfiles/nodejs__web-nodejs-sample/meta.yaml'
Checking devfile 'devfiles/python__python-hello-world/meta.yaml'
--> 1fdbd620353
STEP 21/23: COPY ./build/dockerfiles/rhel.cache_projects.sh /tmp/ 
--> 79165f4ceed
STEP 22/23: RUN /tmp/rhel.cache_projects.sh /build/ && rm -rf /tmp/rhel.cache_projects.sh /tmp/resources.tgz 
--> 7829e229f05
STEP 23/23: RUN ./index.sh > /build/devfiles/index.json &&     ./list_referenced_images.sh devfiles > /build/devfiles/external_images.txt &&     ./list_referenced_images_by_file.sh devfiles > /build/devfiles/external_images_by_devfile.txt &&     chmod -R g+rwX /build/devfiles
COMMIT devfileregistry:tmp
--> 39402735533
Successfully tagged localhost/devfileregistry:tmp
39402735533d6ad88247f5d40cfee30ecf08226c3e4ac78937a4907dce5da53b
<======= END BOOTSTRAP BUILD =======
Downloading root-local.tgz
Downloading resources.tgz
DIFF START *****
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/__init__.py	2023-07-15 11:52:27.582474434 -0400
@@ -1,6 +1,6 @@
 from typing import List, Optional
 
-__version__ = "23.1.2"
+__version__ = "23.2"
 
 
 def main(args: Optional[List[str]] = None) -> int:
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/cache.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/cache.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/cache.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/cache.py	2023-07-15 11:52:27.549474434 -0400
@@ -194,7 +194,17 @@
         self.origin: Optional[DirectUrl] = None
         origin_direct_url_path = Path(self.link.file_path).parent / ORIGIN_JSON_NAME
         if origin_direct_url_path.exists():
-            self.origin = DirectUrl.from_json(origin_direct_url_path.read_text())
+            try:
+                self.origin = DirectUrl.from_json(
+                    origin_direct_url_path.read_text(encoding="utf-8")
+                )
+            except Exception as e:
+                logger.warning(
+                    "Ignoring invalid cache entry origin file %s for %s (%s)",
+                    origin_direct_url_path,
+                    link.filename,
+                    e,
+                )
 
 
 class WheelCache(Cache):
@@ -257,16 +267,26 @@
     @staticmethod
     def record_download_origin(cache_dir: str, download_info: DirectUrl) -> None:
         origin_path = Path(cache_dir) / ORIGIN_JSON_NAME
-        if origin_path.is_file():
-            origin = DirectUrl.from_json(origin_path.read_text())
-            # TODO: use DirectUrl.equivalent when https://github.com/pypa/pip/pull/10564
-            # is merged.
-            if origin.url != download_info.url:
+        if origin_path.exists():
+            try:
+                origin = DirectUrl.from_json(origin_path.read_text(encoding="utf-8"))
+            except Exception as e:
                 logger.warning(
-                    "Origin URL %s in cache entry %s does not match download URL %s. "
-                    "This is likely a pip bug or a cache corruption issue.",
-                    origin.url,
-                    cache_dir,
-                    download_info.url,
+                    "Could not read origin file %s in cache entry (%s). "
+                    "Will attempt to overwrite it.",
+                    origin_path,
+                    e,
                 )
+            else:
+                # TODO: use DirectUrl.equivalent when
+                # https://github.com/pypa/pip/pull/10564 is merged.
+                if origin.url != download_info.url:
+                    logger.warning(
+                        "Origin URL %s in cache entry %s does not match download URL "
+                        "%s. This is likely a pip bug or a cache corruption issue. "
+                        "Will overwrite it with the new value.",
+                        origin.url,
+                        cache_dir,
+                        download_info.url,
+                    )
         origin_path.write_text(download_info.to_json(), encoding="utf-8")
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/cli/base_command.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/cli/base_command.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/cli/base_command.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/cli/base_command.py	2023-07-15 11:52:27.552474434 -0400
@@ -131,6 +131,17 @@
                 ", ".join(sorted(always_enabled_features)),
             )
 
+        # Make sure that the --python argument isn't specified after the
+        # subcommand. We can tell, because if --python was specified,
+        # we should only reach this point if we're running in the created
+        # subprocess, which has the _PIP_RUNNING_IN_SUBPROCESS environment
+        # variable set.
+        if options.python and "_PIP_RUNNING_IN_SUBPROCESS" not in os.environ:
+            logger.critical(
+                "The --python option must be placed before the pip subcommand name"
+            )
+            sys.exit(ERROR)
+
         # TODO: Try to get these passing down from the command?
         #       without resorting to os.environ to hold these.
         #       This also affects isolated builds and it should.
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/check.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/check.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/check.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/check.py	2023-07-15 11:52:27.554474434 -0400
@@ -7,6 +7,7 @@
 from pip._internal.operations.check import (
     check_package_set,
     create_package_set_from_installed,
+    warn_legacy_versions_and_specifiers,
 )
 from pip._internal.utils.misc import write_output
 
@@ -21,6 +22,7 @@
 
     def run(self, options: Values, args: List[str]) -> int:
         package_set, parsing_probs = create_package_set_from_installed()
+        warn_legacy_versions_and_specifiers(package_set)
         missing, conflicting = check_package_set(package_set)
 
         for project_name in missing:
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/completion.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/completion.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/completion.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/completion.py	2023-07-15 11:52:27.554474434 -0400
@@ -22,15 +22,10 @@
         complete -o default -F _pip_completion {prog}
     """,
     "zsh": """
-        function _pip_completion {{
-          local words cword
-          read -Ac words
-          read -cn cword
-          reply=( $( COMP_WORDS="$words[*]" \\
-                     COMP_CWORD=$(( cword-1 )) \\
-                     PIP_AUTO_COMPLETE=1 $words[1] 2>/dev/null ))
-        }}
-        compctl -K _pip_completion {prog}
+        #compdef -P pip[0-9.]#
+        compadd $( COMP_WORDS="$words[*]" \\
+                   COMP_CWORD=$((CURRENT-1)) \\
+                   PIP_AUTO_COMPLETE=1 $words[1] 2>/dev/null )
     """,
     "fish": """
         function __fish_complete_pip
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/download.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/download.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/download.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/download.py	2023-07-15 11:52:27.554474434 -0400
@@ -137,6 +137,10 @@
                 assert req.name is not None
                 preparer.save_linked_requirement(req)
                 downloaded.append(req.name)
+
+        preparer.prepare_linked_requirements_more(requirement_set.requirements.values())
+        requirement_set.warn_legacy_versions_and_specifiers()
+
         if downloaded:
             write_output("Successfully downloaded %s", " ".join(downloaded))
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/freeze.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/freeze.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/freeze.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/freeze.py	2023-07-15 11:52:27.554474434 -0400
@@ -1,6 +1,6 @@
 import sys
 from optparse import Values
-from typing import List
+from typing import AbstractSet, List
 
 from pip._internal.cli import cmdoptions
 from pip._internal.cli.base_command import Command
@@ -8,7 +8,18 @@
 from pip._internal.operations.freeze import freeze
 from pip._internal.utils.compat import stdlib_pkgs
 
-DEV_PKGS = {"pip", "setuptools", "distribute", "wheel"}
+
+def _should_suppress_build_backends() -> bool:
+    return sys.version_info < (3, 12)
+
+
+def _dev_pkgs() -> AbstractSet[str]:
+    pkgs = {"pip"}
+
+    if _should_suppress_build_backends():
+        pkgs |= {"setuptools", "distribute", "wheel"}
+
+    return pkgs
 
 
 class FreezeCommand(Command):
@@ -61,7 +72,7 @@
             action="store_true",
             help=(
                 "Do not skip these packages in the output:"
-                " {}".format(", ".join(DEV_PKGS))
+                " {}".format(", ".join(_dev_pkgs()))
             ),
         )
         self.cmd_opts.add_option(
@@ -77,7 +88,7 @@
     def run(self, options: Values, args: List[str]) -> int:
         skip = set(stdlib_pkgs)
         if not options.freeze_all:
-            skip.update(DEV_PKGS)
+            skip.update(_dev_pkgs())
 
         if options.excludes:
             skip.update(options.excludes)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/install.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/install.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/install.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/install.py	2023-07-15 11:52:27.554474434 -0400
@@ -387,6 +387,9 @@
                         json.dump(report.to_dict(), f, indent=2, ensure_ascii=False)
 
             if options.dry_run:
+                # In non dry-run mode, the legacy versions and specifiers check
+                # will be done as part of conflict detection.
+                requirement_set.warn_legacy_versions_and_specifiers()
                 would_install_items = sorted(
                     (r.metadata["name"], r.metadata["version"])
                     for r in requirement_set.requirements_to_install
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/list.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/list.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/list.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/list.py	2023-07-15 11:52:27.554474434 -0400
@@ -103,7 +103,10 @@
             dest="list_format",
             default="columns",
             choices=("columns", "freeze", "json"),
-            help="Select the output format among: columns (default), freeze, or json",
+            help=(
+                "Select the output format among: columns (default), freeze, or json. "
+                "The 'freeze' format cannot be used with the --outdated option."
+            ),
         )
 
         self.cmd_opts.add_option(
@@ -157,7 +160,7 @@
 
         if options.outdated and options.list_format == "freeze":
             raise CommandError(
-                "List format 'freeze' can not be used with the --outdated option."
+                "List format 'freeze' cannot be used with the --outdated option."
             )
 
         cmdoptions.check_list_path_option(options)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/wheel.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/wheel.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/commands/wheel.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/commands/wheel.py	2023-07-15 11:52:27.554474434 -0400
@@ -153,6 +153,9 @@
             elif should_build_for_wheel_command(req):
                 reqs_to_build.append(req)
 
+        preparer.prepare_linked_requirements_more(requirement_set.requirements.values())
+        requirement_set.warn_legacy_versions_and_specifiers()
+
         # build wheels
         build_successes, build_failures = build(
             reqs_to_build,
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/configuration.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/configuration.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/configuration.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/configuration.py	2023-07-15 11:52:27.549474434 -0400
@@ -210,8 +210,15 @@
             # Ensure directory exists.
             ensure_dir(os.path.dirname(fname))
 
-            with open(fname, "w") as f:
-                parser.write(f)
+            # Ensure directory's permission(need to be writeable)
+            try:
+                with open(fname, "w") as f:
+                    parser.write(f)
+            except OSError as error:
+                raise ConfigurationError(
+                    f"An error occurred while writing to the configuration file "
+                    f"{fname}: {error}"
+                )
 
     #
     # Private routines
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/exceptions.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/exceptions.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/exceptions.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/exceptions.py	2023-07-15 11:52:27.549474434 -0400
@@ -544,7 +544,7 @@
             # so the output can be directly copied into the requirements file.
             package = (
                 self.req.original_link
-                if self.req.original_link
+                if self.req.is_direct
                 # In case someone feeds something downright stupid
                 # to InstallRequirement's constructor.
                 else getattr(self.req, "req", None)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/metadata/importlib/_envs.py	2023-07-15 11:52:27.557474434 -0400
@@ -151,7 +151,7 @@
     deprecated(
         reason=f"Loading egg at {location} is deprecated.",
         replacement="to use pip for package installation.",
-        gone_in=None,
+        gone_in="23.3",
     )
 
 
@@ -174,7 +174,7 @@
         for location in self._paths:
             yield from finder.find(location)
             for dist in finder.find_eggs(location):
-                # _emit_egg_deprecation(dist.location)  # TODO: Enable this.
+                _emit_egg_deprecation(dist.location)
                 yield dist
             # This must go last because that's how pkg_resources tie-breaks.
             yield from finder.find_linked(location)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/models/installation_report.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/models/installation_report.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/models/installation_report.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/models/installation_report.py	2023-07-15 11:52:27.558474434 -0400
@@ -22,7 +22,7 @@
             # is_direct is true if the requirement was a direct URL reference (which
             # includes editable requirements), and false if the requirement was
             # downloaded from a PEP 503 index or --find-links.
-            "is_direct": bool(ireq.original_link),
+            "is_direct": ireq.is_direct,
             # requested is true if the requirement was specified by the user (aka
             # top level requirement), and false if it was installed as a dependency of a
             # requirement. https://peps.python.org/pep-0376/#requested
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/models/link.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/models/link.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/models/link.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/models/link.py	2023-07-15 11:52:27.558474434 -0400
@@ -70,18 +70,6 @@
         assert self.name in _SUPPORTED_HASHES
 
     @classmethod
-    def parse_pep658_hash(cls, dist_info_metadata: str) -> Optional["LinkHash"]:
-        """Parse a PEP 658 data-dist-info-metadata hash."""
-        if dist_info_metadata == "true":
-            return None
-        name, sep, value = dist_info_metadata.partition("=")
-        if not sep:
-            return None
-        if name not in _SUPPORTED_HASHES:
-            return None
-        return cls(name=name, value=value)
-
-    @classmethod
     @functools.lru_cache(maxsize=None)
     def find_hash_url_fragment(cls, url: str) -> Optional["LinkHash"]:
         """Search a string for a checksum algorithm name and encoded output value."""
@@ -107,6 +95,28 @@
         return hashes.is_hash_allowed(self.name, hex_digest=self.value)
 
 
+@dataclass(frozen=True)
+class MetadataFile:
+    """Information about a core metadata file associated with a distribution."""
+
+    hashes: Optional[Dict[str, str]]
+
+    def __post_init__(self) -> None:
+        if self.hashes is not None:
+            assert all(name in _SUPPORTED_HASHES for name in self.hashes)
+
+
+def supported_hashes(hashes: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:
+    # Remove any unsupported hash types from the mapping. If this leaves no
+    # supported hashes, return None
+    if hashes is None:
+        return None
+    hashes = {n: v for n, v in hashes.items() if n in _SUPPORTED_HASHES}
+    if not hashes:
+        return None
+    return hashes
+
+
 def _clean_url_path_part(part: str) -> str:
     """
     Clean a "part" of a URL path (i.e. after splitting on "@" characters).
@@ -179,7 +189,7 @@
         "comes_from",
         "requires_python",
         "yanked_reason",
-        "dist_info_metadata",
+        "metadata_file_data",
         "cache_link_parsing",
         "egg_fragment",
     ]
@@ -190,7 +200,7 @@
         comes_from: Optional[Union[str, "IndexContent"]] = None,
         requires_python: Optional[str] = None,
         yanked_reason: Optional[str] = None,
-        dist_info_metadata: Optional[str] = None,
+        metadata_file_data: Optional[MetadataFile] = None,
         cache_link_parsing: bool = True,
         hashes: Optional[Mapping[str, str]] = None,
     ) -> None:
@@ -208,11 +218,10 @@
             a simple repository HTML link. If the file has been yanked but
             no reason was provided, this should be the empty string. See
             PEP 592 for more information and the specification.
-        :param dist_info_metadata: the metadata attached to the file, or None if no such
-            metadata is provided. This is the value of the "data-dist-info-metadata"
-            attribute, if present, in a simple repository HTML link. This may be parsed
-            into its own `Link` by `self.metadata_link()`. See PEP 658 for more
-            information and the specification.
+        :param metadata_file_data: the metadata attached to the file, or None if
+            no such metadata is provided. This argument, if not None, indicates
+            that a separate metadata file exists, and also optionally supplies
+            hashes for that file.
         :param cache_link_parsing: A flag that is used elsewhere to determine
             whether resources retrieved from this link should be cached. PyPI
             URLs should generally have this set to False, for example.
@@ -220,6 +229,10 @@
             determine the validity of a download.
         """
 
+        # The comes_from, requires_python, and metadata_file_data arguments are
+        # only used by classmethods of this class, and are not used in client
+        # code directly.
+
         # url can be a UNC windows share
         if url.startswith("\\\\"):
             url = path_to_url(url)
@@ -239,7 +252,7 @@
         self.comes_from = comes_from
         self.requires_python = requires_python if requires_python else None
         self.yanked_reason = yanked_reason
-        self.dist_info_metadata = dist_info_metadata
+        self.metadata_file_data = metadata_file_data
 
         super().__init__(key=url, defining_class=Link)
 
@@ -262,9 +275,25 @@
         url = _ensure_quoted_url(urllib.parse.urljoin(page_url, file_url))
         pyrequire = file_data.get("requires-python")
         yanked_reason = file_data.get("yanked")
-        dist_info_metadata = file_data.get("dist-info-metadata")
         hashes = file_data.get("hashes", {})
 
+        # PEP 714: Indexes must use the name core-metadata, but
+        # clients should support the old name as a fallback for compatibility.
+        metadata_info = file_data.get("core-metadata")
+        if metadata_info is None:
+            metadata_info = file_data.get("dist-info-metadata")
+
+        # The metadata info value may be a boolean, or a dict of hashes.
+        if isinstance(metadata_info, dict):
+            # The file exists, and hashes have been supplied
+            metadata_file_data = MetadataFile(supported_hashes(metadata_info))
+        elif metadata_info:
+            # The file exists, but there are no hashes
+            metadata_file_data = MetadataFile(None)
+        else:
+            # False or not present: the file does not exist
+            metadata_file_data = None
+
         # The Link.yanked_reason expects an empty string instead of a boolean.
         if yanked_reason and not isinstance(yanked_reason, str):
             yanked_reason = ""
@@ -278,7 +307,7 @@
             requires_python=pyrequire,
             yanked_reason=yanked_reason,
             hashes=hashes,
-            dist_info_metadata=dist_info_metadata,
+            metadata_file_data=metadata_file_data,
         )
 
     @classmethod
@@ -298,14 +327,39 @@
         url = _ensure_quoted_url(urllib.parse.urljoin(base_url, href))
         pyrequire = anchor_attribs.get("data-requires-python")
         yanked_reason = anchor_attribs.get("data-yanked")
-        dist_info_metadata = anchor_attribs.get("data-dist-info-metadata")
+
+        # PEP 714: Indexes must use the name data-core-metadata, but
+        # clients should support the old name as a fallback for compatibility.
+        metadata_info = anchor_attribs.get("data-core-metadata")
+        if metadata_info is None:
+            metadata_info = anchor_attribs.get("data-dist-info-metadata")
+        # The metadata info value may be the string "true", or a string of
+        # the form "hashname=hashval"
+        if metadata_info == "true":
+            # The file exists, but there are no hashes
+            metadata_file_data = MetadataFile(None)
+        elif metadata_info is None:
+            # The file does not exist
+            metadata_file_data = None
+        else:
+            # The file exists, and hashes have been supplied
+            hashname, sep, hashval = metadata_info.partition("=")
+            if sep == "=":
+                metadata_file_data = MetadataFile(supported_hashes({hashname: hashval}))
+            else:
+                # Error - data is wrong. Treat as no hashes supplied.
+                logger.debug(
+                    "Index returned invalid data-dist-info-metadata value: %s",
+                    metadata_info,
+                )
+                metadata_file_data = MetadataFile(None)
 
         return cls(
             url,
             comes_from=page_url,
             requires_python=pyrequire,
             yanked_reason=yanked_reason,
-            dist_info_metadata=dist_info_metadata,
+            metadata_file_data=metadata_file_data,
         )
 
     def __str__(self) -> str:
@@ -407,17 +461,13 @@
         return match.group(1)
 
     def metadata_link(self) -> Optional["Link"]:
-        """Implementation of PEP 658 parsing."""
-        # Note that Link.from_element() parsing the "data-dist-info-metadata" attribute
-        # from an HTML anchor tag is typically how the Link.dist_info_metadata attribute
-        # gets set.
-        if self.dist_info_metadata is None:
+        """Return a link to the associated core metadata file (if any)."""
+        if self.metadata_file_data is None:
             return None
         metadata_url = f"{self.url_without_fragment}.metadata"
-        metadata_link_hash = LinkHash.parse_pep658_hash(self.dist_info_metadata)
-        if metadata_link_hash is None:
+        if self.metadata_file_data.hashes is None:
             return Link(metadata_url)
-        return Link(metadata_url, hashes=metadata_link_hash.as_dict())
+        return Link(metadata_url, hashes=self.metadata_file_data.hashes)
 
     def as_hashes(self) -> Hashes:
         return Hashes({k: [v] for k, v in self._hashes.items()})
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/network/auth.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/network/auth.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/network/auth.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/network/auth.py	2023-07-15 11:52:27.551474434 -0400
@@ -514,7 +514,9 @@
 
         # Consume content and release the original connection to allow our new
         #   request to reuse the same one.
-        resp.content
+        # The result of the assignment isn't used, it's just needed to consume
+        # the content.
+        _ = resp.content
         resp.raw.release_conn()
 
         # Add our new username and password to the request
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/operations/check.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/operations/check.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/operations/check.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/operations/check.py	2023-07-15 11:52:27.553474434 -0400
@@ -5,12 +5,15 @@
 from typing import Callable, Dict, List, NamedTuple, Optional, Set, Tuple
 
 from pip._vendor.packaging.requirements import Requirement
+from pip._vendor.packaging.specifiers import LegacySpecifier
 from pip._vendor.packaging.utils import NormalizedName, canonicalize_name
+from pip._vendor.packaging.version import LegacyVersion
 
 from pip._internal.distributions import make_distribution_for_install_requirement
 from pip._internal.metadata import get_default_environment
 from pip._internal.metadata.base import DistributionVersion
 from pip._internal.req.req_install import InstallRequirement
+from pip._internal.utils.deprecation import deprecated
 
 logger = logging.getLogger(__name__)
 
@@ -57,6 +60,8 @@
     package name and returns a boolean.
     """
 
+    warn_legacy_versions_and_specifiers(package_set)
+
     missing = {}
     conflicting = {}
 
@@ -147,3 +152,36 @@
                 break
 
     return packages_affected
+
+
+def warn_legacy_versions_and_specifiers(package_set: PackageSet) -> None:
+    for project_name, package_details in package_set.items():
+        if isinstance(package_details.version, LegacyVersion):
+            deprecated(
+                reason=(
+                    f"{project_name} {package_details.version} "
+                    f"has a non-standard version number."
+                ),
+                replacement=(
+                    f"to upgrade to a newer version of {project_name} "
+                    f"or contact the author to suggest that they "
+                    f"release a version with a conforming version number"
+                ),
+                issue=12063,
+                gone_in="23.3",
+            )
+        for dep in package_details.dependencies:
+            if any(isinstance(spec, LegacySpecifier) for spec in dep.specifier):
+                deprecated(
+                    reason=(
+                        f"{project_name} {package_details.version} "
+                        f"has a non-standard dependency specifier {dep}."
+                    ),
+                    replacement=(
+                        f"to upgrade to a newer version of {project_name} "
+                        f"or contact the author to suggest that they "
+                        f"release a version with a conforming dependency specifiers"
+                    ),
+                    issue=12063,
+                    gone_in="23.3",
+                )
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/operations/prepare.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/operations/prepare.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/operations/prepare.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/operations/prepare.py	2023-07-15 11:52:27.553474434 -0400
@@ -352,7 +352,7 @@
         # a surprising hash mismatch in the future.
         # file:/// URLs aren't pinnable, so don't complain about them
         # not being pinned.
-        if req.original_link is None and not req.is_pinned:
+        if not req.is_direct and not req.is_pinned:
             raise HashUnpinned()
 
         # If known-good hashes are missing for this requirement,
@@ -410,7 +410,7 @@
         #     NB: raw_name will fall back to the name from the install requirement if
         #     the Name: field is not present, but it's noted in the raw_name docstring
         #     that that should NEVER happen anyway.
-        if metadata_dist.raw_name != req.req.name:
+        if canonicalize_name(metadata_dist.raw_name) != canonicalize_name(req.req.name):
             raise MetadataInconsistent(
                 req, "Name", req.req.name, metadata_dist.raw_name
             )
@@ -471,6 +471,19 @@
             logger.debug("Downloading link %s to %s", link, filepath)
             req = links_to_fully_download[link]
             req.local_file_path = filepath
+            # TODO: This needs fixing for sdists
+            # This is an emergency fix for #11847, which reports that
+            # distributions get downloaded twice when metadata is loaded
+            # from a PEP 658 standalone metadata file. Setting _downloaded
+            # fixes this for wheels, but breaks the sdist case (tests
+            # test_download_metadata). As PyPI is currently only serving
+            # metadata for wheels, this is not an immediate issue.
+            # Fixing the problem properly looks like it will require a
+            # complete refactoring of the `prepare_linked_requirements_more`
+            # logic, and I haven't a clue where to start on that, so for now
+            # I have fixed the issue *just* for wheels.
+            if req.is_wheel:
+                self._downloaded[req.link.url] = filepath
 
         # This step is necessary to ensure all lazy wheels are processed
         # successfully by the 'download', 'wheel', and 'install' commands.
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/req/req_install.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/req/req_install.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/req/req_install.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/req/req_install.py	2023-07-15 11:52:27.556474434 -0400
@@ -104,6 +104,8 @@
             if link.is_file:
                 self.source_dir = os.path.normpath(os.path.abspath(link.file_path))
 
+        # original_link is the direct URL that was provided by the user for the
+        # requirement, either directly or via a constraints file.
         if link is None and req and req.url:
             # PEP 508 URL requirement
             link = Link(req.url)
@@ -245,6 +247,11 @@
         return self.req.specifier
 
     @property
+    def is_direct(self) -> bool:
+        """Whether this requirement was specified as a direct URL."""
+        return self.original_link is not None
+
+    @property
     def is_pinned(self) -> bool:
         """Return whether I am pinned to an exact version.
 
@@ -293,7 +300,7 @@
         good_hashes = self.hash_options.copy()
         if trust_internet:
             link = self.link
-        elif self.original_link and self.user_supplied:
+        elif self.is_direct and self.user_supplied:
             link = self.original_link
         else:
             link = None
@@ -804,7 +811,7 @@
             req_description=str(self.req),
             pycompile=pycompile,
             warn_script_location=warn_script_location,
-            direct_url=self.download_info if self.original_link else None,
+            direct_url=self.download_info if self.is_direct else None,
             requested=self.user_supplied,
         )
         self.install_succeeded = True
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/req/req_set.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/req/req_set.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/req/req_set.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/req/req_set.py	2023-07-15 11:52:27.556474434 -0400
@@ -2,9 +2,12 @@
 from collections import OrderedDict
 from typing import Dict, List
 
+from pip._vendor.packaging.specifiers import LegacySpecifier
 from pip._vendor.packaging.utils import canonicalize_name
+from pip._vendor.packaging.version import LegacyVersion
 
 from pip._internal.req.req_install import InstallRequirement
+from pip._internal.utils.deprecation import deprecated
 
 logger = logging.getLogger(__name__)
 
@@ -80,3 +83,37 @@
             for install_req in self.all_requirements
             if not install_req.constraint and not install_req.satisfied_by
         ]
+
+    def warn_legacy_versions_and_specifiers(self) -> None:
+        for req in self.requirements_to_install:
+            version = req.get_dist().version
+            if isinstance(version, LegacyVersion):
+                deprecated(
+                    reason=(
+                        f"pip has selected the non standard version {version} "
+                        f"of {req}. In the future this version will be "
+                        f"ignored as it isn't standard compliant."
+                    ),
+                    replacement=(
+                        "set or update constraints to select another version "
+                        "or contact the package author to fix the version number"
+                    ),
+                    issue=12063,
+                    gone_in="23.3",
+                )
+            for dep in req.get_dist().iter_dependencies():
+                if any(isinstance(spec, LegacySpecifier) for spec in dep.specifier):
+                    deprecated(
+                        reason=(
+                            f"pip has selected {req} {version} which has non "
+                            f"standard dependency specifier {dep}. "
+                            f"In the future this version of {req} will be "
+                            f"ignored as it isn't standard compliant."
+                        ),
+                        replacement=(
+                            "set or update constraints to select another version "
+                            "or contact the package author to fix the version number"
+                        ),
+                        issue=12063,
+                        gone_in="23.3",
+                    )
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/candidates.py	2023-07-15 11:52:27.556474434 -0400
@@ -341,6 +341,7 @@
         self.dist = dist
         self._ireq = _make_install_req_from_dist(dist, template)
         self._factory = factory
+        self._version = None
 
         # This is just logging some messages, so we can do it eagerly.
         # The returned dist would be exactly the same as self.dist because we
@@ -376,7 +377,9 @@
 
     @property
     def version(self) -> CandidateVersion:
-        return self.dist.version
+        if self._version is None:
+            self._version = self.dist.version
+        return self._version
 
     @property
     def is_editable(self) -> bool:
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/reporter.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/reporter.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/reporter.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/reporter.py	2023-07-15 11:52:27.557474434 -0400
@@ -20,7 +20,7 @@
                 "requirements. This could take a while."
             ),
             8: (
-                "pip is looking at multiple versions of {package_name} to "
+                "pip is still looking at multiple versions of {package_name} to "
                 "determine which version is compatible with other "
                 "requirements. This could take a while."
             ),
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/resolution/resolvelib/resolver.py	2023-07-15 11:52:27.557474434 -0400
@@ -159,6 +159,9 @@
 
         reqs = req_set.all_requirements
         self.factory.preparer.prepare_linked_requirements_more(reqs)
+        for req in reqs:
+            req.prepared = True
+            req.needs_more_preparation = False
         return req_set
 
     def get_installation_order(
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/utils/glibc.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/utils/glibc.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/utils/glibc.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/utils/glibc.py	2023-07-15 11:52:27.550474434 -0400
@@ -1,6 +1,3 @@
-# The following comment should be removed at some point in the future.
-# mypy: strict-optional=False
-
 import os
 import sys
 from typing import Optional, Tuple
@@ -20,8 +17,11 @@
     if sys.platform == "win32":
         return None
     try:
+        gnu_libc_version = os.confstr("CS_GNU_LIBC_VERSION")
+        if gnu_libc_version is None:
+            return None
         # os.confstr("CS_GNU_LIBC_VERSION") returns a string like "glibc 2.17":
-        _, version = os.confstr("CS_GNU_LIBC_VERSION").split()
+        _, version = gnu_libc_version.split()
     except (AttributeError, OSError, ValueError):
         # os.confstr() or CS_GNU_LIBC_VERSION not available (or a bad value)...
         return None
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/utils/misc.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/utils/misc.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/utils/misc.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/utils/misc.py	2023-07-15 11:52:27.550474434 -0400
@@ -127,10 +127,15 @@
 # Tenacity raises RetryError by default, explicitly raise the original exception
 @retry(reraise=True, stop=stop_after_delay(3), wait=wait_fixed(0.5))
 def rmtree(dir: str, ignore_errors: bool = False) -> None:
-    shutil.rmtree(dir, ignore_errors=ignore_errors, onerror=rmtree_errorhandler)
+    if sys.version_info >= (3, 12):
+        shutil.rmtree(dir, ignore_errors=ignore_errors, onexc=rmtree_errorhandler)
+    else:
+        shutil.rmtree(dir, ignore_errors=ignore_errors, onerror=rmtree_errorhandler)
 
 
-def rmtree_errorhandler(func: Callable[..., Any], path: str, exc_info: ExcInfo) -> None:
+def rmtree_errorhandler(
+    func: Callable[..., Any], path: str, exc_info: Union[ExcInfo, BaseException]
+) -> None:
     """On Windows, the files in .svn are read-only, so when rmtree() tries to
     remove them, an exception is thrown.  We catch that here, remove the
     read-only attribute, and hopefully continue without problems."""
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/vcs/mercurial.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/vcs/mercurial.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_internal/vcs/mercurial.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_internal/vcs/mercurial.py	2023-07-15 11:52:27.552474434 -0400
@@ -31,7 +31,7 @@
 
     @staticmethod
     def get_base_rev_args(rev: str) -> List[str]:
-        return [rev]
+        return ["-r", rev]
 
     def fetch_new(
         self, dest: str, url: HiddenText, rev_options: RevOptions, verbosity: int
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/__main__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/__main__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/__main__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/__main__.py	2023-07-15 11:52:27.582474434 -0400
@@ -1,6 +1,5 @@
 import os
 import sys
-import warnings
 
 # Remove '' and current working directory from the first entry
 # of sys.path, if present to avoid using current directory
@@ -20,12 +19,6 @@
     sys.path.insert(0, path)
 
 if __name__ == "__main__":
-    # Work around the error reported in #9540, pending a proper fix.
-    # Note: It is essential the warning filter is set *before* importing
-    #       pip, as the deprecation happens at import time, not runtime.
-    warnings.filterwarnings(
-        "ignore", category=DeprecationWarning, module=".*packaging\\.version"
-    )
     from pip._internal.cli.main import main as _main
 
     sys.exit(_main())
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/certifi/cacert.pem /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/certifi/cacert.pem
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/certifi/cacert.pem	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/certifi/cacert.pem	2023-07-15 11:52:27.573474434 -0400
@@ -4525,3 +4525,65 @@
 9zuxNuie9sRGKEkz0FhDKmMpzE2xtHqiuQ04pV1IKv3LsnNdo4gIxwwCMQDAqy0O
 be0YottT6SXbVQjgUMzfRGEWgqtJsLKB7HOHeLRMsmIbEvoWTSVLY70eN9k=
 -----END CERTIFICATE-----
+
+# Issuer: CN=BJCA Global Root CA1 O=BEIJING CERTIFICATE AUTHORITY
+# Subject: CN=BJCA Global Root CA1 O=BEIJING CERTIFICATE AUTHORITY
+# Label: "BJCA Global Root CA1"
+# Serial: 113562791157148395269083148143378328608
+# MD5 Fingerprint: 42:32:99:76:43:33:36:24:35:07:82:9b:28:f9:d0:90
+# SHA1 Fingerprint: d5:ec:8d:7b:4c:ba:79:f4:e7:e8:cb:9d:6b:ae:77:83:10:03:21:6a
+# SHA256 Fingerprint: f3:89:6f:88:fe:7c:0a:88:27:66:a7:fa:6a:d2:74:9f:b5:7a:7f:3e:98:fb:76:9c:1f:a7:b0:9c:2c:44:d5:ae
+-----BEGIN CERTIFICATE-----
+MIIFdDCCA1ygAwIBAgIQVW9l47TZkGobCdFsPsBsIDANBgkqhkiG9w0BAQsFADBU
+MQswCQYDVQQGEwJDTjEmMCQGA1UECgwdQkVJSklORyBDRVJUSUZJQ0FURSBBVVRI
+T1JJVFkxHTAbBgNVBAMMFEJKQ0EgR2xvYmFsIFJvb3QgQ0ExMB4XDTE5MTIxOTAz
+MTYxN1oXDTQ0MTIxMjAzMTYxN1owVDELMAkGA1UEBhMCQ04xJjAkBgNVBAoMHUJF
+SUpJTkcgQ0VSVElGSUNBVEUgQVVUSE9SSVRZMR0wGwYDVQQDDBRCSkNBIEdsb2Jh
+bCBSb290IENBMTCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAPFmCL3Z
+xRVhy4QEQaVpN3cdwbB7+sN3SJATcmTRuHyQNZ0YeYjjlwE8R4HyDqKYDZ4/N+AZ
+spDyRhySsTphzvq3Rp4Dhtczbu33RYx2N95ulpH3134rhxfVizXuhJFyV9xgw8O5
+58dnJCNPYwpj9mZ9S1WnP3hkSWkSl+BMDdMJoDIwOvqfwPKcxRIqLhy1BDPapDgR
+at7GGPZHOiJBhyL8xIkoVNiMpTAK+BcWyqw3/XmnkRd4OJmtWO2y3syJfQOcs4ll
+5+M7sSKGjwZteAf9kRJ/sGsciQ35uMt0WwfCyPQ10WRjeulumijWML3mG90Vr4Tq
+nMfK9Q7q8l0ph49pczm+LiRvRSGsxdRpJQaDrXpIhRMsDQa4bHlW/KNnMoH1V6XK
+V0Jp6VwkYe/iMBhORJhVb3rCk9gZtt58R4oRTklH2yiUAguUSiz5EtBP6DF+bHq/
+pj+bOT0CFqMYs2esWz8sgytnOYFcuX6U1WTdno9uruh8W7TXakdI136z1C2OVnZO
+z2nxbkRs1CTqjSShGL+9V/6pmTW12xB3uD1IutbB5/EjPtffhZ0nPNRAvQoMvfXn
+jSXWgXSHRtQpdaJCbPdzied9v3pKH9MiyRVVz99vfFXQpIsHETdfg6YmV6YBW37+
+WGgHqel62bno/1Afq8K0wM7o6v0PvY1NuLxxAgMBAAGjQjBAMB0GA1UdDgQWBBTF
+7+3M2I0hxkjk49cULqcWk+WYATAPBgNVHRMBAf8EBTADAQH/MA4GA1UdDwEB/wQE
+AwIBBjANBgkqhkiG9w0BAQsFAAOCAgEAUoKsITQfI/Ki2Pm4rzc2IInRNwPWaZ+4
+YRC6ojGYWUfo0Q0lHhVBDOAqVdVXUsv45Mdpox1NcQJeXyFFYEhcCY5JEMEE3Kli
+awLwQ8hOnThJdMkycFRtwUf8jrQ2ntScvd0g1lPJGKm1Vrl2i5VnZu69mP6u775u
++2D2/VnGKhs/I0qUJDAnyIm860Qkmss9vk/Ves6OF8tiwdneHg56/0OGNFK8YT88
+X7vZdrRTvJez/opMEi4r89fO4aL/3Xtw+zuhTaRjAv04l5U/BXCga99igUOLtFkN
+SoxUnMW7gZ/NfaXvCyUeOiDbHPwfmGcCCtRzRBPbUYQaVQNW4AB+dAb/OMRyHdOo
+P2gxXdMJxy6MW2Pg6Nwe0uxhHvLe5e/2mXZgLR6UcnHGCyoyx5JO1UbXHfmpGQrI
++pXObSOYqgs4rZpWDW+N8TEAiMEXnM0ZNjX+VVOg4DwzX5Ze4jLp3zO7Bkqp2IRz
+znfSxqxx4VyjHQy7Ct9f4qNx2No3WqB4K/TUfet27fJhcKVlmtOJNBir+3I+17Q9
+eVzYH6Eze9mCUAyTF6ps3MKCuwJXNq+YJyo5UOGwifUll35HaBC07HPKs5fRJNz2
+YqAo07WjuGS3iGJCz51TzZm+ZGiPTx4SSPfSKcOYKMryMguTjClPPGAyzQWWYezy
+r/6zcCwupvI=
+-----END CERTIFICATE-----
+
+# Issuer: CN=BJCA Global Root CA2 O=BEIJING CERTIFICATE AUTHORITY
+# Subject: CN=BJCA Global Root CA2 O=BEIJING CERTIFICATE AUTHORITY
+# Label: "BJCA Global Root CA2"
+# Serial: 58605626836079930195615843123109055211
+# MD5 Fingerprint: 5e:0a:f6:47:5f:a6:14:e8:11:01:95:3f:4d:01:eb:3c
+# SHA1 Fingerprint: f4:27:86:eb:6e:b8:6d:88:31:67:02:fb:ba:66:a4:53:00:aa:7a:a6
+# SHA256 Fingerprint: 57:4d:f6:93:1e:27:80:39:66:7b:72:0a:fd:c1:60:0f:c2:7e:b6:6d:d3:09:29:79:fb:73:85:64:87:21:28:82
+-----BEGIN CERTIFICATE-----
+MIICJTCCAaugAwIBAgIQLBcIfWQqwP6FGFkGz7RK6zAKBggqhkjOPQQDAzBUMQsw
+CQYDVQQGEwJDTjEmMCQGA1UECgwdQkVJSklORyBDRVJUSUZJQ0FURSBBVVRIT1JJ
+VFkxHTAbBgNVBAMMFEJKQ0EgR2xvYmFsIFJvb3QgQ0EyMB4XDTE5MTIxOTAzMTgy
+MVoXDTQ0MTIxMjAzMTgyMVowVDELMAkGA1UEBhMCQ04xJjAkBgNVBAoMHUJFSUpJ
+TkcgQ0VSVElGSUNBVEUgQVVUSE9SSVRZMR0wGwYDVQQDDBRCSkNBIEdsb2JhbCBS
+b290IENBMjB2MBAGByqGSM49AgEGBSuBBAAiA2IABJ3LgJGNU2e1uVCxA/jlSR9B
+IgmwUVJY1is0j8USRhTFiy8shP8sbqjV8QnjAyEUxEM9fMEsxEtqSs3ph+B99iK+
++kpRuDCK/eHeGBIK9ke35xe/J4rUQUyWPGCWwf0VHKNCMEAwHQYDVR0OBBYEFNJK
+sVF/BvDRgh9Obl+rg/xI1LCRMA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQD
+AgEGMAoGCCqGSM49BAMDA2gAMGUCMBq8W9f+qdJUDkpd0m2xQNz0Q9XSSpkZElaA
+94M04TVOSG0ED1cxMDAtsaqdAzjbBgIxAMvMh1PLet8gUXOQwKhbYdDFUDn9hf7B
+43j4ptZLvZuHjw/l1lOWqzzIQNph91Oj9w==
+-----END CERTIFICATE-----
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/certifi/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/certifi/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/certifi/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/certifi/__init__.py	2023-07-15 11:52:27.573474434 -0400
@@ -1,4 +1,4 @@
 from .core import contents, where
 
 __all__ = ["contents", "where"]
-__version__ = "2022.12.07"
+__version__ = "2023.05.07"
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pkg_resources/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pkg_resources/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pkg_resources/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pkg_resources/__init__.py	2023-07-15 11:52:27.579474434 -0400
@@ -13,11 +13,8 @@
 .zip files and with custom PEP 302 loaders that support the ``get_data()``
 method.
 
-This module is deprecated. Users are directed to
-`importlib.resources <https://docs.python.org/3/library/importlib.resources.html>`_
-and
-`importlib.metadata <https://docs.python.org/3/library/importlib.metadata.html>`_
-instead.
+This module is deprecated. Users are directed to :mod:`importlib.resources`,
+:mod:`importlib.metadata` and :pypi:`packaging` instead.
 """
 
 import sys
@@ -118,7 +115,12 @@
 _namespace_packages = None
 
 
-warnings.warn("pkg_resources is deprecated as an API", DeprecationWarning)
+warnings.warn(
+    "pkg_resources is deprecated as an API. "
+    "See https://setuptools.pypa.io/en/latest/pkg_resources.html",
+    DeprecationWarning,
+    stacklevel=2
+)
 
 
 _PEP440_FALLBACK = re.compile(r"^v?(?P<safe>(?:[0-9]+!)?[0-9]+(?:\.[0-9]+)*)", re.I)
@@ -1659,10 +1661,9 @@
 
         # for compatibility, warn; in future
         # raise ValueError(msg)
-        warnings.warn(
+        issue_warning(
             msg[:-1] + " and will raise exceptions in a future release.",
             DeprecationWarning,
-            stacklevel=4,
         )
 
     def _get(self, path):
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/android.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/android.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/android.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/android.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,3 +1,4 @@
+"""Android."""
 from __future__ import annotations
 
 import os
@@ -30,7 +31,8 @@
     @property
     def user_config_dir(self) -> str:
         """
-        :return: config directory tied to the user, e.g. ``/data/user/<userid>/<packagename>/shared_prefs/<AppName>``
+        :return: config directory tied to the user, e.g. \
+        ``/data/user/<userid>/<packagename>/shared_prefs/<AppName>``
         """
         return self._append_app_name_and_version(cast(str, _android_folder()), "shared_prefs")
 
@@ -62,17 +64,35 @@
         """
         path = self.user_cache_dir
         if self.opinion:
-            path = os.path.join(path, "log")
+            path = os.path.join(path, "log")  # noqa: PTH118
         return path
 
     @property
     def user_documents_dir(self) -> str:
-        """
-        :return: documents directory tied to the user e.g. ``/storage/emulated/0/Documents``
-        """
+        """:return: documents directory tied to the user e.g. ``/storage/emulated/0/Documents``"""
         return _android_documents_folder()
 
     @property
+    def user_downloads_dir(self) -> str:
+        """:return: downloads directory tied to the user e.g. ``/storage/emulated/0/Downloads``"""
+        return _android_downloads_folder()
+
+    @property
+    def user_pictures_dir(self) -> str:
+        """:return: pictures directory tied to the user e.g. ``/storage/emulated/0/Pictures``"""
+        return _android_pictures_folder()
+
+    @property
+    def user_videos_dir(self) -> str:
+        """:return: videos directory tied to the user e.g. ``/storage/emulated/0/DCIM/Camera``"""
+        return _android_videos_folder()
+
+    @property
+    def user_music_dir(self) -> str:
+        """:return: music directory tied to the user e.g. ``/storage/emulated/0/Music``"""
+        return _android_music_folder()
+
+    @property
     def user_runtime_dir(self) -> str:
         """
         :return: runtime directory tied to the user, same as `user_cache_dir` if not opinionated else ``tmp`` in it,
@@ -80,20 +100,20 @@
         """
         path = self.user_cache_dir
         if self.opinion:
-            path = os.path.join(path, "tmp")
+            path = os.path.join(path, "tmp")  # noqa: PTH118
         return path
 
 
 @lru_cache(maxsize=1)
 def _android_folder() -> str | None:
-    """:return: base folder for the Android OS or None if cannot be found"""
+    """:return: base folder for the Android OS or None if it cannot be found"""
     try:
         # First try to get path to android app via pyjnius
         from jnius import autoclass
 
-        Context = autoclass("android.content.Context")  # noqa: N806
-        result: str | None = Context.getFilesDir().getParentFile().getAbsolutePath()
-    except Exception:
+        context = autoclass("android.content.Context")
+        result: str | None = context.getFilesDir().getParentFile().getAbsolutePath()
+    except Exception:  # noqa: BLE001
         # if fails find an android folder looking path on the sys.path
         pattern = re.compile(r"/data/(data|user/\d+)/(.+)/files")
         for path in sys.path:
@@ -112,15 +132,79 @@
     try:
         from jnius import autoclass
 
-        Context = autoclass("android.content.Context")  # noqa: N806
-        Environment = autoclass("android.os.Environment")  # noqa: N806
-        documents_dir: str = Context.getExternalFilesDir(Environment.DIRECTORY_DOCUMENTS).getAbsolutePath()
-    except Exception:
+        context = autoclass("android.content.Context")
+        environment = autoclass("android.os.Environment")
+        documents_dir: str = context.getExternalFilesDir(environment.DIRECTORY_DOCUMENTS).getAbsolutePath()
+    except Exception:  # noqa: BLE001
         documents_dir = "/storage/emulated/0/Documents"
 
     return documents_dir
 
 
+@lru_cache(maxsize=1)
+def _android_downloads_folder() -> str:
+    """:return: downloads folder for the Android OS"""
+    # Get directories with pyjnius
+    try:
+        from jnius import autoclass
+
+        context = autoclass("android.content.Context")
+        environment = autoclass("android.os.Environment")
+        downloads_dir: str = context.getExternalFilesDir(environment.DIRECTORY_DOWNLOADS).getAbsolutePath()
+    except Exception:  # noqa: BLE001
+        downloads_dir = "/storage/emulated/0/Downloads"
+
+    return downloads_dir
+
+
+@lru_cache(maxsize=1)
+def _android_pictures_folder() -> str:
+    """:return: pictures folder for the Android OS"""
+    # Get directories with pyjnius
+    try:
+        from jnius import autoclass
+
+        context = autoclass("android.content.Context")
+        environment = autoclass("android.os.Environment")
+        pictures_dir: str = context.getExternalFilesDir(environment.DIRECTORY_PICTURES).getAbsolutePath()
+    except Exception:  # noqa: BLE001
+        pictures_dir = "/storage/emulated/0/Pictures"
+
+    return pictures_dir
+
+
+@lru_cache(maxsize=1)
+def _android_videos_folder() -> str:
+    """:return: videos folder for the Android OS"""
+    # Get directories with pyjnius
+    try:
+        from jnius import autoclass
+
+        context = autoclass("android.content.Context")
+        environment = autoclass("android.os.Environment")
+        videos_dir: str = context.getExternalFilesDir(environment.DIRECTORY_DCIM).getAbsolutePath()
+    except Exception:  # noqa: BLE001
+        videos_dir = "/storage/emulated/0/DCIM/Camera"
+
+    return videos_dir
+
+
+@lru_cache(maxsize=1)
+def _android_music_folder() -> str:
+    """:return: music folder for the Android OS"""
+    # Get directories with pyjnius
+    try:
+        from jnius import autoclass
+
+        context = autoclass("android.content.Context")
+        environment = autoclass("android.os.Environment")
+        music_dir: str = context.getExternalFilesDir(environment.DIRECTORY_MUSIC).getAbsolutePath()
+    except Exception:  # noqa: BLE001
+        music_dir = "/storage/emulated/0/Music"
+
+    return music_dir
+
+
 __all__ = [
     "Android",
 ]
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/api.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/api.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/api.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/api.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,29 +1,33 @@
+"""Base API."""
 from __future__ import annotations
 
 import os
-import sys
 from abc import ABC, abstractmethod
 from pathlib import Path
+from typing import TYPE_CHECKING
 
-if sys.version_info >= (3, 8):  # pragma: no branch
-    from typing import Literal  # pragma: no cover
+if TYPE_CHECKING:
+    import sys
+
+    if sys.version_info >= (3, 8):  # pragma: no cover (py38+)
+        from typing import Literal
+    else:  # pragma: no cover (py38+)
+        from pip._vendor.typing_extensions import Literal
 
 
 class PlatformDirsABC(ABC):
-    """
-    Abstract base class for platform directories.
-    """
+    """Abstract base class for platform directories."""
 
-    def __init__(
+    def __init__(  # noqa: PLR0913
         self,
         appname: str | None = None,
         appauthor: str | None | Literal[False] = None,
         version: str | None = None,
-        roaming: bool = False,
-        multipath: bool = False,
-        opinion: bool = True,
-        ensure_exists: bool = False,
-    ):
+        roaming: bool = False,  # noqa: FBT001, FBT002
+        multipath: bool = False,  # noqa: FBT001, FBT002
+        opinion: bool = True,  # noqa: FBT001, FBT002
+        ensure_exists: bool = False,  # noqa: FBT001, FBT002
+    ) -> None:
         """
         Create a new platform directory.
 
@@ -70,7 +74,7 @@
             params.append(self.appname)
             if self.version:
                 params.append(self.version)
-        path = os.path.join(base[0], *params)
+        path = os.path.join(base[0], *params)  # noqa: PTH118
         self._optionally_create_directory(path)
         return path
 
@@ -125,6 +129,26 @@
 
     @property
     @abstractmethod
+    def user_downloads_dir(self) -> str:
+        """:return: downloads directory tied to the user"""
+
+    @property
+    @abstractmethod
+    def user_pictures_dir(self) -> str:
+        """:return: pictures directory tied to the user"""
+
+    @property
+    @abstractmethod
+    def user_videos_dir(self) -> str:
+        """:return: videos directory tied to the user"""
+
+    @property
+    @abstractmethod
+    def user_music_dir(self) -> str:
+        """:return: music directory tied to the user"""
+
+    @property
+    @abstractmethod
     def user_runtime_dir(self) -> str:
         """:return: runtime directory tied to the user"""
 
@@ -174,6 +198,26 @@
         return Path(self.user_documents_dir)
 
     @property
+    def user_downloads_path(self) -> Path:
+        """:return: downloads path tied to the user"""
+        return Path(self.user_downloads_dir)
+
+    @property
+    def user_pictures_path(self) -> Path:
+        """:return: pictures path tied to the user"""
+        return Path(self.user_pictures_dir)
+
+    @property
+    def user_videos_path(self) -> Path:
+        """:return: videos path tied to the user"""
+        return Path(self.user_videos_dir)
+
+    @property
+    def user_music_path(self) -> Path:
+        """:return: music path tied to the user"""
+        return Path(self.user_music_dir)
+
+    @property
     def user_runtime_path(self) -> Path:
         """:return: runtime path tied to the user"""
         return Path(self.user_runtime_dir)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/__init__.py	2023-07-15 11:52:27.558474434 -0400
@@ -6,17 +6,20 @@
 
 import os
 import sys
-from pathlib import Path
-
-if sys.version_info >= (3, 8):  # pragma: no cover (py38+)
-    from typing import Literal
-else:  # pragma: no cover (py38+)
-    from pip._vendor.typing_extensions import Literal
+from typing import TYPE_CHECKING
 
 from .api import PlatformDirsABC
 from .version import __version__
 from .version import __version_tuple__ as __version_info__
 
+if TYPE_CHECKING:
+    from pathlib import Path
+
+    if sys.version_info >= (3, 8):  # pragma: no cover (py38+)
+        from typing import Literal
+    else:  # pragma: no cover (py38+)
+        from pip._vendor.typing_extensions import Literal
+
 
 def _set_platform_dir_class() -> type[PlatformDirsABC]:
     if sys.platform == "win32":
@@ -48,8 +51,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    roaming: bool = False,
-    ensure_exists: bool = False,
+    roaming: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -72,8 +75,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    multipath: bool = False,
-    ensure_exists: bool = False,
+    multipath: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -96,8 +99,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    roaming: bool = False,
-    ensure_exists: bool = False,
+    roaming: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -120,8 +123,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    multipath: bool = False,
-    ensure_exists: bool = False,
+    multipath: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -144,8 +147,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -168,8 +171,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -192,8 +195,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    roaming: bool = False,
-    ensure_exists: bool = False,
+    roaming: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -216,8 +219,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -237,18 +240,36 @@
 
 
 def user_documents_dir() -> str:
-    """
-    :returns: documents directory tied to the user
-    """
+    """:returns: documents directory tied to the user"""
     return PlatformDirs().user_documents_dir
 
 
+def user_downloads_dir() -> str:
+    """:returns: downloads directory tied to the user"""
+    return PlatformDirs().user_downloads_dir
+
+
+def user_pictures_dir() -> str:
+    """:returns: pictures directory tied to the user"""
+    return PlatformDirs().user_pictures_dir
+
+
+def user_videos_dir() -> str:
+    """:returns: videos directory tied to the user"""
+    return PlatformDirs().user_videos_dir
+
+
+def user_music_dir() -> str:
+    """:returns: music directory tied to the user"""
+    return PlatformDirs().user_music_dir
+
+
 def user_runtime_dir(
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> str:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -271,8 +292,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    roaming: bool = False,
-    ensure_exists: bool = False,
+    roaming: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -295,8 +316,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    multipath: bool = False,
-    ensure_exists: bool = False,
+    multipath: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -319,8 +340,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    roaming: bool = False,
-    ensure_exists: bool = False,
+    roaming: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -343,8 +364,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    multipath: bool = False,
-    ensure_exists: bool = False,
+    multipath: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -367,8 +388,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -391,8 +412,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -415,8 +436,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    roaming: bool = False,
-    ensure_exists: bool = False,
+    roaming: bool = False,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -439,8 +460,8 @@
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -460,18 +481,36 @@
 
 
 def user_documents_path() -> Path:
-    """
-    :returns: documents path tied to the user
-    """
+    """:returns: documents path tied to the user"""
     return PlatformDirs().user_documents_path
 
 
+def user_downloads_path() -> Path:
+    """:returns: downloads path tied to the user"""
+    return PlatformDirs().user_downloads_path
+
+
+def user_pictures_path() -> Path:
+    """:returns: pictures path tied to the user"""
+    return PlatformDirs().user_pictures_path
+
+
+def user_videos_path() -> Path:
+    """:returns: videos path tied to the user"""
+    return PlatformDirs().user_videos_path
+
+
+def user_music_path() -> Path:
+    """:returns: music path tied to the user"""
+    return PlatformDirs().user_music_path
+
+
 def user_runtime_path(
     appname: str | None = None,
     appauthor: str | None | Literal[False] = None,
     version: str | None = None,
-    opinion: bool = True,
-    ensure_exists: bool = False,
+    opinion: bool = True,  # noqa: FBT001, FBT002
+    ensure_exists: bool = False,  # noqa: FBT001, FBT002
 ) -> Path:
     """
     :param appname: See `appname <platformdirs.api.PlatformDirsABC.appname>`.
@@ -502,6 +541,10 @@
     "user_state_dir",
     "user_log_dir",
     "user_documents_dir",
+    "user_downloads_dir",
+    "user_pictures_dir",
+    "user_videos_dir",
+    "user_music_dir",
     "user_runtime_dir",
     "site_data_dir",
     "site_config_dir",
@@ -512,6 +555,10 @@
     "user_state_path",
     "user_log_path",
     "user_documents_path",
+    "user_downloads_path",
+    "user_pictures_path",
+    "user_videos_path",
+    "user_music_path",
     "user_runtime_path",
     "site_data_path",
     "site_config_path",
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/macos.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/macos.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/macos.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/macos.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,6 +1,7 @@
+"""macOS."""
 from __future__ import annotations
 
-import os
+import os.path
 
 from .api import PlatformDirsABC
 
@@ -17,7 +18,7 @@
     @property
     def user_data_dir(self) -> str:
         """:return: data directory tied to the user, e.g. ``~/Library/Application Support/$appname/$version``"""
-        return self._append_app_name_and_version(os.path.expanduser("~/Library/Application Support"))
+        return self._append_app_name_and_version(os.path.expanduser("~/Library/Application Support"))  # noqa: PTH111
 
     @property
     def site_data_dir(self) -> str:
@@ -37,7 +38,7 @@
     @property
     def user_cache_dir(self) -> str:
         """:return: cache directory tied to the user, e.g. ``~/Library/Caches/$appname/$version``"""
-        return self._append_app_name_and_version(os.path.expanduser("~/Library/Caches"))
+        return self._append_app_name_and_version(os.path.expanduser("~/Library/Caches"))  # noqa: PTH111
 
     @property
     def site_cache_dir(self) -> str:
@@ -52,17 +53,37 @@
     @property
     def user_log_dir(self) -> str:
         """:return: log directory tied to the user, e.g. ``~/Library/Logs/$appname/$version``"""
-        return self._append_app_name_and_version(os.path.expanduser("~/Library/Logs"))
+        return self._append_app_name_and_version(os.path.expanduser("~/Library/Logs"))  # noqa: PTH111
 
     @property
     def user_documents_dir(self) -> str:
         """:return: documents directory tied to the user, e.g. ``~/Documents``"""
-        return os.path.expanduser("~/Documents")
+        return os.path.expanduser("~/Documents")  # noqa: PTH111
+
+    @property
+    def user_downloads_dir(self) -> str:
+        """:return: downloads directory tied to the user, e.g. ``~/Downloads``"""
+        return os.path.expanduser("~/Downloads")  # noqa: PTH111
+
+    @property
+    def user_pictures_dir(self) -> str:
+        """:return: pictures directory tied to the user, e.g. ``~/Pictures``"""
+        return os.path.expanduser("~/Pictures")  # noqa: PTH111
+
+    @property
+    def user_videos_dir(self) -> str:
+        """:return: videos directory tied to the user, e.g. ``~/Movies``"""
+        return os.path.expanduser("~/Movies")  # noqa: PTH111
+
+    @property
+    def user_music_dir(self) -> str:
+        """:return: music directory tied to the user, e.g. ``~/Music``"""
+        return os.path.expanduser("~/Music")  # noqa: PTH111
 
     @property
     def user_runtime_dir(self) -> str:
         """:return: runtime directory tied to the user, e.g. ``~/Library/Caches/TemporaryItems/$appname/$version``"""
-        return self._append_app_name_and_version(os.path.expanduser("~/Library/Caches/TemporaryItems"))
+        return self._append_app_name_and_version(os.path.expanduser("~/Library/Caches/TemporaryItems"))  # noqa: PTH111
 
 
 __all__ = [
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/__main__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/__main__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/__main__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/__main__.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,3 +1,4 @@
+"""Main entry point."""
 from __future__ import annotations
 
 from pip._vendor.platformdirs import PlatformDirs, __version__
@@ -9,6 +10,10 @@
     "user_state_dir",
     "user_log_dir",
     "user_documents_dir",
+    "user_downloads_dir",
+    "user_pictures_dir",
+    "user_videos_dir",
+    "user_music_dir",
     "user_runtime_dir",
     "site_data_dir",
     "site_config_dir",
@@ -17,30 +22,31 @@
 
 
 def main() -> None:
+    """Run main entry point."""
     app_name = "MyApp"
     app_author = "MyCompany"
 
-    print(f"-- platformdirs {__version__} --")
+    print(f"-- platformdirs {__version__} --")  # noqa: T201
 
-    print("-- app dirs (with optional 'version')")
+    print("-- app dirs (with optional 'version')")  # noqa: T201
     dirs = PlatformDirs(app_name, app_author, version="1.0")
     for prop in PROPS:
-        print(f"{prop}: {getattr(dirs, prop)}")
+        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201
 
-    print("\n-- app dirs (without optional 'version')")
+    print("\n-- app dirs (without optional 'version')")  # noqa: T201
     dirs = PlatformDirs(app_name, app_author)
     for prop in PROPS:
-        print(f"{prop}: {getattr(dirs, prop)}")
+        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201
 
-    print("\n-- app dirs (without optional 'appauthor')")
+    print("\n-- app dirs (without optional 'appauthor')")  # noqa: T201
     dirs = PlatformDirs(app_name)
     for prop in PROPS:
-        print(f"{prop}: {getattr(dirs, prop)}")
+        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201
 
-    print("\n-- app dirs (with disabled 'appauthor')")
+    print("\n-- app dirs (with disabled 'appauthor')")  # noqa: T201
     dirs = PlatformDirs(app_name, appauthor=False)
     for prop in PROPS:
-        print(f"{prop}: {getattr(dirs, prop)}")
+        print(f"{prop}: {getattr(dirs, prop)}")  # noqa: T201
 
 
 if __name__ == "__main__":
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/unix.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/unix.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/unix.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/unix.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,3 +1,4 @@
+"""Unix."""
 from __future__ import annotations
 
 import os
@@ -7,12 +8,14 @@
 
 from .api import PlatformDirsABC
 
-if sys.platform.startswith("linux"):  # pragma: no branch # no op check, only to please the type checker
-    from os import getuid
-else:
+if sys.platform == "win32":
 
     def getuid() -> int:
-        raise RuntimeError("should only be used on Linux")
+        msg = "should only be used on Unix"
+        raise RuntimeError(msg)
+
+else:
+    from os import getuid
 
 
 class Unix(PlatformDirsABC):
@@ -36,7 +39,7 @@
         """
         path = os.environ.get("XDG_DATA_HOME", "")
         if not path.strip():
-            path = os.path.expanduser("~/.local/share")
+            path = os.path.expanduser("~/.local/share")  # noqa: PTH111
         return self._append_app_name_and_version(path)
 
     @property
@@ -56,7 +59,7 @@
         path_list = path.split(os.pathsep)
         if not self.multipath:
             path_list = path_list[0:1]
-        path_list = [self._append_app_name_and_version(os.path.expanduser(p)) for p in path_list]
+        path_list = [self._append_app_name_and_version(os.path.expanduser(p)) for p in path_list]  # noqa: PTH111
         return os.pathsep.join(path_list)
 
     @property
@@ -67,7 +70,7 @@
         """
         path = os.environ.get("XDG_CONFIG_HOME", "")
         if not path.strip():
-            path = os.path.expanduser("~/.config")
+            path = os.path.expanduser("~/.config")  # noqa: PTH111
         return self._append_app_name_and_version(path)
 
     @property
@@ -91,15 +94,13 @@
         """
         path = os.environ.get("XDG_CACHE_HOME", "")
         if not path.strip():
-            path = os.path.expanduser("~/.cache")
+            path = os.path.expanduser("~/.cache")  # noqa: PTH111
         return self._append_app_name_and_version(path)
 
     @property
     def site_cache_dir(self) -> str:
-        """
-        :return: cache directory shared by users, e.g. ``/var/tmp/$appname/$version``
-        """
-        return self._append_app_name_and_version("/var/tmp")
+        """:return: cache directory shared by users, e.g. ``/var/tmp/$appname/$version``"""
+        return self._append_app_name_and_version("/var/tmp")  # noqa: S108
 
     @property
     def user_state_dir(self) -> str:
@@ -109,41 +110,60 @@
         """
         path = os.environ.get("XDG_STATE_HOME", "")
         if not path.strip():
-            path = os.path.expanduser("~/.local/state")
+            path = os.path.expanduser("~/.local/state")  # noqa: PTH111
         return self._append_app_name_and_version(path)
 
     @property
     def user_log_dir(self) -> str:
-        """
-        :return: log directory tied to the user, same as `user_state_dir` if not opinionated else ``log`` in it
-        """
+        """:return: log directory tied to the user, same as `user_state_dir` if not opinionated else ``log`` in it"""
         path = self.user_state_dir
         if self.opinion:
-            path = os.path.join(path, "log")
+            path = os.path.join(path, "log")  # noqa: PTH118
         return path
 
     @property
     def user_documents_dir(self) -> str:
-        """
-        :return: documents directory tied to the user, e.g. ``~/Documents``
-        """
-        documents_dir = _get_user_dirs_folder("XDG_DOCUMENTS_DIR")
-        if documents_dir is None:
-            documents_dir = os.environ.get("XDG_DOCUMENTS_DIR", "").strip()
-            if not documents_dir:
-                documents_dir = os.path.expanduser("~/Documents")
+        """:return: documents directory tied to the user, e.g. ``~/Documents``"""
+        return _get_user_media_dir("XDG_DOCUMENTS_DIR", "~/Documents")
 
-        return documents_dir
+    @property
+    def user_downloads_dir(self) -> str:
+        """:return: downloads directory tied to the user, e.g. ``~/Downloads``"""
+        return _get_user_media_dir("XDG_DOWNLOAD_DIR", "~/Downloads")
+
+    @property
+    def user_pictures_dir(self) -> str:
+        """:return: pictures directory tied to the user, e.g. ``~/Pictures``"""
+        return _get_user_media_dir("XDG_PICTURES_DIR", "~/Pictures")
+
+    @property
+    def user_videos_dir(self) -> str:
+        """:return: videos directory tied to the user, e.g. ``~/Videos``"""
+        return _get_user_media_dir("XDG_VIDEOS_DIR", "~/Videos")
+
+    @property
+    def user_music_dir(self) -> str:
+        """:return: music directory tied to the user, e.g. ``~/Music``"""
+        return _get_user_media_dir("XDG_MUSIC_DIR", "~/Music")
 
     @property
     def user_runtime_dir(self) -> str:
         """
         :return: runtime directory tied to the user, e.g. ``/run/user/$(id -u)/$appname/$version`` or
-         ``$XDG_RUNTIME_DIR/$appname/$version``
+         ``$XDG_RUNTIME_DIR/$appname/$version``.
+
+         For FreeBSD/OpenBSD/NetBSD, it would return ``/var/run/user/$(id -u)/$appname/$version`` if
+         exists, otherwise ``/tmp/runtime-$(id -u)/$appname/$version``, if``$XDG_RUNTIME_DIR``
+         is not set.
         """
         path = os.environ.get("XDG_RUNTIME_DIR", "")
         if not path.strip():
-            path = f"/run/user/{getuid()}"
+            if sys.platform.startswith(("freebsd", "openbsd", "netbsd")):
+                path = f"/var/run/user/{getuid()}"
+                if not Path(path).exists():
+                    path = f"/tmp/runtime-{getuid()}"  # noqa: S108
+            else:
+                path = f"/run/user/{getuid()}"
         return self._append_app_name_and_version(path)
 
     @property
@@ -168,13 +188,23 @@
         return Path(directory)
 
 
+def _get_user_media_dir(env_var: str, fallback_tilde_path: str) -> str:
+    media_dir = _get_user_dirs_folder(env_var)
+    if media_dir is None:
+        media_dir = os.environ.get(env_var, "").strip()
+        if not media_dir:
+            media_dir = os.path.expanduser(fallback_tilde_path)  # noqa: PTH111
+
+    return media_dir
+
+
 def _get_user_dirs_folder(key: str) -> str | None:
-    """Return directory from user-dirs.dirs config file. See https://freedesktop.org/wiki/Software/xdg-user-dirs/"""
-    user_dirs_config_path = os.path.join(Unix().user_config_dir, "user-dirs.dirs")
-    if os.path.exists(user_dirs_config_path):
+    """Return directory from user-dirs.dirs config file. See https://freedesktop.org/wiki/Software/xdg-user-dirs/."""
+    user_dirs_config_path = Path(Unix().user_config_dir) / "user-dirs.dirs"
+    if user_dirs_config_path.exists():
         parser = ConfigParser()
 
-        with open(user_dirs_config_path) as stream:
+        with user_dirs_config_path.open() as stream:
             # Add fake section header, so ConfigParser doesn't complain
             parser.read_string(f"[top]\n{stream.read()}")
 
@@ -183,8 +213,7 @@
 
         path = parser["top"][key].strip('"')
         # Handle relative home paths
-        path = path.replace("$HOME", os.path.expanduser("~"))
-        return path
+        return path.replace("$HOME", os.path.expanduser("~"))  # noqa: PTH111
 
     return None
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/version.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/version.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/version.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/version.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,4 +1,4 @@
 # file generated by setuptools_scm
 # don't change, don't track in version control
-__version__ = version = '3.2.0'
-__version_tuple__ = version_tuple = (3, 2, 0)
+__version__ = version = '3.8.1'
+__version_tuple__ = version_tuple = (3, 8, 1)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/windows.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/windows.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/platformdirs/windows.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/platformdirs/windows.py	2023-07-15 11:52:27.558474434 -0400
@@ -1,16 +1,21 @@
+"""Windows."""
 from __future__ import annotations
 
 import ctypes
 import os
 import sys
 from functools import lru_cache
-from typing import Callable
+from typing import TYPE_CHECKING
 
 from .api import PlatformDirsABC
 
+if TYPE_CHECKING:
+    from collections.abc import Callable
+
 
 class Windows(PlatformDirsABC):
-    """`MSDN on where to store app data files
+    """
+    `MSDN on where to store app data files
     <http://support.microsoft.com/default.aspx?scid=kb;en-us;310294#XSLTH3194121123120121120120>`_.
     Makes use of the
     `appname <platformdirs.api.PlatformDirsABC.appname>`,
@@ -43,7 +48,7 @@
                 params.append(opinion_value)
             if self.version:
                 params.append(self.version)
-        path = os.path.join(path, *params)
+        path = os.path.join(path, *params)  # noqa: PTH118
         self._optionally_create_directory(path)
         return path
 
@@ -85,36 +90,53 @@
 
     @property
     def user_log_dir(self) -> str:
-        """
-        :return: log directory tied to the user, same as `user_data_dir` if not opinionated else ``Logs`` in it
-        """
+        """:return: log directory tied to the user, same as `user_data_dir` if not opinionated else ``Logs`` in it"""
         path = self.user_data_dir
         if self.opinion:
-            path = os.path.join(path, "Logs")
+            path = os.path.join(path, "Logs")  # noqa: PTH118
             self._optionally_create_directory(path)
         return path
 
     @property
     def user_documents_dir(self) -> str:
-        """
-        :return: documents directory tied to the user e.g. ``%USERPROFILE%\\Documents``
-        """
+        """:return: documents directory tied to the user e.g. ``%USERPROFILE%\\Documents``"""
         return os.path.normpath(get_win_folder("CSIDL_PERSONAL"))
 
     @property
+    def user_downloads_dir(self) -> str:
+        """:return: downloads directory tied to the user e.g. ``%USERPROFILE%\\Downloads``"""
+        return os.path.normpath(get_win_folder("CSIDL_DOWNLOADS"))
+
+    @property
+    def user_pictures_dir(self) -> str:
+        """:return: pictures directory tied to the user e.g. ``%USERPROFILE%\\Pictures``"""
+        return os.path.normpath(get_win_folder("CSIDL_MYPICTURES"))
+
+    @property
+    def user_videos_dir(self) -> str:
+        """:return: videos directory tied to the user e.g. ``%USERPROFILE%\\Videos``"""
+        return os.path.normpath(get_win_folder("CSIDL_MYVIDEO"))
+
+    @property
+    def user_music_dir(self) -> str:
+        """:return: music directory tied to the user e.g. ``%USERPROFILE%\\Music``"""
+        return os.path.normpath(get_win_folder("CSIDL_MYMUSIC"))
+
+    @property
     def user_runtime_dir(self) -> str:
         """
         :return: runtime directory tied to the user, e.g.
          ``%USERPROFILE%\\AppData\\Local\\Temp\\$appauthor\\$appname``
         """
-        path = os.path.normpath(os.path.join(get_win_folder("CSIDL_LOCAL_APPDATA"), "Temp"))
+        path = os.path.normpath(os.path.join(get_win_folder("CSIDL_LOCAL_APPDATA"), "Temp"))  # noqa: PTH118
         return self._append_parts(path)
 
 
 def get_win_folder_from_env_vars(csidl_name: str) -> str:
     """Get folder from environment variables."""
-    if csidl_name == "CSIDL_PERSONAL":  # does not have an environment name
-        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Documents")
+    result = get_win_folder_if_csidl_name_not_env_var(csidl_name)
+    if result is not None:
+        return result
 
     env_var_name = {
         "CSIDL_APPDATA": "APPDATA",
@@ -122,28 +144,54 @@
         "CSIDL_LOCAL_APPDATA": "LOCALAPPDATA",
     }.get(csidl_name)
     if env_var_name is None:
-        raise ValueError(f"Unknown CSIDL name: {csidl_name}")
+        msg = f"Unknown CSIDL name: {csidl_name}"
+        raise ValueError(msg)
     result = os.environ.get(env_var_name)
     if result is None:
-        raise ValueError(f"Unset environment variable: {env_var_name}")
+        msg = f"Unset environment variable: {env_var_name}"
+        raise ValueError(msg)
     return result
 
 
+def get_win_folder_if_csidl_name_not_env_var(csidl_name: str) -> str | None:
+    """Get folder for a CSIDL name that does not exist as an environment variable."""
+    if csidl_name == "CSIDL_PERSONAL":
+        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Documents")  # noqa: PTH118
+
+    if csidl_name == "CSIDL_DOWNLOADS":
+        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Downloads")  # noqa: PTH118
+
+    if csidl_name == "CSIDL_MYPICTURES":
+        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Pictures")  # noqa: PTH118
+
+    if csidl_name == "CSIDL_MYVIDEO":
+        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Videos")  # noqa: PTH118
+
+    if csidl_name == "CSIDL_MYMUSIC":
+        return os.path.join(os.path.normpath(os.environ["USERPROFILE"]), "Music")  # noqa: PTH118
+    return None
+
+
 def get_win_folder_from_registry(csidl_name: str) -> str:
-    """Get folder from the registry.
+    """
+    Get folder from the registry.
 
-    This is a fallback technique at best. I'm not sure if using the
-    registry for this guarantees us the correct answer for all CSIDL_*
-    names.
+    This is a fallback technique at best. I'm not sure if using the registry for these guarantees us the correct answer
+    for all CSIDL_* names.
     """
     shell_folder_name = {
         "CSIDL_APPDATA": "AppData",
         "CSIDL_COMMON_APPDATA": "Common AppData",
         "CSIDL_LOCAL_APPDATA": "Local AppData",
         "CSIDL_PERSONAL": "Personal",
+        "CSIDL_DOWNLOADS": "{374DE290-123F-4565-9164-39C4925E467B}",
+        "CSIDL_MYPICTURES": "My Pictures",
+        "CSIDL_MYVIDEO": "My Video",
+        "CSIDL_MYMUSIC": "My Music",
     }.get(csidl_name)
     if shell_folder_name is None:
-        raise ValueError(f"Unknown CSIDL name: {csidl_name}")
+        msg = f"Unknown CSIDL name: {csidl_name}"
+        raise ValueError(msg)
     if sys.platform != "win32":  # only needed for mypy type checker to know that this code runs only on Windows
         raise NotImplementedError
     import winreg
@@ -155,25 +203,37 @@
 
 def get_win_folder_via_ctypes(csidl_name: str) -> str:
     """Get folder with ctypes."""
+    # There is no 'CSIDL_DOWNLOADS'.
+    # Use 'CSIDL_PROFILE' (40) and append the default folder 'Downloads' instead.
+    # https://learn.microsoft.com/en-us/windows/win32/shell/knownfolderid
+
     csidl_const = {
         "CSIDL_APPDATA": 26,
         "CSIDL_COMMON_APPDATA": 35,
         "CSIDL_LOCAL_APPDATA": 28,
         "CSIDL_PERSONAL": 5,
+        "CSIDL_MYPICTURES": 39,
+        "CSIDL_MYVIDEO": 14,
+        "CSIDL_MYMUSIC": 13,
+        "CSIDL_DOWNLOADS": 40,
     }.get(csidl_name)
     if csidl_const is None:
-        raise ValueError(f"Unknown CSIDL name: {csidl_name}")
+        msg = f"Unknown CSIDL name: {csidl_name}"
+        raise ValueError(msg)
 
     buf = ctypes.create_unicode_buffer(1024)
     windll = getattr(ctypes, "windll")  # noqa: B009 # using getattr to avoid false positive with mypy type checker
     windll.shell32.SHGetFolderPathW(None, csidl_const, None, 0, buf)
 
     # Downgrade to short path name if it has highbit chars.
-    if any(ord(c) > 255 for c in buf):
+    if any(ord(c) > 255 for c in buf):  # noqa: PLR2004
         buf2 = ctypes.create_unicode_buffer(1024)
         if windll.kernel32.GetShortPathNameW(buf.value, buf2, 1024):
             buf = buf2
 
+    if csidl_name == "CSIDL_DOWNLOADS":
+        return os.path.join(buf.value, "Downloads")  # noqa: PTH118
+
     return buf.value
 
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/cmdline.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/cmdline.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/cmdline.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/cmdline.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Command line interface.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -185,7 +185,7 @@
         return 0
 
     if argns.V:
-        print('Pygments version %s, (c) 2006-2022 by Georg Brandl, Matthäus '
+        print('Pygments version %s, (c) 2006-2023 by Georg Brandl, Matthäus '
               'Chajdas and contributors.' % __version__)
         return 0
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/console.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/console.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/console.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/console.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Format colored console output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/filter.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/filter.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/filter.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/filter.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Module that implements the default filter.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/filters/__init__.py	2023-07-15 11:52:27.561474434 -0400
@@ -5,7 +5,7 @@
     Module containing filter lookup functions and default
     filters.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatter.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Base formatter class.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -26,7 +26,21 @@
     """
     Converts a token stream to text.
 
-    Options accepted:
+    Formatters should have attributes to help selecting them. These
+    are similar to the corresponding :class:`~pygments.lexer.Lexer`
+    attributes.
+
+    .. autoattribute:: name
+       :no-value:
+
+    .. autoattribute:: aliases
+       :no-value:
+
+    .. autoattribute:: filenames
+       :no-value:
+
+    You can pass options as keyword arguments to the constructor.
+    All formatters accept these basic options:
 
     ``style``
         The style to use, can be a string or a Style subclass
@@ -47,15 +61,19 @@
         support (default: None).
     ``outencoding``
         Overrides ``encoding`` if given.
+
     """
 
-    #: Name of the formatter
+    #: Full name for the formatter, in human-readable form.
     name = None
 
-    #: Shortcuts for the formatter
+    #: A list of short, unique identifiers that can be used to lookup
+    #: the formatter from a list, e.g. using :func:`.get_formatter_by_name()`.
     aliases = []
 
-    #: fn match rules
+    #: A list of fnmatch patterns that match filenames for which this
+    #: formatter can produce output. The patterns in this list should be unique
+    #: among all formatters.
     filenames = []
 
     #: If True, this formatter outputs Unicode strings when no encoding
@@ -63,6 +81,11 @@
     unicodeoutput = True
 
     def __init__(self, **options):
+        """
+        As with lexers, this constructor takes arbitrary optional arguments,
+        and if you override it, you should first process your own options, then
+        call the base class implementation.
+        """
         self.style = _lookup_style(options.get('style', 'default'))
         self.full = get_bool_opt(options, 'full', False)
         self.title = options.get('title', '')
@@ -75,18 +98,25 @@
 
     def get_style_defs(self, arg=''):
         """
-        Return the style definitions for the current style as a string.
+        This method must return statements or declarations suitable to define
+        the current style for subsequent highlighted text (e.g. CSS classes
+        in the `HTMLFormatter`).
+
+        The optional argument `arg` can be used to modify the generation and
+        is formatter dependent (it is standardized because it can be given on
+        the command line).
 
-        ``arg`` is an additional argument whose meaning depends on the
-        formatter used. Note that ``arg`` can also be a list or tuple
-        for some formatters like the html formatter.
+        This method is called by the ``-S`` :doc:`command-line option <cmdline>`,
+        the `arg` is then given by the ``-a`` option.
         """
         return ''
 
     def format(self, tokensource, outfile):
         """
-        Format ``tokensource``, an iterable of ``(tokentype, tokenstring)``
-        tuples and write it into ``outfile``.
+        This method must format the tokens from the `tokensource` iterable and
+        write the formatted version to the file object `outfile`.
+
+        Formatter options can control how exactly the tokens are converted.
         """
         if self.encoding:
             # wrap the outfile in a StreamWriter
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/bbcode.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/bbcode.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/bbcode.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/bbcode.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     BBcode formatter.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/groff.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/groff.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/groff.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/groff.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for groff output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -84,7 +84,7 @@
             if ndef['color'] is not None:
                 colors.add(ndef['color'])
 
-        for color in colors:
+        for color in sorted(colors):
             outfile.write('.defcolor ' + color + ' rgb #' + color + '\n')
 
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/html.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/html.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/html.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/html.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for HTML output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -62,7 +62,7 @@
 CSSFILE_TEMPLATE = '''\
 /*
 generated by Pygments <https://pygments.org/>
-Copyright 2006-2022 by the Pygments team.
+Copyright 2006-2023 by the Pygments team.
 Licensed under the BSD license, see LICENSE for details.
 */
 %(styledefs)s
@@ -73,7 +73,7 @@
    "http://www.w3.org/TR/html4/strict.dtd">
 <!--
 generated by Pygments <https://pygments.org/>
-Copyright 2006-2022 by the Pygments team.
+Copyright 2006-2023 by the Pygments team.
 Licensed under the BSD license, see LICENSE for details.
 -->
 <html>
@@ -112,9 +112,9 @@
 
 class HtmlFormatter(Formatter):
     r"""
-    Format tokens as HTML 4 ``<span>`` tags within a ``<pre>`` tag, wrapped
-    in a ``<div>`` tag. The ``<div>``'s CSS class can be set by the `cssclass`
-    option.
+    Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed
+    in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option).
+    The ``<div>``'s CSS class can be set by the `cssclass` option.
 
     If the `linenos` option is set to ``"table"``, the ``<pre>`` is
     additionally wrapped inside a ``<table>`` which has one row and two
@@ -140,8 +140,6 @@
 
     (whitespace added to improve clarity).
 
-    Wrapping can be disabled using the `nowrap` option.
-
     A list of lines can be specified using the `hl_lines` option to make these
     lines highlighted (as of Pygments 0.11).
 
@@ -187,8 +185,8 @@
     Additional options accepted:
 
     `nowrap`
-        If set to ``True``, don't wrap the tokens at all, not even inside a ``<pre>``
-        tag. This disables most other options (default: ``False``).
+        If set to ``True``, don't add a ``<pre>`` and a ``<div>`` tag
+        around the tokens. This disables most other options (default: ``False``).
 
     `full`
         Tells the formatter to output a "full" document, i.e. a complete
@@ -635,7 +633,7 @@
             # write CSS file only if noclobber_cssfile isn't given as an option.
             try:
                 if not os.path.exists(cssfilename) or not self.noclobber_cssfile:
-                    with open(cssfilename, "w") as cf:
+                    with open(cssfilename, "w", encoding="utf-8") as cf:
                         cf.write(CSSFILE_TEMPLATE %
                                  {'styledefs': self.get_style_defs('body')})
             except OSError as err:
@@ -721,7 +719,7 @@
         yield 0, dummyoutfile.getvalue()
         yield 0, '</div>'
         yield 0, '</td></tr></table>'
-        
+
 
     def _wrap_inlinelinenos(self, inner):
         # need a list of lines since we need the width of a single number :(
@@ -946,9 +944,9 @@
         output = source
         if self.wrapcode:
             output = self._wrap_code(output)
-        
+
         output = self._wrap_pre(output)
-    
+
         return output
 
     def format_unencoded(self, tokensource, outfile):
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/img.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/img.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/img.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/img.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for Pixmap output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/__init__.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,13 +4,14 @@
 
     Pygments formatters.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
+import re
 import sys
 import types
-from fnmatch import fnmatch
+import fnmatch
 from os.path import basename
 
 from pip._vendor.pygments.formatters._mapping import FORMATTERS
@@ -21,6 +22,16 @@
            'get_all_formatters', 'load_formatter_from_file'] + list(FORMATTERS)
 
 _formatter_cache = {}  # classes by name
+_pattern_cache = {}
+
+
+def _fn_matches(fn, glob):
+    """Return whether the supplied file name fn matches pattern filename."""
+    if glob not in _pattern_cache:
+        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))
+        return pattern.match(fn)
+    return _pattern_cache[glob].match(fn)
+
 
 def _load_formatters(module_name):
     """Load a formatter (and all others in the module too)."""
@@ -57,9 +68,12 @@
 
 
 def get_formatter_by_name(_alias, **options):
-    """Lookup and instantiate a formatter by alias.
+    """
+    Return an instance of a :class:`.Formatter` subclass that has `alias` in its
+    aliases list. The formatter is given the `options` at its instantiation.
 
-    Raises ClassNotFound if not found.
+    Will raise :exc:`pygments.util.ClassNotFound` if no formatter with that
+    alias is found.
     """
     cls = find_formatter_class(_alias)
     if cls is None:
@@ -67,19 +81,18 @@
     return cls(**options)
 
 
-def load_formatter_from_file(filename, formattername="CustomFormatter",
-                             **options):
-    """Load a formatter from a file.
-
-    This method expects a file located relative to the current working
-    directory, which contains a class named CustomFormatter. By default,
-    it expects the Formatter to be named CustomFormatter; you can specify
-    your own class name as the second argument to this function.
+def load_formatter_from_file(filename, formattername="CustomFormatter", **options):
+    """
+    Return a `Formatter` subclass instance loaded from the provided file, relative
+    to the current directory.
 
-    Users should be very careful with the input, because this method
-    is equivalent to running eval on the input file.
+    The file is expected to contain a Formatter class named ``formattername``
+    (by default, CustomFormatter). Users should be very careful with the input, because
+    this method is equivalent to running ``eval()`` on the input file. The formatter is
+    given the `options` at its instantiation.
 
-    Raises ClassNotFound if there are any problems importing the Formatter.
+    :exc:`pygments.util.ClassNotFound` is raised if there are any errors loading
+    the formatter.
 
     .. versionadded:: 2.2
     """
@@ -104,20 +117,23 @@
 
 
 def get_formatter_for_filename(fn, **options):
-    """Lookup and instantiate a formatter by filename pattern.
+    """
+    Return a :class:`.Formatter` subclass instance that has a filename pattern
+    matching `fn`. The formatter is given the `options` at its instantiation.
 
-    Raises ClassNotFound if not found.
+    Will raise :exc:`pygments.util.ClassNotFound` if no formatter for that filename
+    is found.
     """
     fn = basename(fn)
     for modname, name, _, filenames, _ in FORMATTERS.values():
         for filename in filenames:
-            if fnmatch(fn, filename):
+            if _fn_matches(fn, filename):
                 if name not in _formatter_cache:
                     _load_formatters(modname)
                 return _formatter_cache[name](**options)
     for cls in find_plugin_formatters():
         for filename in cls.filenames:
-            if fnmatch(fn, filename):
+            if _fn_matches(fn, filename):
                 return cls(**options)
     raise ClassNotFound("no formatter found for file name %r" % fn)
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/irc.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/irc.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/irc.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/irc.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for IRC output
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/latex.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/latex.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/latex.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/latex.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for LaTeX fancyvrb output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/_mapping.py	2023-07-15 11:52:27.561474434 -0400
@@ -1,12 +1,12 @@
 # Automatically generated by scripts/gen_mapfiles.py.
-# DO NOT EDIT BY HAND; run `make mapfiles` instead.
+# DO NOT EDIT BY HAND; run `tox -e mapfiles` instead.
 
 FORMATTERS = {
     'BBCodeFormatter': ('pygments.formatters.bbcode', 'BBCode', ('bbcode', 'bb'), (), 'Format tokens with BBcodes. These formatting codes are used by many bulletin boards, so you can highlight your sourcecode with pygments before posting it there.'),
     'BmpImageFormatter': ('pygments.formatters.img', 'img_bmp', ('bmp', 'bitmap'), ('*.bmp',), 'Create a bitmap image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
     'GifImageFormatter': ('pygments.formatters.img', 'img_gif', ('gif',), ('*.gif',), 'Create a GIF image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
     'GroffFormatter': ('pygments.formatters.groff', 'groff', ('groff', 'troff', 'roff'), (), 'Format tokens with groff escapes to change their color and font style.'),
-    'HtmlFormatter': ('pygments.formatters.html', 'HTML', ('html',), ('*.html', '*.htm'), "Format tokens as HTML 4 ``<span>`` tags within a ``<pre>`` tag, wrapped in a ``<div>`` tag. The ``<div>``'s CSS class can be set by the `cssclass` option."),
+    'HtmlFormatter': ('pygments.formatters.html', 'HTML', ('html',), ('*.html', '*.htm'), "Format tokens as HTML 4 ``<span>`` tags. By default, the content is enclosed in a ``<pre>`` tag, itself wrapped in a ``<div>`` tag (but see the `nowrap` option). The ``<div>``'s CSS class can be set by the `cssclass` option."),
     'IRCFormatter': ('pygments.formatters.irc', 'IRC', ('irc', 'IRC'), (), 'Format tokens with IRC color sequences'),
     'ImageFormatter': ('pygments.formatters.img', 'img', ('img', 'IMG', 'png'), ('*.png',), 'Create a PNG image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
     'JpgImageFormatter': ('pygments.formatters.img', 'img_jpg', ('jpg', 'jpeg'), ('*.jpg',), 'Create a JPEG image from source code. This uses the Python Imaging Library to generate a pixmap from the source code.'),
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/other.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/other.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/other.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/other.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Other formatters: NullFormatter, RawTokenFormatter.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/pangomarkup.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/pangomarkup.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/pangomarkup.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/pangomarkup.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for Pango markup output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/rtf.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/rtf.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/rtf.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/rtf.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     A formatter that generates RTF files.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/svg.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/svg.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/svg.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/svg.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for SVG output.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal256.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal256.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal256.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal256.py	2023-07-15 11:52:27.561474434 -0400
@@ -10,7 +10,7 @@
 
     Formatter version 1.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/formatters/terminal.py	2023-07-15 11:52:27.561474434 -0400
@@ -4,7 +4,7 @@
 
     Formatter for terminal output with ANSI sequences.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/__init__.py	2023-07-15 11:52:27.560474434 -0400
@@ -21,12 +21,12 @@
     .. _Pygments master branch:
        https://github.com/pygments/pygments/archive/master.zip#egg=Pygments-dev
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 from io import StringIO, BytesIO
 
-__version__ = '2.14.0'
+__version__ = '2.15.1'
 __docformat__ = 'restructuredtext'
 
 __all__ = ['lex', 'format', 'highlight']
@@ -34,7 +34,9 @@
 
 def lex(code, lexer):
     """
-    Lex ``code`` with ``lexer`` and return an iterable of tokens.
+    Lex `code` with the `lexer` (must be a `Lexer` instance)
+    and return an iterable of tokens. Currently, this only calls
+    `lexer.get_tokens()`.
     """
     try:
         return lexer.get_tokens(code)
@@ -49,11 +51,12 @@
 
 def format(tokens, formatter, outfile=None):  # pylint: disable=redefined-builtin
     """
-    Format a tokenlist ``tokens`` with the formatter ``formatter``.
+    Format ``tokens`` (an iterable of tokens) with the formatter ``formatter``
+    (a `Formatter` instance).
 
-    If ``outfile`` is given and a valid file object (an object
-    with a ``write`` method), the result will be written to it, otherwise
-    it is returned as a string.
+    If ``outfile`` is given and a valid file object (an object with a
+    ``write`` method), the result will be written to it, otherwise it
+    is returned as a string.
     """
     try:
         if not outfile:
@@ -73,10 +76,7 @@
 
 def highlight(code, lexer, formatter, outfile=None):
     """
-    Lex ``code`` with ``lexer`` and format it with the formatter ``formatter``.
-
-    If ``outfile`` is given and a valid file object (an object
-    with a ``write`` method), the result will be written to it, otherwise
-    it is returned as a string.
+    This is the most high-level highlighting function. It combines `lex` and
+    `format` in one function.
     """
     return format(lex(code, lexer), formatter, outfile)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexer.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Base lexer classes.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -50,7 +50,31 @@
     """
     Lexer for a specific language.
 
-    Basic options recognized:
+    See also :doc:`lexerdevelopment`, a high-level guide to writing
+    lexers.
+
+    Lexer classes have attributes used for choosing the most appropriate
+    lexer based on various criteria.
+
+    .. autoattribute:: name
+       :no-value:
+    .. autoattribute:: aliases
+       :no-value:
+    .. autoattribute:: filenames
+       :no-value:
+    .. autoattribute:: alias_filenames
+    .. autoattribute:: mimetypes
+       :no-value:
+    .. autoattribute:: priority
+
+    Lexers included in Pygments should have an additional attribute:
+
+    .. autoattribute:: url
+       :no-value:
+
+    You can pass options to the constructor. The basic options recognized
+    by all lexers and processed by the base `Lexer` class are:
+
     ``stripnl``
         Strip leading and trailing newlines from the input (default: True).
     ``stripall``
@@ -74,28 +98,55 @@
         Overrides the ``encoding`` if given.
     """
 
-    #: Name of the lexer
+    #: Full name of the lexer, in human-readable form
     name = None
 
-    #: URL of the language specification/definition
-    url = None
-
-    #: Shortcuts for the lexer
+    #: A list of short, unique identifiers that can be used to look
+    #: up the lexer from a list, e.g., using `get_lexer_by_name()`.
     aliases = []
 
-    #: File name globs
+    #: A list of `fnmatch` patterns that match filenames which contain
+    #: content for this lexer. The patterns in this list should be unique among
+    #: all lexers.
     filenames = []
 
-    #: Secondary file name globs
+    #: A list of `fnmatch` patterns that match filenames which may or may not
+    #: contain content for this lexer. This list is used by the
+    #: :func:`.guess_lexer_for_filename()` function, to determine which lexers
+    #: are then included in guessing the correct one. That means that
+    #: e.g. every lexer for HTML and a template language should include
+    #: ``\*.html`` in this list.
     alias_filenames = []
 
-    #: MIME types
+    #: A list of MIME types for content that can be lexed with this lexer.
     mimetypes = []
 
     #: Priority, should multiple lexers match and no content is provided
     priority = 0
 
+    #: URL of the language specification/definition. Used in the Pygments
+    #: documentation.
+    url = None
+
     def __init__(self, **options):
+        """
+        This constructor takes arbitrary options as keyword arguments.
+        Every subclass must first process its own options and then call
+        the `Lexer` constructor, since it processes the basic
+        options like `stripnl`.
+
+        An example looks like this:
+
+        .. sourcecode:: python
+
+           def __init__(self, **options):
+               self.compress = options.get('compress', '')
+               Lexer.__init__(self, **options)
+
+        As these options must all be specifiable as strings (due to the
+        command line usage), there are various utility functions
+        available to help with that, see `Utilities`_.
+        """
         self.options = options
         self.stripnl = get_bool_opt(options, 'stripnl', True)
         self.stripall = get_bool_opt(options, 'stripall', False)
@@ -124,10 +175,13 @@
 
     def analyse_text(text):
         """
-        Has to return a float between ``0`` and ``1`` that indicates
-        if a lexer wants to highlight this text. Used by ``guess_lexer``.
-        If this method returns ``0`` it won't highlight it in any case, if
-        it returns ``1`` highlighting with this lexer is guaranteed.
+        A static method which is called for lexer guessing.
+
+        It should analyse the text and return a float in the range
+        from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
+        will not be selected as the most probable one, if it returns
+        ``1.0``, it will be selected immediately.  This is used by
+        `guess_lexer`.
 
         The `LexerMeta` metaclass automatically wraps this function so
         that it works like a static method (no ``self`` or ``cls``
@@ -138,12 +192,17 @@
 
     def get_tokens(self, text, unfiltered=False):
         """
-        Return an iterable of (tokentype, value) pairs generated from
-        `text`. If `unfiltered` is set to `True`, the filtering mechanism
-        is bypassed even if filters are defined.
+        This method is the basic interface of a lexer. It is called by
+        the `highlight()` function. It must process the text and return an
+        iterable of ``(tokentype, value)`` pairs from `text`.
+
+        Normally, you don't need to override this method. The default
+        implementation processes the options recognized by all lexers
+        (`stripnl`, `stripall` and so on), and then yields all tokens
+        from `get_tokens_unprocessed()`, with the ``index`` dropped.
 
-        Also preprocess the text, i.e. expand tabs and strip it if
-        wanted and applies registered filters.
+        If `unfiltered` is set to `True`, the filtering mechanism is
+        bypassed even if filters are defined.
         """
         if not isinstance(text, str):
             if self.encoding == 'guess':
@@ -197,11 +256,12 @@
 
     def get_tokens_unprocessed(self, text):
         """
-        Return an iterable of (index, tokentype, value) pairs where "index"
-        is the starting position of the token within the input text.
+        This method should process the text and return an iterable of
+        ``(index, tokentype, value)`` tuples where ``index`` is the starting
+        position of the token within the input text.
 
-        In subclasses, implement this method as a generator to
-        maximize effectiveness.
+        It must be overridden by subclasses. It is recommended to
+        implement it as a generator to maximize effectiveness.
         """
         raise NotImplementedError
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/__init__.py	2023-07-15 11:52:27.562474434 -0400
@@ -4,13 +4,14 @@
 
     Pygments lexers.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
+import re
 import sys
 import types
-from fnmatch import fnmatch
+import fnmatch
 from os.path import basename
 
 from pip._vendor.pygments.lexers._mapping import LEXERS
@@ -27,6 +28,16 @@
            'guess_lexer', 'load_lexer_from_file'] + list(LEXERS) + list(COMPAT)
 
 _lexer_cache = {}
+_pattern_cache = {}
+
+
+def _fn_matches(fn, glob):
+    """Return whether the supplied file name fn matches pattern filename."""
+    if glob not in _pattern_cache:
+        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))
+        return pattern.match(fn)
+    return _pattern_cache[glob].match(fn)
+
 
 def _load_lexers(module_name):
     """Load a lexer (and all others in the module too)."""
@@ -51,9 +62,9 @@
 
 
 def find_lexer_class(name):
-    """Lookup a lexer class by name.
-
-    Return None if not found.
+    """
+    Return the `Lexer` subclass that with the *name* attribute as given by
+    the *name* argument.
     """
     if name in _lexer_cache:
         return _lexer_cache[name]
@@ -69,10 +80,15 @@
 
 
 def find_lexer_class_by_name(_alias):
-    """Lookup a lexer class by alias.
+    """
+    Return the `Lexer` subclass that has `alias` in its aliases list, without
+    instantiating it.
 
     Like `get_lexer_by_name`, but does not instantiate the class.
 
+    Will raise :exc:`pygments.util.ClassNotFound` if no lexer with that alias is
+    found.
+
     .. versionadded:: 2.2
     """
     if not _alias:
@@ -91,9 +107,13 @@
 
 
 def get_lexer_by_name(_alias, **options):
-    """Get a lexer by an alias.
+    """
+    Return an instance of a `Lexer` subclass that has `alias` in its
+    aliases list. The lexer is given the `options` at its
+    instantiation.
 
-    Raises ClassNotFound if not found.
+    Will raise :exc:`pygments.util.ClassNotFound` if no lexer with that alias is
+    found.
     """
     if not _alias:
         raise ClassNotFound('no lexer for alias %r found' % _alias)
@@ -158,13 +178,13 @@
     fn = basename(_fn)
     for modname, name, _, filenames, _ in LEXERS.values():
         for filename in filenames:
-            if fnmatch(fn, filename):
+            if _fn_matches(fn, filename):
                 if name not in _lexer_cache:
                     _load_lexers(modname)
                 matches.append((_lexer_cache[name], filename))
     for cls in find_plugin_lexers():
         for filename in cls.filenames:
-            if fnmatch(fn, filename):
+            if _fn_matches(fn, filename):
                 matches.append((cls, filename))
 
     if isinstance(code, bytes):
@@ -192,10 +212,15 @@
 def get_lexer_for_filename(_fn, code=None, **options):
     """Get a lexer for a filename.
 
-    If multiple lexers match the filename pattern, use ``analyse_text()`` to
-    figure out which one is more appropriate.
+    Return a `Lexer` subclass instance that has a filename pattern
+    matching `fn`. The lexer is given the `options` at its
+    instantiation.
 
-    Raises ClassNotFound if not found.
+    Raise :exc:`pygments.util.ClassNotFound` if no lexer for that filename
+    is found.
+
+    If multiple lexers match the filename pattern, use their ``analyse_text()``
+    methods to figure out which one is more appropriate.
     """
     res = find_lexer_class_for_filename(_fn, code)
     if not res:
@@ -204,9 +229,12 @@
 
 
 def get_lexer_for_mimetype(_mime, **options):
-    """Get a lexer for a mimetype.
+    """
+    Return a `Lexer` subclass instance that has `mime` in its mimetype
+    list. The lexer is given the `options` at its instantiation.
 
-    Raises ClassNotFound if not found.
+    Will raise :exc:`pygments.util.ClassNotFound` if not lexer for that mimetype
+    is found.
     """
     for modname, name, _, _, mimetypes in LEXERS.values():
         if _mime in mimetypes:
@@ -232,30 +260,22 @@
 
 def guess_lexer_for_filename(_fn, _text, **options):
     """
-    Lookup all lexers that handle those filenames primary (``filenames``)
-    or secondary (``alias_filenames``). Then run a text analysis for those
-    lexers and choose the best result.
-
-    usage::
-
-        >>> from pygments.lexers import guess_lexer_for_filename
-        >>> guess_lexer_for_filename('hello.html', '<%= @foo %>')
-        <pygments.lexers.templates.RhtmlLexer object at 0xb7d2f32c>
-        >>> guess_lexer_for_filename('hello.html', '<h1>{{ title|e }}</h1>')
-        <pygments.lexers.templates.HtmlDjangoLexer object at 0xb7d2f2ac>
-        >>> guess_lexer_for_filename('style.css', 'a { color: <?= $link ?> }')
-        <pygments.lexers.templates.CssPhpLexer object at 0xb7ba518c>
+    As :func:`guess_lexer()`, but only lexers which have a pattern in `filenames`
+    or `alias_filenames` that matches `filename` are taken into consideration.
+
+    :exc:`pygments.util.ClassNotFound` is raised if no lexer thinks it can
+    handle the content.
     """
     fn = basename(_fn)
     primary = {}
     matching_lexers = set()
     for lexer in _iter_lexerclasses():
         for filename in lexer.filenames:
-            if fnmatch(fn, filename):
+            if _fn_matches(fn, filename):
                 matching_lexers.add(lexer)
                 primary[lexer] = True
         for filename in lexer.alias_filenames:
-            if fnmatch(fn, filename):
+            if _fn_matches(fn, filename):
                 matching_lexers.add(lexer)
                 primary[lexer] = False
     if not matching_lexers:
@@ -282,7 +302,15 @@
 
 
 def guess_lexer(_text, **options):
-    """Guess a lexer by strong distinctions in the text (eg, shebang)."""
+    """
+    Return a `Lexer` subclass instance that's guessed from the text in
+    `text`. For that, the :meth:`.analyse_text()` method of every known lexer
+    class is called with the text as argument, and the lexer which returned the
+    highest value will be instantiated and returned.
+
+    :exc:`pygments.util.ClassNotFound` is raised if no lexer thinks it can
+    handle the content.
+    """
 
     if not isinstance(_text, str):
         inencoding = options.get('inencoding', options.get('encoding'))
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/_mapping.py	2023-07-15 11:52:27.562474434 -0400
@@ -1,5 +1,5 @@
 # Automatically generated by scripts/gen_mapfiles.py.
-# DO NOT EDIT BY HAND; run `make mapfiles` instead.
+# DO NOT EDIT BY HAND; run `tox -e mapfiles` instead.
 
 LEXERS = {
     'ABAPLexer': ('pip._vendor.pygments.lexers.business', 'ABAP', ('abap',), ('*.abap', '*.ABAP'), ('text/x-abap',)),
@@ -71,6 +71,7 @@
     'CadlLexer': ('pip._vendor.pygments.lexers.archetype', 'cADL', ('cadl',), ('*.cadl',), ()),
     'CapDLLexer': ('pip._vendor.pygments.lexers.esoteric', 'CapDL', ('capdl',), ('*.cdl',), ()),
     'CapnProtoLexer': ('pip._vendor.pygments.lexers.capnproto', "Cap'n Proto", ('capnp',), ('*.capnp',), ()),
+    'CarbonLexer': ('pip._vendor.pygments.lexers.carbon', 'Carbon', ('carbon',), ('*.carbon',), ('text/x-carbon',)),
     'CbmBasicV2Lexer': ('pip._vendor.pygments.lexers.basic', 'CBM BASIC V2', ('cbmbas',), ('*.bas',), ()),
     'CddlLexer': ('pip._vendor.pygments.lexers.cddl', 'CDDL', ('cddl',), ('*.cddl',), ('text/x-cddl',)),
     'CeylonLexer': ('pip._vendor.pygments.lexers.jvm', 'Ceylon', ('ceylon',), ('*.ceylon',), ('text/x-ceylon',)),
@@ -121,6 +122,7 @@
     'DarcsPatchLexer': ('pip._vendor.pygments.lexers.diff', 'Darcs Patch', ('dpatch',), ('*.dpatch', '*.darcspatch'), ()),
     'DartLexer': ('pip._vendor.pygments.lexers.javascript', 'Dart', ('dart',), ('*.dart',), ('text/x-dart',)),
     'Dasm16Lexer': ('pip._vendor.pygments.lexers.asm', 'DASM16', ('dasm16',), ('*.dasm16', '*.dasm'), ('text/x-dasm16',)),
+    'DaxLexer': ('pip._vendor.pygments.lexers.dax', 'Dax', ('dax',), ('*.dax',), ()),
     'DebianControlLexer': ('pip._vendor.pygments.lexers.installers', 'Debian Control file', ('debcontrol', 'control'), ('control',), ()),
     'DelphiLexer': ('pip._vendor.pygments.lexers.pascal', 'Delphi', ('delphi', 'pas', 'pascal', 'objectpascal'), ('*.pas', '*.dpr'), ('text/x-pascal',)),
     'DevicetreeLexer': ('pip._vendor.pygments.lexers.devicetree', 'Devicetree', ('devicetree', 'dts'), ('*.dts', '*.dtsi'), ('text/x-c',)),
@@ -368,6 +370,7 @@
     'PortugolLexer': ('pip._vendor.pygments.lexers.pascal', 'Portugol', ('portugol',), ('*.alg', '*.portugol'), ()),
     'PostScriptLexer': ('pip._vendor.pygments.lexers.graphics', 'PostScript', ('postscript', 'postscr'), ('*.ps', '*.eps'), ('application/postscript',)),
     'PostgresConsoleLexer': ('pip._vendor.pygments.lexers.sql', 'PostgreSQL console (psql)', ('psql', 'postgresql-console', 'postgres-console'), (), ('text/x-postgresql-psql',)),
+    'PostgresExplainLexer': ('pip._vendor.pygments.lexers.sql', 'PostgreSQL EXPLAIN dialect', ('postgres-explain',), ('*.explain',), ('text/x-postgresql-explain',)),
     'PostgresLexer': ('pip._vendor.pygments.lexers.sql', 'PostgreSQL SQL dialect', ('postgresql', 'postgres'), (), ('text/x-postgresql',)),
     'PovrayLexer': ('pip._vendor.pygments.lexers.graphics', 'POVRay', ('pov',), ('*.pov', '*.inc'), ('text/x-povray',)),
     'PowerShellLexer': ('pip._vendor.pygments.lexers.shell', 'PowerShell', ('powershell', 'pwsh', 'posh', 'ps1', 'psm1'), ('*.ps1', '*.psm1'), ('text/x-powershell',)),
@@ -488,7 +491,7 @@
     'TeraTermLexer': ('pip._vendor.pygments.lexers.teraterm', 'Tera Term macro', ('teratermmacro', 'teraterm', 'ttl'), ('*.ttl',), ('text/x-teratermmacro',)),
     'TermcapLexer': ('pip._vendor.pygments.lexers.configs', 'Termcap', ('termcap',), ('termcap', 'termcap.src'), ()),
     'TerminfoLexer': ('pip._vendor.pygments.lexers.configs', 'Terminfo', ('terminfo',), ('terminfo', 'terminfo.src'), ()),
-    'TerraformLexer': ('pip._vendor.pygments.lexers.configs', 'Terraform', ('terraform', 'tf'), ('*.tf',), ('application/x-tf', 'application/x-terraform')),
+    'TerraformLexer': ('pip._vendor.pygments.lexers.configs', 'Terraform', ('terraform', 'tf', 'hcl'), ('*.tf', '*.hcl'), ('application/x-tf', 'application/x-terraform')),
     'TexLexer': ('pip._vendor.pygments.lexers.markup', 'TeX', ('tex', 'latex'), ('*.tex', '*.aux', '*.toc'), ('text/x-tex', 'text/x-latex')),
     'TextLexer': ('pip._vendor.pygments.lexers.special', 'Text only', ('text',), ('*.txt',), ('text/plain',)),
     'ThingsDBLexer': ('pip._vendor.pygments.lexers.thingsdb', 'ThingsDB', ('ti', 'thingsdb'), ('*.ti',), ()),
@@ -528,7 +531,9 @@
     'WDiffLexer': ('pip._vendor.pygments.lexers.diff', 'WDiff', ('wdiff',), ('*.wdiff',), ()),
     'WatLexer': ('pip._vendor.pygments.lexers.webassembly', 'WebAssembly', ('wast', 'wat'), ('*.wat', '*.wast'), ()),
     'WebIDLLexer': ('pip._vendor.pygments.lexers.webidl', 'Web IDL', ('webidl',), ('*.webidl',), ()),
+    'WgslLexer': ('pip._vendor.pygments.lexers.wgsl', 'WebGPU Shading Language', ('wgsl',), ('*.wgsl',), ('text/wgsl',)),
     'WhileyLexer': ('pip._vendor.pygments.lexers.whiley', 'Whiley', ('whiley',), ('*.whiley',), ('text/x-whiley',)),
+    'WikitextLexer': ('pip._vendor.pygments.lexers.markup', 'Wikitext', ('wikitext', 'mediawiki'), (), ('text/x-wiki',)),
     'WoWTocLexer': ('pip._vendor.pygments.lexers.wowtoc', 'World of Warcraft TOC', ('wowtoc',), ('*.toc',), ()),
     'WrenLexer': ('pip._vendor.pygments.lexers.wren', 'Wren', ('wren',), ('*.wren',), ()),
     'X10Lexer': ('pip._vendor.pygments.lexers.x10', 'X10', ('x10', 'xten'), ('*.x10',), ('text/x-x10',)),
@@ -540,6 +545,7 @@
     'XmlPhpLexer': ('pip._vendor.pygments.lexers.templates', 'XML+PHP', ('xml+php',), (), ('application/xml+php',)),
     'XmlSmartyLexer': ('pip._vendor.pygments.lexers.templates', 'XML+Smarty', ('xml+smarty',), (), ('application/xml+smarty',)),
     'XorgLexer': ('pip._vendor.pygments.lexers.xorg', 'Xorg', ('xorg.conf',), ('xorg.conf',), ()),
+    'XppLexer': ('pip._vendor.pygments.lexers.dotnet', 'X++', ('xpp', 'x++'), ('*.xpp',), ()),
     'XsltLexer': ('pip._vendor.pygments.lexers.html', 'XSLT', ('xslt',), ('*.xsl', '*.xslt', '*.xpl'), ('application/xsl+xml', 'application/xslt+xml')),
     'XtendLexer': ('pip._vendor.pygments.lexers.jvm', 'Xtend', ('xtend',), ('*.xtend',), ('text/x-xtend',)),
     'XtlangLexer': ('pip._vendor.pygments.lexers.lisp', 'xtlang', ('extempore',), ('*.xtm',), ()),
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/lexers/python.py	2023-07-15 11:52:27.562474434 -0400
@@ -4,15 +4,15 @@
 
     Lexers for Python and related languages.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
 import re
 import keyword
 
-from pip._vendor.pygments.lexer import Lexer, RegexLexer, include, bygroups, using, \
-    default, words, combined, do_insertions, this, line_re
+from pip._vendor.pygments.lexer import DelegatingLexer, Lexer, RegexLexer, include, \
+    bygroups, using, default, words, combined, do_insertions, this, line_re
 from pip._vendor.pygments.util import get_bool_opt, shebang_matches
 from pip._vendor.pygments.token import Text, Comment, Operator, Keyword, Name, String, \
     Number, Punctuation, Generic, Other, Error, Whitespace
@@ -234,16 +234,16 @@
         ],
         'builtins': [
             (words((
-                '__import__', 'abs', 'all', 'any', 'bin', 'bool', 'bytearray',
-                'breakpoint', 'bytes', 'chr', 'classmethod', 'compile', 'complex',
-                'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval', 'filter',
-                'float', 'format', 'frozenset', 'getattr', 'globals', 'hasattr',
-                'hash', 'hex', 'id', 'input', 'int', 'isinstance', 'issubclass',
-                'iter', 'len', 'list', 'locals', 'map', 'max', 'memoryview',
-                'min', 'next', 'object', 'oct', 'open', 'ord', 'pow', 'print',
-                'property', 'range', 'repr', 'reversed', 'round', 'set', 'setattr',
-                'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super', 'tuple',
-                'type', 'vars', 'zip'), prefix=r'(?<!\.)', suffix=r'\b'),
+                '__import__', 'abs', 'aiter', 'all', 'any', 'bin', 'bool', 'bytearray',
+                'breakpoint', 'bytes', 'callable', 'chr', 'classmethod', 'compile',
+                'complex', 'delattr', 'dict', 'dir', 'divmod', 'enumerate', 'eval',
+                'filter', 'float', 'format', 'frozenset', 'getattr', 'globals',
+                'hasattr', 'hash', 'hex', 'id', 'input', 'int', 'isinstance',
+                'issubclass', 'iter', 'len', 'list', 'locals', 'map', 'max',
+                'memoryview', 'min', 'next', 'object', 'oct', 'open', 'ord', 'pow',
+                'print', 'property', 'range', 'repr', 'reversed', 'round', 'set',
+                'setattr', 'slice', 'sorted', 'staticmethod', 'str', 'sum', 'super',
+                'tuple', 'type', 'vars', 'zip'), prefix=r'(?<!\.)', suffix=r'\b'),
              Name.Builtin),
             (r'(?<!\.)(self|Ellipsis|NotImplemented|cls)\b', Name.Builtin.Pseudo),
             (words((
@@ -341,7 +341,7 @@
             (r'\.', Name.Namespace),
             # if None occurs here, it's "raise x from None", since None can
             # never be a module name
-            (r'None\b', Name.Builtin.Pseudo, '#pop'),
+            (r'None\b', Keyword.Constant, '#pop'),
             (uni_name, Name.Namespace),
             default('#pop'),
         ],
@@ -635,15 +635,50 @@
     def analyse_text(text):
         return shebang_matches(text, r'pythonw?2(\.\d)?')
 
+class _PythonConsoleLexerBase(RegexLexer):
+    name = 'Python console session'
+    aliases = ['pycon']
+    mimetypes = ['text/x-python-doctest']
+
+    """Auxiliary lexer for `PythonConsoleLexer`.
+
+    Code tokens are output as ``Token.Other.Code``, traceback tokens as
+    ``Token.Other.Traceback``.
+    """
+    tokens = {
+        'root': [
+            (r'(>>> )(.*\n)', bygroups(Generic.Prompt, Other.Code), 'continuations'),
+            # This happens, e.g., when tracebacks are embedded in documentation;
+            # trailing whitespaces are often stripped in such contexts.
+            (r'(>>>)(\n)', bygroups(Generic.Prompt, Whitespace)),
+            (r'(\^C)?Traceback \(most recent call last\):\n', Other.Traceback, 'traceback'),
+            # SyntaxError starts with this
+            (r'  File "[^"]+", line \d+', Other.Traceback, 'traceback'),
+            (r'.*\n', Generic.Output),
+        ],
+        'continuations': [
+            (r'(\.\.\. )(.*\n)', bygroups(Generic.Prompt, Other.Code)),
+            # See above.
+            (r'(\.\.\.)(\n)', bygroups(Generic.Prompt, Whitespace)),
+            default('#pop'),
+        ],
+        'traceback': [
+            # As soon as we see a traceback, consume everything until the next
+            # >>> prompt.
+            (r'(?=>>>( |$))', Text, '#pop'),
+            (r'(KeyboardInterrupt)(\n)', bygroups(Name.Class, Whitespace)),
+            (r'.*\n', Other.Traceback),
+        ],
+    }
 
-class PythonConsoleLexer(Lexer):
+class PythonConsoleLexer(DelegatingLexer):
     """
     For Python console output or doctests, such as:
 
     .. sourcecode:: pycon
 
         >>> a = 'foo'
-        >>> print a
+        >>> print(a)
         foo
         >>> 1 / 0
         Traceback (most recent call last):
@@ -659,70 +694,28 @@
         .. versionchanged:: 2.5
            Now defaults to ``True``.
     """
+
     name = 'Python console session'
     aliases = ['pycon']
     mimetypes = ['text/x-python-doctest']
 
     def __init__(self, **options):
-        self.python3 = get_bool_opt(options, 'python3', True)
-        Lexer.__init__(self, **options)
-
-    def get_tokens_unprocessed(self, text):
-        if self.python3:
-            pylexer = PythonLexer(**self.options)
-            tblexer = PythonTracebackLexer(**self.options)
+        python3 = get_bool_opt(options, 'python3', True)
+        if python3:
+            pylexer = PythonLexer
+            tblexer = PythonTracebackLexer
         else:
-            pylexer = Python2Lexer(**self.options)
-            tblexer = Python2TracebackLexer(**self.options)
-
-        curcode = ''
-        insertions = []
-        curtb = ''
-        tbindex = 0
-        tb = 0
-        for match in line_re.finditer(text):
-            line = match.group()
-            if line.startswith('>>> ') or line.startswith('... '):
-                tb = 0
-                insertions.append((len(curcode),
-                                   [(0, Generic.Prompt, line[:4])]))
-                curcode += line[4:]
-            elif line.rstrip() == '...' and not tb:
-                # only a new >>> prompt can end an exception block
-                # otherwise an ellipsis in place of the traceback frames
-                # will be mishandled
-                insertions.append((len(curcode),
-                                   [(0, Generic.Prompt, '...')]))
-                curcode += line[3:]
-            else:
-                if curcode:
-                    yield from do_insertions(
-                        insertions, pylexer.get_tokens_unprocessed(curcode))
-                    curcode = ''
-                    insertions = []
-                if (line.startswith('Traceback (most recent call last):') or
-                        re.match('  File "[^"]+", line \\d+\\n$', line)):
-                    tb = 1
-                    curtb = line
-                    tbindex = match.start()
-                elif line == 'KeyboardInterrupt\n':
-                    yield match.start(), Name.Class, line
-                elif tb:
-                    curtb += line
-                    if not (line.startswith(' ') or line.strip() == '...'):
-                        tb = 0
-                        for i, t, v in tblexer.get_tokens_unprocessed(curtb):
-                            yield tbindex+i, t, v
-                        curtb = ''
-                else:
-                    yield match.start(), Generic.Output, line
-        if curcode:
-            yield from do_insertions(insertions,
-                                     pylexer.get_tokens_unprocessed(curcode))
-        if curtb:
-            for i, t, v in tblexer.get_tokens_unprocessed(curtb):
-                yield tbindex+i, t, v
-
+            pylexer = Python2Lexer
+            tblexer = Python2TracebackLexer
+        # We have two auxiliary lexers. Use DelegatingLexer twice with
+        # different tokens.  TODO: DelegatingLexer should support this
+        # directly, by accepting a tuplet of auxiliary lexers and a tuple of
+        # distinguishing tokens. Then we wouldn't need this intermediary
+        # class.
+        class _ReplaceInnerCode(DelegatingLexer):
+            def __init__(self, **options):
+                super().__init__(pylexer, _PythonConsoleLexerBase, Other.Code, **options)
+        super().__init__(tblexer, _ReplaceInnerCode, Other.Traceback, **options)
 
 class PythonTracebackLexer(RegexLexer):
     """
@@ -743,7 +736,7 @@
     tokens = {
         'root': [
             (r'\n', Whitespace),
-            (r'^Traceback \(most recent call last\):\n', Generic.Traceback, 'intb'),
+            (r'^(\^C)?Traceback \(most recent call last\):\n', Generic.Traceback, 'intb'),
             (r'^During handling of the above exception, another '
              r'exception occurred:\n\n', Generic.Traceback),
             (r'^The above exception was the direct cause of the '
@@ -763,7 +756,8 @@
             (r'^([^:]+)(: )(.+)(\n)',
              bygroups(Generic.Error, Text, Name, Whitespace), '#pop'),
             (r'^([a-zA-Z_][\w.]*)(:?\n)',
-             bygroups(Generic.Error, Whitespace), '#pop')
+             bygroups(Generic.Error, Whitespace), '#pop'),
+            default('#pop'),
         ],
         'markers': [
             # Either `PEP 657 <https://www.python.org/dev/peps/pep-0657/>`
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/__main__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/__main__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/__main__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/__main__.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Main entry point for ``python -m pygments``.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/modeline.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/modeline.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/modeline.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/modeline.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     A simple modeline parser (based on pymodeline).
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/plugin.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/plugin.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/plugin.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/plugin.py	2023-07-15 11:52:27.560474434 -0400
@@ -34,7 +34,7 @@
         yourfilter = yourfilter:YourFilter
 
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/regexopt.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/regexopt.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/regexopt.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/regexopt.py	2023-07-15 11:52:27.560474434 -0400
@@ -5,7 +5,7 @@
     An algorithm that generates optimized regexes for matching long lists of
     literal strings.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/scanner.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/scanner.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/scanner.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/scanner.py	2023-07-15 11:52:27.560474434 -0400
@@ -11,7 +11,7 @@
     Have a look at the `DelphiLexer` to get an idea of how to use
     this scanner.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 import re
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/sphinxext.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/sphinxext.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/sphinxext.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/sphinxext.py	2023-07-15 11:52:27.560474434 -0400
@@ -5,7 +5,7 @@
     Sphinx extension to generate automatic documentation of lexers,
     formatters and filters.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/style.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/style.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/style.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/style.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Basic style object.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/styles/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/styles/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/styles/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/styles/__init__.py	2023-07-15 11:52:27.562474434 -0400
@@ -4,15 +4,15 @@
 
     Contains built-in styles.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
 from pip._vendor.pygments.plugin import find_plugin_styles
 from pip._vendor.pygments.util import ClassNotFound
 
-
-#: Maps style names to 'submodule::classname'.
+#: A dictionary of built-in styles, mapping style names to
+#: ``'submodule::classname'`` strings.
 STYLE_MAP = {
     'default':  'default::DefaultStyle',
     'emacs':    'emacs::EmacsStyle',
@@ -66,6 +66,13 @@
 
 
 def get_style_by_name(name):
+    """
+    Return a style class by its short name. The names of the builtin styles
+    are listed in :data:`pygments.styles.STYLE_MAP`.
+
+    Will raise :exc:`pygments.util.ClassNotFound` if no style of that name is
+    found.
+    """
     if name in STYLE_MAP:
         mod, cls = STYLE_MAP[name].split('::')
         builtin = "yes"
@@ -90,8 +97,7 @@
 
 
 def get_all_styles():
-    """Return a generator for all styles by name,
-    both builtin and plugin."""
+    """Return a generator for all styles by name, both builtin and plugin."""
     yield from STYLE_MAP
     for name, _ in find_plugin_styles():
         yield name
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/token.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/token.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/token.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/token.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Basic token types and the standard tokens.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/unistring.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/unistring.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/unistring.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/unistring.py	2023-07-15 11:52:27.560474434 -0400
@@ -7,7 +7,7 @@
 
     Inspired by chartypes_create.py from the MoinMoin project.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -112,7 +112,7 @@
 
     categories = {'xid_start': [], 'xid_continue': []}
 
-    with open(__file__) as fp:
+    with open(__file__, encoding='utf-8') as fp:
         content = fp.read()
 
     header = content[:content.find('Cc =')]
@@ -136,7 +136,7 @@
         if ('a' + c).isidentifier():
             categories['xid_continue'].append(c)
 
-    with open(__file__, 'w') as fp:
+    with open(__file__, 'w', encoding='utf-8') as fp:
         fp.write(header)
 
         for cat in sorted(categories):
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/util.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/util.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pygments/util.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pygments/util.py	2023-07-15 11:52:27.560474434 -0400
@@ -4,7 +4,7 @@
 
     Utility functions.
 
-    :copyright: Copyright 2006-2022 by the Pygments team, see AUTHORS.
+    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
     :license: BSD, see LICENSE for details.
 """
 
@@ -32,10 +32,16 @@
 
 
 class OptionError(Exception):
-    pass
-
+    """
+    This exception will be raised by all option processing functions if
+    the type or value of the argument is not correct.
+    """
 
 def get_choice_opt(options, optname, allowed, default=None, normcase=False):
+    """
+    If the key `optname` from the dictionary is not in the sequence
+    `allowed`, raise an error, otherwise return it.
+    """
     string = options.get(optname, default)
     if normcase:
         string = string.lower()
@@ -46,6 +52,17 @@
 
 
 def get_bool_opt(options, optname, default=None):
+    """
+    Intuitively, this is `options.get(optname, default)`, but restricted to
+    Boolean value. The Booleans can be represented as string, in order to accept
+    Boolean value from the command line arguments. If the key `optname` is
+    present in the dictionary `options` and is not associated with a Boolean,
+    raise an `OptionError`. If it is absent, `default` is returned instead.
+
+    The valid string values for ``True`` are ``1``, ``yes``, ``true`` and
+    ``on``, the ones for ``False`` are ``0``, ``no``, ``false`` and ``off``
+    (matched case-insensitively).
+    """
     string = options.get(optname, default)
     if isinstance(string, bool):
         return string
@@ -66,6 +83,7 @@
 
 
 def get_int_opt(options, optname, default=None):
+    """As :func:`get_bool_opt`, but interpret the value as an integer."""
     string = options.get(optname, default)
     try:
         return int(string)
@@ -78,8 +96,12 @@
                           'must give an integer value' % (
                               string, optname))
 
-
 def get_list_opt(options, optname, default=None):
+    """
+    If the key `optname` from the dictionary `options` is a string,
+    split it at whitespace and return it. If it is already a list
+    or a tuple, it is returned as a list.
+    """
     val = options.get(optname, default)
     if isinstance(val, str):
         return val.split()
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/actions.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/actions.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/actions.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/actions.py	2023-07-15 11:52:27.574474434 -0400
@@ -1,7 +1,7 @@
 # actions.py
 
 from .exceptions import ParseException
-from .util import col
+from .util import col, replaced_by_pep8
 
 
 class OnlyOnce:
@@ -38,7 +38,7 @@
 
     def verify_col(strg, locn, toks):
         if col(locn, strg) != n:
-            raise ParseException(strg, locn, "matched token not at column {}".format(n))
+            raise ParseException(strg, locn, f"matched token not at column {n}")
 
     return verify_col
 
@@ -148,15 +148,13 @@
                 raise ParseException(
                     s,
                     l,
-                    "attribute {!r} has value {!r}, must be {!r}".format(
-                        attrName, tokens[attrName], attrValue
-                    ),
+                    f"attribute {attrName!r} has value {tokens[attrName]!r}, must be {attrValue!r}",
                 )
 
     return pa
 
 
-with_attribute.ANY_VALUE = object()
+with_attribute.ANY_VALUE = object()  # type: ignore [attr-defined]
 
 
 def with_class(classname, namespace=""):
@@ -195,13 +193,25 @@
         1 4 0 1 0
         1,3 2,3 1,1
     """
-    classattr = "{}:class".format(namespace) if namespace else "class"
+    classattr = f"{namespace}:class" if namespace else "class"
     return with_attribute(**{classattr: classname})
 
 
 # pre-PEP8 compatibility symbols
-replaceWith = replace_with
-removeQuotes = remove_quotes
-withAttribute = with_attribute
-withClass = with_class
-matchOnlyAtCol = match_only_at_col
+# fmt: off
+@replaced_by_pep8(replace_with)
+def replaceWith(): ...
+
+@replaced_by_pep8(remove_quotes)
+def removeQuotes(): ...
+
+@replaced_by_pep8(with_attribute)
+def withAttribute(): ...
+
+@replaced_by_pep8(with_class)
+def withClass(): ...
+
+@replaced_by_pep8(match_only_at_col)
+def matchOnlyAtCol(): ...
+
+# fmt: on
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/common.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/common.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/common.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/common.py	2023-07-15 11:52:27.574474434 -0400
@@ -1,6 +1,6 @@
 # common.py
 from .core import *
-from .helpers import delimited_list, any_open_tag, any_close_tag
+from .helpers import DelimitedList, any_open_tag, any_close_tag
 from datetime import datetime
 
 
@@ -22,17 +22,17 @@
 
     Parse actions:
 
-    - :class:`convertToInteger`
-    - :class:`convertToFloat`
-    - :class:`convertToDate`
-    - :class:`convertToDatetime`
-    - :class:`stripHTMLTags`
-    - :class:`upcaseTokens`
-    - :class:`downcaseTokens`
+    - :class:`convert_to_integer`
+    - :class:`convert_to_float`
+    - :class:`convert_to_date`
+    - :class:`convert_to_datetime`
+    - :class:`strip_html_tags`
+    - :class:`upcase_tokens`
+    - :class:`downcase_tokens`
 
     Example::
 
-        pyparsing_common.number.runTests('''
+        pyparsing_common.number.run_tests('''
             # any int or real number, returned as the appropriate type
             100
             -100
@@ -42,7 +42,7 @@
             1e-12
             ''')
 
-        pyparsing_common.fnumber.runTests('''
+        pyparsing_common.fnumber.run_tests('''
             # any int or real number, returned as float
             100
             -100
@@ -52,19 +52,19 @@
             1e-12
             ''')
 
-        pyparsing_common.hex_integer.runTests('''
+        pyparsing_common.hex_integer.run_tests('''
             # hex numbers
             100
             FF
             ''')
 
-        pyparsing_common.fraction.runTests('''
+        pyparsing_common.fraction.run_tests('''
             # fractions
             1/2
             -3/4
             ''')
 
-        pyparsing_common.mixed_integer.runTests('''
+        pyparsing_common.mixed_integer.run_tests('''
             # mixed fractions
             1
             1/2
@@ -73,8 +73,8 @@
             ''')
 
         import uuid
-        pyparsing_common.uuid.setParseAction(tokenMap(uuid.UUID))
-        pyparsing_common.uuid.runTests('''
+        pyparsing_common.uuid.set_parse_action(token_map(uuid.UUID))
+        pyparsing_common.uuid.run_tests('''
             # uuid
             12345678-1234-5678-1234-567812345678
             ''')
@@ -260,8 +260,8 @@
         Example::
 
             date_expr = pyparsing_common.iso8601_date.copy()
-            date_expr.setParseAction(pyparsing_common.convertToDate())
-            print(date_expr.parseString("1999-12-31"))
+            date_expr.set_parse_action(pyparsing_common.convert_to_date())
+            print(date_expr.parse_string("1999-12-31"))
 
         prints::
 
@@ -287,8 +287,8 @@
         Example::
 
             dt_expr = pyparsing_common.iso8601_datetime.copy()
-            dt_expr.setParseAction(pyparsing_common.convertToDatetime())
-            print(dt_expr.parseString("1999-12-31T23:59:59.999"))
+            dt_expr.set_parse_action(pyparsing_common.convert_to_datetime())
+            print(dt_expr.parse_string("1999-12-31T23:59:59.999"))
 
         prints::
 
@@ -326,9 +326,9 @@
 
             # strip HTML links from normal text
             text = '<td>More info at the <a href="https://github.com/pyparsing/pyparsing/wiki">pyparsing</a> wiki page</td>'
-            td, td_end = makeHTMLTags("TD")
-            table_text = td + SkipTo(td_end).setParseAction(pyparsing_common.stripHTMLTags)("body") + td_end
-            print(table_text.parseString(text).body)
+            td, td_end = make_html_tags("TD")
+            table_text = td + SkipTo(td_end).set_parse_action(pyparsing_common.strip_html_tags)("body") + td_end
+            print(table_text.parse_string(text).body)
 
         Prints::
 
@@ -348,7 +348,7 @@
         .streamline()
         .set_name("commaItem")
     )
-    comma_separated_list = delimited_list(
+    comma_separated_list = DelimitedList(
         Opt(quoted_string.copy() | _commasepitem, default="")
     ).set_name("comma separated list")
     """Predefined expression of 1 or more printable words or quoted strings, separated by commas."""
@@ -363,7 +363,7 @@
     url = Regex(
         # https://mathiasbynens.be/demo/url-regex
         # https://gist.github.com/dperini/729294
-        r"^" +
+        r"(?P<url>" +
         # protocol identifier (optional)
         # short syntax // still required
         r"(?:(?:(?P<scheme>https?|ftp):)?\/\/)" +
@@ -405,18 +405,26 @@
         r"(\?(?P<query>[^#]*))?" +
         # fragment (optional)
         r"(#(?P<fragment>\S*))?" +
-        r"$"
+        r")"
     ).set_name("url")
+    """URL (http/https/ftp scheme)"""
     # fmt: on
 
     # pre-PEP8 compatibility names
     convertToInteger = convert_to_integer
+    """Deprecated - use :class:`convert_to_integer`"""
     convertToFloat = convert_to_float
+    """Deprecated - use :class:`convert_to_float`"""
     convertToDate = convert_to_date
+    """Deprecated - use :class:`convert_to_date`"""
     convertToDatetime = convert_to_datetime
+    """Deprecated - use :class:`convert_to_datetime`"""
     stripHTMLTags = strip_html_tags
+    """Deprecated - use :class:`strip_html_tags`"""
     upcaseTokens = upcase_tokens
+    """Deprecated - use :class:`upcase_tokens`"""
     downcaseTokens = downcase_tokens
+    """Deprecated - use :class:`downcase_tokens`"""
 
 
 _builtin_exprs = [
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/core.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/core.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/core.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/core.py	2023-07-15 11:52:27.574474434 -0400
@@ -1,19 +1,22 @@
 #
 # core.py
 #
+
+from collections import deque
 import os
 import typing
 from typing import (
-    NamedTuple,
-    Union,
-    Callable,
     Any,
+    Callable,
     Generator,
-    Tuple,
     List,
-    TextIO,
-    Set,
+    NamedTuple,
     Sequence,
+    Set,
+    TextIO,
+    Tuple,
+    Union,
+    cast,
 )
 from abc import ABC, abstractmethod
 from enum import Enum
@@ -40,6 +43,7 @@
     _flatten,
     LRUMemo as _LRUMemo,
     UnboundedMemo as _UnboundedMemo,
+    replaced_by_pep8,
 )
 from .exceptions import *
 from .actions import *
@@ -134,6 +138,7 @@
 class Diagnostics(Enum):
     """
     Diagnostic configuration (all default to disabled)
+
     - ``warn_multiple_tokens_in_named_alternation`` - flag to enable warnings when a results
       name is defined on a :class:`MatchFirst` or :class:`Or` expression with one or more :class:`And` subexpressions
     - ``warn_ungrouped_named_tokens_in_collection`` - flag to enable warnings when a results
@@ -228,6 +233,8 @@
 }
 
 _generatorType = types.GeneratorType
+ParseImplReturnType = Tuple[int, Any]
+PostParseReturnType = Union[ParseResults, Sequence[ParseResults]]
 ParseAction = Union[
     Callable[[], Any],
     Callable[[ParseResults], Any],
@@ -256,7 +263,7 @@
 alphanums = alphas + nums
 printables = "".join([c for c in string.printable if c not in string.whitespace])
 
-_trim_arity_call_line: traceback.StackSummary = None
+_trim_arity_call_line: traceback.StackSummary = None  # type: ignore[assignment]
 
 
 def _trim_arity(func, max_limit=3):
@@ -269,11 +276,6 @@
     limit = 0
     found_arity = False
 
-    def extract_tb(tb, limit=0):
-        frames = traceback.extract_tb(tb, limit=limit)
-        frame_summary = frames[-1]
-        return [frame_summary[:2]]
-
     # synthesize what would be returned by traceback.extract_stack at the call to
     # user's parse action 'func', so that we don't incur call penalty at parse time
 
@@ -297,8 +299,10 @@
                     raise
                 else:
                     tb = te.__traceback__
+                    frames = traceback.extract_tb(tb, limit=2)
+                    frame_summary = frames[-1]
                     trim_arity_type_error = (
-                        extract_tb(tb, limit=2)[-1][:2] == pa_call_line_synth
+                        [frame_summary[:2]][-1][:2] == pa_call_line_synth
                     )
                     del tb
 
@@ -320,7 +324,7 @@
 
 
 def condition_as_parse_action(
-    fn: ParseCondition, message: str = None, fatal: bool = False
+    fn: ParseCondition, message: typing.Optional[str] = None, fatal: bool = False
 ) -> ParseAction:
     """
     Function to convert a simple predicate function that returns ``True`` or ``False``
@@ -353,15 +357,9 @@
     cache_hit_str = "*" if cache_hit else ""
     print(
         (
-            "{}Match {} at loc {}({},{})\n  {}\n  {}^".format(
-                cache_hit_str,
-                expr,
-                loc,
-                lineno(loc, instring),
-                col(loc, instring),
-                line(loc, instring),
-                " " * (col(loc, instring) - 1),
-            )
+            f"{cache_hit_str}Match {expr} at loc {loc}({lineno(loc, instring)},{col(loc, instring)})\n"
+            f"  {line(loc, instring)}\n"
+            f"  {' ' * (col(loc, instring) - 1)}^"
         )
     )
 
@@ -375,7 +373,7 @@
     cache_hit: bool = False,
 ):
     cache_hit_str = "*" if cache_hit else ""
-    print("{}Matched {} -> {}".format(cache_hit_str, expr, toks.as_list()))
+    print(f"{cache_hit_str}Matched {expr} -> {toks.as_list()}")
 
 
 def _default_exception_debug_action(
@@ -386,11 +384,7 @@
     cache_hit: bool = False,
 ):
     cache_hit_str = "*" if cache_hit else ""
-    print(
-        "{}Match {} failed, {} raised: {}".format(
-            cache_hit_str, expr, type(exc).__name__, exc
-        )
-    )
+    print(f"{cache_hit_str}Match {expr} failed, {type(exc).__name__} raised: {exc}")
 
 
 def null_debug_action(*args):
@@ -402,7 +396,7 @@
 
     DEFAULT_WHITE_CHARS: str = " \n\t\r"
     verbose_stacktrace: bool = False
-    _literalStringClass: typing.Optional[type] = None
+    _literalStringClass: type = None  # type: ignore[assignment]
 
     @staticmethod
     def set_default_whitespace_chars(chars: str) -> None:
@@ -447,6 +441,18 @@
         """
         ParserElement._literalStringClass = cls
 
+    @classmethod
+    def using_each(cls, seq, **class_kwargs):
+        """
+        Yields a sequence of class(obj, **class_kwargs) for obj in seq.
+
+        Example::
+
+            LPAR, RPAR, LBRACE, RBRACE, SEMI = Suppress.using_each("(){};")
+
+        """
+        yield from (cls(obj, **class_kwargs) for obj in seq)
+
     class DebugActions(NamedTuple):
         debug_try: typing.Optional[DebugStartAction]
         debug_match: typing.Optional[DebugSuccessAction]
@@ -455,9 +461,9 @@
     def __init__(self, savelist: bool = False):
         self.parseAction: List[ParseAction] = list()
         self.failAction: typing.Optional[ParseFailAction] = None
-        self.customName = None
-        self._defaultName = None
-        self.resultsName = None
+        self.customName: str = None  # type: ignore[assignment]
+        self._defaultName: typing.Optional[str] = None
+        self.resultsName: str = None  # type: ignore[assignment]
         self.saveAsList = savelist
         self.skipWhitespace = True
         self.whiteChars = set(ParserElement.DEFAULT_WHITE_CHARS)
@@ -490,12 +496,29 @@
             base.suppress_warning(Diagnostics.warn_on_parse_using_empty_Forward)
 
             # statement would normally raise a warning, but is now suppressed
-            print(base.parseString("x"))
+            print(base.parse_string("x"))
 
         """
         self.suppress_warnings_.append(warning_type)
         return self
 
+    def visit_all(self):
+        """General-purpose method to yield all expressions and sub-expressions
+        in a grammar. Typically just for internal use.
+        """
+        to_visit = deque([self])
+        seen = set()
+        while to_visit:
+            cur = to_visit.popleft()
+
+            # guard against looping forever through recursive grammars
+            if cur in seen:
+                continue
+            seen.add(cur)
+
+            to_visit.extend(cur.recurse())
+            yield cur
+
     def copy(self) -> "ParserElement":
         """
         Make a copy of this :class:`ParserElement`.  Useful for defining
@@ -585,11 +608,11 @@
                 pdb.set_trace()
                 return _parseMethod(instring, loc, doActions, callPreParse)
 
-            breaker._originalParseMethod = _parseMethod
-            self._parse = breaker
+            breaker._originalParseMethod = _parseMethod  # type: ignore [attr-defined]
+            self._parse = breaker  # type: ignore [assignment]
         else:
             if hasattr(self._parse, "_originalParseMethod"):
-                self._parse = self._parse._originalParseMethod
+                self._parse = self._parse._originalParseMethod  # type: ignore [attr-defined, assignment]
         return self
 
     def set_parse_action(self, *fns: ParseAction, **kwargs) -> "ParserElement":
@@ -601,9 +624,9 @@
         Each parse action ``fn`` is a callable method with 0-3 arguments, called as
         ``fn(s, loc, toks)`` , ``fn(loc, toks)`` , ``fn(toks)`` , or just ``fn()`` , where:
 
-        - s   = the original string being parsed (see note below)
-        - loc = the location of the matching substring
-        - toks = a list of the matched tokens, packaged as a :class:`ParseResults` object
+        - ``s``    = the original string being parsed (see note below)
+        - ``loc``  = the location of the matching substring
+        - ``toks`` = a list of the matched tokens, packaged as a :class:`ParseResults` object
 
         The parsed tokens are passed to the parse action as ParseResults. They can be
         modified in place using list-style append, extend, and pop operations to update
@@ -621,7 +644,7 @@
 
         Optional keyword arguments:
 
-        - call_during_try = (default= ``False``) indicate if parse action should be run during
+        - ``call_during_try`` = (default= ``False``) indicate if parse action should be run during
           lookaheads and alternate testing. For parse actions that have side effects, it is
           important to only call the parse action once it is determined that it is being
           called as part of a successful parse. For parse actions that perform additional
@@ -697,10 +720,10 @@
 
         Optional keyword arguments:
 
-        - message = define a custom message to be used in the raised exception
-        - fatal = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise
+        - ``message`` = define a custom message to be used in the raised exception
+        - ``fatal`` = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise
           ParseException
-        - call_during_try = boolean to indicate if this method should be called during internal tryParse calls,
+        - ``call_during_try`` = boolean to indicate if this method should be called during internal tryParse calls,
           default=False
 
         Example::
@@ -716,7 +739,9 @@
         for fn in fns:
             self.parseAction.append(
                 condition_as_parse_action(
-                    fn, message=kwargs.get("message"), fatal=kwargs.get("fatal", False)
+                    fn,
+                    message=str(kwargs.get("message")),
+                    fatal=bool(kwargs.get("fatal", False)),
                 )
             )
 
@@ -731,30 +756,33 @@
         Fail acton fn is a callable function that takes the arguments
         ``fn(s, loc, expr, err)`` where:
 
-        - s = string being parsed
-        - loc = location where expression match was attempted and failed
-        - expr = the parse expression that failed
-        - err = the exception thrown
+        - ``s`` = string being parsed
+        - ``loc`` = location where expression match was attempted and failed
+        - ``expr`` = the parse expression that failed
+        - ``err`` = the exception thrown
 
         The function returns no value.  It may throw :class:`ParseFatalException`
         if it is desired to stop parsing immediately."""
         self.failAction = fn
         return self
 
-    def _skipIgnorables(self, instring, loc):
+    def _skipIgnorables(self, instring: str, loc: int) -> int:
+        if not self.ignoreExprs:
+            return loc
         exprsFound = True
+        ignore_expr_fns = [e._parse for e in self.ignoreExprs]
         while exprsFound:
             exprsFound = False
-            for e in self.ignoreExprs:
+            for ignore_fn in ignore_expr_fns:
                 try:
                     while 1:
-                        loc, dummy = e._parse(instring, loc)
+                        loc, dummy = ignore_fn(instring, loc)
                         exprsFound = True
                 except ParseException:
                     pass
         return loc
 
-    def preParse(self, instring, loc):
+    def preParse(self, instring: str, loc: int) -> int:
         if self.ignoreExprs:
             loc = self._skipIgnorables(instring, loc)
 
@@ -830,7 +858,7 @@
                 try:
                     for fn in self.parseAction:
                         try:
-                            tokens = fn(instring, tokens_start, ret_tokens)
+                            tokens = fn(instring, tokens_start, ret_tokens)  # type: ignore [call-arg, arg-type]
                         except IndexError as parse_action_exc:
                             exc = ParseException("exception raised in parse action")
                             raise exc from parse_action_exc
@@ -853,7 +881,7 @@
             else:
                 for fn in self.parseAction:
                     try:
-                        tokens = fn(instring, tokens_start, ret_tokens)
+                        tokens = fn(instring, tokens_start, ret_tokens)  # type: ignore [call-arg, arg-type]
                     except IndexError as parse_action_exc:
                         exc = ParseException("exception raised in parse action")
                         raise exc from parse_action_exc
@@ -875,17 +903,24 @@
 
         return loc, ret_tokens
 
-    def try_parse(self, instring: str, loc: int, raise_fatal: bool = False) -> int:
+    def try_parse(
+        self,
+        instring: str,
+        loc: int,
+        *,
+        raise_fatal: bool = False,
+        do_actions: bool = False,
+    ) -> int:
         try:
-            return self._parse(instring, loc, doActions=False)[0]
+            return self._parse(instring, loc, doActions=do_actions)[0]
         except ParseFatalException:
             if raise_fatal:
                 raise
             raise ParseException(instring, loc, self.errmsg, self)
 
-    def can_parse_next(self, instring: str, loc: int) -> bool:
+    def can_parse_next(self, instring: str, loc: int, do_actions: bool = False) -> bool:
         try:
-            self.try_parse(instring, loc)
+            self.try_parse(instring, loc, do_actions=do_actions)
         except (ParseException, IndexError):
             return False
         else:
@@ -897,10 +932,23 @@
         Tuple[int, "Forward", bool], Tuple[int, Union[ParseResults, Exception]]
     ] = {}
 
+    class _CacheType(dict):
+        """
+        class to help type checking
+        """
+
+        not_in_cache: bool
+
+        def get(self, *args):
+            ...
+
+        def set(self, *args):
+            ...
+
     # argument cache for optimizing repeated calls when backtracking through recursive expressions
     packrat_cache = (
-        {}
-    )  # this is set later by enabled_packrat(); this is here so that reset_cache() doesn't fail
+        _CacheType()
+    )  # set later by enable_packrat(); this is here so that reset_cache() doesn't fail
     packrat_cache_lock = RLock()
     packrat_cache_stats = [0, 0]
 
@@ -930,24 +978,25 @@
                 ParserElement.packrat_cache_stats[HIT] += 1
                 if self.debug and self.debugActions.debug_try:
                     try:
-                        self.debugActions.debug_try(instring, loc, self, cache_hit=True)
+                        self.debugActions.debug_try(instring, loc, self, cache_hit=True)  # type: ignore [call-arg]
                     except TypeError:
                         pass
                 if isinstance(value, Exception):
                     if self.debug and self.debugActions.debug_fail:
                         try:
                             self.debugActions.debug_fail(
-                                instring, loc, self, value, cache_hit=True
+                                instring, loc, self, value, cache_hit=True  # type: ignore [call-arg]
                             )
                         except TypeError:
                             pass
                     raise value
 
+                value = cast(Tuple[int, ParseResults, int], value)
                 loc_, result, endloc = value[0], value[1].copy(), value[2]
                 if self.debug and self.debugActions.debug_match:
                     try:
                         self.debugActions.debug_match(
-                            instring, loc_, endloc, self, result, cache_hit=True
+                            instring, loc_, endloc, self, result, cache_hit=True  # type: ignore [call-arg]
                         )
                     except TypeError:
                         pass
@@ -1009,7 +1058,7 @@
 
         Parameters:
 
-        - cache_size_limit - (default=``None``) - memoize at most this many
+        - ``cache_size_limit`` - (default=``None``) - memoize at most this many
           ``Forward`` elements during matching; if ``None`` (the default),
           memoize all ``Forward`` elements.
 
@@ -1022,9 +1071,9 @@
         elif ParserElement._packratEnabled:
             raise RuntimeError("Packrat and Bounded Recursion are not compatible")
         if cache_size_limit is None:
-            ParserElement.recursion_memos = _UnboundedMemo()
+            ParserElement.recursion_memos = _UnboundedMemo()  # type: ignore[assignment]
         elif cache_size_limit > 0:
-            ParserElement.recursion_memos = _LRUMemo(capacity=cache_size_limit)
+            ParserElement.recursion_memos = _LRUMemo(capacity=cache_size_limit)  # type: ignore[assignment]
         else:
             raise NotImplementedError("Memo size of %s" % cache_size_limit)
         ParserElement._left_recursion_enabled = True
@@ -1040,7 +1089,7 @@
 
         Parameters:
 
-        - cache_size_limit - (default= ``128``) - if an integer value is provided
+        - ``cache_size_limit`` - (default= ``128``) - if an integer value is provided
           will limit the size of the packrat cache; if None is passed, then
           the cache size will be unbounded; if 0 is passed, the cache will
           be effectively disabled.
@@ -1070,7 +1119,7 @@
             if cache_size_limit is None:
                 ParserElement.packrat_cache = _UnboundedCache()
             else:
-                ParserElement.packrat_cache = _FifoCache(cache_size_limit)
+                ParserElement.packrat_cache = _FifoCache(cache_size_limit)  # type: ignore[assignment]
             ParserElement._parse = ParserElement._parseCache
 
     def parse_string(
@@ -1088,7 +1137,7 @@
           an object with attributes if the given parser includes results names.
 
         If the input string is required to match the entire grammar, ``parse_all`` flag must be set to ``True``. This
-        is also equivalent to ending the grammar with :class:`StringEnd`().
+        is also equivalent to ending the grammar with :class:`StringEnd`\\ ().
 
         To report proper column numbers, ``parse_string`` operates on a copy of the input string where all tabs are
         converted to spaces (8 spaces per tab, as per the default in ``string.expandtabs``). If the input string
@@ -1198,7 +1247,9 @@
         try:
             while loc <= instrlen and matches < maxMatches:
                 try:
-                    preloc = preparseFn(instring, loc)
+                    preloc: int = preparseFn(instring, loc)
+                    nextLoc: int
+                    tokens: ParseResults
                     nextLoc, tokens = parseFn(instring, preloc, callPreParse=False)
                 except ParseException:
                     loc = preloc + 1
@@ -1352,7 +1403,7 @@
     def __add__(self, other) -> "ParserElement":
         """
         Implementation of ``+`` operator - returns :class:`And`. Adding strings to a :class:`ParserElement`
-        converts them to :class:`Literal`s by default.
+        converts them to :class:`Literal`\\ s by default.
 
         Example::
 
@@ -1364,11 +1415,11 @@
 
             Hello, World! -> ['Hello', ',', 'World', '!']
 
-        ``...`` may be used as a parse expression as a short form of :class:`SkipTo`.
+        ``...`` may be used as a parse expression as a short form of :class:`SkipTo`::
 
             Literal('start') + ... + Literal('end')
 
-        is equivalent to:
+        is equivalent to::
 
             Literal('start') + SkipTo('end')("_skipped*") + Literal('end')
 
@@ -1382,11 +1433,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return And([self, other])
 
     def __radd__(self, other) -> "ParserElement":
@@ -1399,11 +1446,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return other + self
 
     def __sub__(self, other) -> "ParserElement":
@@ -1413,11 +1456,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return self + And._ErrorStop() + other
 
     def __rsub__(self, other) -> "ParserElement":
@@ -1427,11 +1466,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return other - self
 
     def __mul__(self, other) -> "ParserElement":
@@ -1440,11 +1475,12 @@
         ``expr + expr + expr``.  Expressions may also be multiplied by a 2-integer
         tuple, similar to ``{min, max}`` multipliers in regular expressions.  Tuples
         may also include ``None`` as in:
+
         - ``expr*(n, None)`` or ``expr*(n, )`` is equivalent
-             to ``expr*n + ZeroOrMore(expr)``
-             (read as "at least n instances of ``expr``")
+          to ``expr*n + ZeroOrMore(expr)``
+          (read as "at least n instances of ``expr``")
         - ``expr*(None, n)`` is equivalent to ``expr*(0, n)``
-             (read as "0 to n instances of ``expr``")
+          (read as "0 to n instances of ``expr``")
         - ``expr*(None, None)`` is equivalent to ``ZeroOrMore(expr)``
         - ``expr*(1, None)`` is equivalent to ``OneOrMore(expr)``
 
@@ -1477,17 +1513,9 @@
                 minElements, optElements = other
                 optElements -= minElements
             else:
-                raise TypeError(
-                    "cannot multiply ParserElement and ({}) objects".format(
-                        ",".join(type(item).__name__ for item in other)
-                    )
-                )
+                return NotImplemented
         else:
-            raise TypeError(
-                "cannot multiply ParserElement and {} objects".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
 
         if minElements < 0:
             raise ValueError("cannot multiply ParserElement by negative value")
@@ -1531,13 +1559,12 @@
             return _PendingSkip(self, must_skip=True)
 
         if isinstance(other, str_type):
+            # `expr | ""` is equivalent to `Opt(expr)`
+            if other == "":
+                return Opt(self)
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return MatchFirst([self, other])
 
     def __ror__(self, other) -> "ParserElement":
@@ -1547,11 +1574,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return other | self
 
     def __xor__(self, other) -> "ParserElement":
@@ -1561,11 +1584,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return Or([self, other])
 
     def __rxor__(self, other) -> "ParserElement":
@@ -1575,11 +1594,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return other ^ self
 
     def __and__(self, other) -> "ParserElement":
@@ -1589,11 +1604,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return Each([self, other])
 
     def __rand__(self, other) -> "ParserElement":
@@ -1603,11 +1614,7 @@
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
         if not isinstance(other, ParserElement):
-            raise TypeError(
-                "Cannot combine element of type {} with ParserElement".format(
-                    type(other).__name__
-                )
-            )
+            return NotImplemented
         return other & self
 
     def __invert__(self) -> "ParserElement":
@@ -1636,38 +1643,58 @@
 
         ``None`` may be used in place of ``...``.
 
-        Note that ``expr[..., n]`` and ``expr[m, n]``do not raise an exception
-        if more than ``n`` ``expr``s exist in the input stream.  If this behavior is
+        Note that ``expr[..., n]`` and ``expr[m, n]`` do not raise an exception
+        if more than ``n`` ``expr``\\ s exist in the input stream.  If this behavior is
         desired, then write ``expr[..., n] + ~expr``.
+
+        For repetition with a stop_on expression, use slice notation:
+
+        - ``expr[...: end_expr]`` and ``expr[0, ...: end_expr]`` are equivalent to ``ZeroOrMore(expr, stop_on=end_expr)``
+        - ``expr[1, ...: end_expr]`` is equivalent to ``OneOrMore(expr, stop_on=end_expr)``
+
         """
 
+        stop_on_defined = False
+        stop_on = NoMatch()
+        if isinstance(key, slice):
+            key, stop_on = key.start, key.stop
+            if key is None:
+                key = ...
+            stop_on_defined = True
+        elif isinstance(key, tuple) and isinstance(key[-1], slice):
+            key, stop_on = (key[0], key[1].start), key[1].stop
+            stop_on_defined = True
+
         # convert single arg keys to tuples
+        if isinstance(key, str_type):
+            key = (key,)
         try:
-            if isinstance(key, str_type):
-                key = (key,)
             iter(key)
         except TypeError:
             key = (key, key)
 
         if len(key) > 2:
             raise TypeError(
-                "only 1 or 2 index arguments supported ({}{})".format(
-                    key[:5], "... [{}]".format(len(key)) if len(key) > 5 else ""
-                )
+                f"only 1 or 2 index arguments supported ({key[:5]}{f'... [{len(key)}]' if len(key) > 5 else ''})"
             )
 
         # clip to 2 elements
         ret = self * tuple(key[:2])
+        ret = typing.cast(_MultipleMatch, ret)
+
+        if stop_on_defined:
+            ret.stopOn(stop_on)
+
         return ret
 
-    def __call__(self, name: str = None) -> "ParserElement":
+    def __call__(self, name: typing.Optional[str] = None) -> "ParserElement":
         """
         Shortcut for :class:`set_results_name`, with ``list_all_matches=False``.
 
         If ``name`` is given with a trailing ``'*'`` character, then ``list_all_matches`` will be
         passed as ``True``.
 
-        If ``name` is omitted, same as calling :class:`copy`.
+        If ``name`` is omitted, same as calling :class:`copy`.
 
         Example::
 
@@ -1775,17 +1802,18 @@
           should have the signature ``fn(input_string: str, location: int, expression: ParserElement, exception: Exception, cache_hit: bool)``
         """
         self.debugActions = self.DebugActions(
-            start_action or _default_start_debug_action,
-            success_action or _default_success_debug_action,
-            exception_action or _default_exception_debug_action,
+            start_action or _default_start_debug_action,  # type: ignore[truthy-function]
+            success_action or _default_success_debug_action,  # type: ignore[truthy-function]
+            exception_action or _default_exception_debug_action,  # type: ignore[truthy-function]
         )
         self.debug = True
         return self
 
-    def set_debug(self, flag: bool = True) -> "ParserElement":
+    def set_debug(self, flag: bool = True, recurse: bool = False) -> "ParserElement":
         """
         Enable display of debugging messages while doing pattern matching.
         Set ``flag`` to ``True`` to enable, ``False`` to disable.
+        Set ``recurse`` to ``True`` to set the debug flag on this expression and all sub-expressions.
 
         Example::
 
@@ -1819,6 +1847,11 @@
         which makes debugging and exception messages easier to understand - for instance, the default
         name created for the :class:`Word` expression without calling ``set_name`` is ``"W:(A-Za-z)"``.
         """
+        if recurse:
+            for expr in self.visit_all():
+                expr.set_debug(flag, recurse=False)
+            return self
+
         if flag:
             self.set_debug_actions(
                 _default_start_debug_action,
@@ -1836,7 +1869,7 @@
         return self._defaultName
 
     @abstractmethod
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         """
         Child classes must define this method, which defines how the ``default_name`` is set.
         """
@@ -1844,7 +1877,9 @@
     def set_name(self, name: str) -> "ParserElement":
         """
         Define name for this expression, makes debugging and exception messages clearer.
+
         Example::
+
             Word(nums).parse_string("ABC")  # -> Exception: Expected W:(0-9) (at char 0), (line:1, col:1)
             Word(nums).set_name("integer").parse_string("ABC")  # -> Exception: Expected integer (at char 0), (line:1, col:1)
         """
@@ -1870,7 +1905,7 @@
         self._defaultName = None
         return self
 
-    def recurse(self) -> Sequence["ParserElement"]:
+    def recurse(self) -> List["ParserElement"]:
         return []
 
     def _checkRecursion(self, parseElementList):
@@ -1882,6 +1917,11 @@
         """
         Check defined expressions for valid structure, check for infinite recursive definitions.
         """
+        warnings.warn(
+            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
+            DeprecationWarning,
+            stacklevel=2,
+        )
         self._checkRecursion([])
 
     def parse_file(
@@ -1899,8 +1939,10 @@
         """
         parseAll = parseAll or parse_all
         try:
+            file_or_filename = typing.cast(TextIO, file_or_filename)
             file_contents = file_or_filename.read()
         except AttributeError:
+            file_or_filename = typing.cast(str, file_or_filename)
             with open(file_or_filename, "r", encoding=encoding) as f:
                 file_contents = f.read()
         try:
@@ -1932,6 +1974,7 @@
         inline microtests of sub expressions while building up larger parser.
 
         Parameters:
+
         - ``test_string`` - to test against this expression for a match
         - ``parse_all`` - (default= ``True``) - flag to pass to :class:`parse_string` when running tests
 
@@ -1955,7 +1998,7 @@
         full_dump: bool = True,
         print_results: bool = True,
         failure_tests: bool = False,
-        post_parse: Callable[[str, ParseResults], str] = None,
+        post_parse: typing.Optional[Callable[[str, ParseResults], str]] = None,
         file: typing.Optional[TextIO] = None,
         with_line_numbers: bool = False,
         *,
@@ -1963,7 +2006,7 @@
         fullDump: bool = True,
         printResults: bool = True,
         failureTests: bool = False,
-        postParse: Callable[[str, ParseResults], str] = None,
+        postParse: typing.Optional[Callable[[str, ParseResults], str]] = None,
     ) -> Tuple[bool, List[Tuple[str, Union[ParseResults, Exception]]]]:
         """
         Execute the parse expression on a series of test strings, showing each
@@ -1971,6 +2014,7 @@
         run a parse expression against a list of sample strings.
 
         Parameters:
+
         - ``tests`` - a list of separate test strings, or a multiline string of test strings
         - ``parse_all`` - (default= ``True``) - flag to pass to :class:`parse_string` when running tests
         - ``comment`` - (default= ``'#'``) - expression for indicating embedded comments in the test
@@ -2067,22 +2111,27 @@
         failureTests = failureTests or failure_tests
         postParse = postParse or post_parse
         if isinstance(tests, str_type):
+            tests = typing.cast(str, tests)
             line_strip = type(tests).strip
             tests = [line_strip(test_line) for test_line in tests.rstrip().splitlines()]
-        if isinstance(comment, str_type):
-            comment = Literal(comment)
+        comment_specified = comment is not None
+        if comment_specified:
+            if isinstance(comment, str_type):
+                comment = typing.cast(str, comment)
+                comment = Literal(comment)
+        comment = typing.cast(ParserElement, comment)
         if file is None:
             file = sys.stdout
         print_ = file.write
 
         result: Union[ParseResults, Exception]
-        allResults = []
-        comments = []
+        allResults: List[Tuple[str, Union[ParseResults, Exception]]] = []
+        comments: List[str] = []
         success = True
         NL = Literal(r"\n").add_parse_action(replace_with("\n")).ignore(quoted_string)
         BOM = "\ufeff"
         for t in tests:
-            if comment is not None and comment.matches(t, False) or comments and not t:
+            if comment_specified and comment.matches(t, False) or comments and not t:
                 comments.append(
                     pyparsing_test.with_line_numbers(t) if with_line_numbers else t
                 )
@@ -2107,7 +2156,7 @@
                 success = success and failureTests
                 result = pe
             except Exception as exc:
-                out.append("FAIL-EXCEPTION: {}: {}".format(type(exc).__name__, exc))
+                out.append(f"FAIL-EXCEPTION: {type(exc).__name__}: {exc}")
                 if ParserElement.verbose_stacktrace:
                     out.extend(traceback.format_tb(exc.__traceback__))
                 success = success and failureTests
@@ -2127,9 +2176,7 @@
                     except Exception as e:
                         out.append(result.dump(full=fullDump))
                         out.append(
-                            "{} failed: {}: {}".format(
-                                postParse.__name__, type(e).__name__, e
-                            )
+                            f"{postParse.__name__} failed: {type(e).__name__}: {e}"
                         )
                 else:
                     out.append(result.dump(full=fullDump))
@@ -2148,19 +2195,28 @@
         vertical: int = 3,
         show_results_names: bool = False,
         show_groups: bool = False,
+        embed: bool = False,
         **kwargs,
     ) -> None:
         """
         Create a railroad diagram for the parser.
 
         Parameters:
-        - output_html (str or file-like object) - output target for generated
+
+        - ``output_html`` (str or file-like object) - output target for generated
           diagram HTML
-        - vertical (int) - threshold for formatting multiple alternatives vertically
+        - ``vertical`` (int) - threshold for formatting multiple alternatives vertically
           instead of horizontally (default=3)
-        - show_results_names - bool flag whether diagram should show annotations for
+        - ``show_results_names`` - bool flag whether diagram should show annotations for
           defined results names
-        - show_groups - bool flag whether groups should be highlighted with an unlabeled surrounding box
+        - ``show_groups`` - bool flag whether groups should be highlighted with an unlabeled surrounding box
+        - ``embed`` - bool flag whether generated HTML should omit <HEAD>, <BODY>, and <DOCTYPE> tags to embed
+          the resulting HTML in an enclosing HTML source
+        - ``head`` - str containing additional HTML to insert into the <HEAD> section of the generated code;
+          can be used to insert custom CSS styling
+        - ``body`` - str containing additional HTML to insert at the beginning of the <BODY> section of the
+          generated code
+
         Additional diagram-formatting keyword arguments can also be included;
         see railroad.Diagram class.
         """
@@ -2183,38 +2239,93 @@
         )
         if isinstance(output_html, (str, Path)):
             with open(output_html, "w", encoding="utf-8") as diag_file:
-                diag_file.write(railroad_to_html(railroad))
+                diag_file.write(railroad_to_html(railroad, embed=embed, **kwargs))
         else:
             # we were passed a file-like object, just write to it
-            output_html.write(railroad_to_html(railroad))
+            output_html.write(railroad_to_html(railroad, embed=embed, **kwargs))
+
+    # Compatibility synonyms
+    # fmt: off
+    @staticmethod
+    @replaced_by_pep8(inline_literals_using)
+    def inlineLiteralsUsing(): ...
+
+    @staticmethod
+    @replaced_by_pep8(set_default_whitespace_chars)
+    def setDefaultWhitespaceChars(): ...
+
+    @replaced_by_pep8(set_results_name)
+    def setResultsName(self): ...
+
+    @replaced_by_pep8(set_break)
+    def setBreak(self): ...
+
+    @replaced_by_pep8(set_parse_action)
+    def setParseAction(self): ...
+
+    @replaced_by_pep8(add_parse_action)
+    def addParseAction(self): ...
+
+    @replaced_by_pep8(add_condition)
+    def addCondition(self): ...
+
+    @replaced_by_pep8(set_fail_action)
+    def setFailAction(self): ...
+
+    @replaced_by_pep8(try_parse)
+    def tryParse(self): ...
+
+    @staticmethod
+    @replaced_by_pep8(enable_left_recursion)
+    def enableLeftRecursion(): ...
+
+    @staticmethod
+    @replaced_by_pep8(enable_packrat)
+    def enablePackrat(): ...
+
+    @replaced_by_pep8(parse_string)
+    def parseString(self): ...
+
+    @replaced_by_pep8(scan_string)
+    def scanString(self): ...
+
+    @replaced_by_pep8(transform_string)
+    def transformString(self): ...
+
+    @replaced_by_pep8(search_string)
+    def searchString(self): ...
+
+    @replaced_by_pep8(ignore_whitespace)
+    def ignoreWhitespace(self): ...
+
+    @replaced_by_pep8(leave_whitespace)
+    def leaveWhitespace(self): ...
+
+    @replaced_by_pep8(set_whitespace_chars)
+    def setWhitespaceChars(self): ...
+
+    @replaced_by_pep8(parse_with_tabs)
+    def parseWithTabs(self): ...
+
+    @replaced_by_pep8(set_debug_actions)
+    def setDebugActions(self): ...
+
+    @replaced_by_pep8(set_debug)
+    def setDebug(self): ...
+
+    @replaced_by_pep8(set_name)
+    def setName(self): ...
+
+    @replaced_by_pep8(parse_file)
+    def parseFile(self): ...
+
+    @replaced_by_pep8(run_tests)
+    def runTests(self): ...
 
-    setDefaultWhitespaceChars = set_default_whitespace_chars
-    inlineLiteralsUsing = inline_literals_using
-    setResultsName = set_results_name
-    setBreak = set_break
-    setParseAction = set_parse_action
-    addParseAction = add_parse_action
-    addCondition = add_condition
-    setFailAction = set_fail_action
-    tryParse = try_parse
     canParseNext = can_parse_next
     resetCache = reset_cache
-    enableLeftRecursion = enable_left_recursion
-    enablePackrat = enable_packrat
-    parseString = parse_string
-    scanString = scan_string
-    searchString = search_string
-    transformString = transform_string
-    setWhitespaceChars = set_whitespace_chars
-    parseWithTabs = parse_with_tabs
-    setDebugActions = set_debug_actions
-    setDebug = set_debug
     defaultName = default_name
-    setName = set_name
-    parseFile = parse_file
-    runTests = run_tests
-    ignoreWhitespace = ignore_whitespace
-    leaveWhitespace = leave_whitespace
+    # fmt: on
 
 
 class _PendingSkip(ParserElement):
@@ -2225,7 +2336,7 @@
         self.anchor = expr
         self.must_skip = must_skip
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return str(self.anchor + Empty()).replace("Empty", "...")
 
     def __add__(self, other) -> "ParserElement":
@@ -2266,21 +2377,10 @@
     def __init__(self):
         super().__init__(savelist=False)
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return type(self).__name__
 
 
-class Empty(Token):
-    """
-    An empty token, will always match.
-    """
-
-    def __init__(self):
-        super().__init__()
-        self.mayReturnEmpty = True
-        self.mayIndexError = False
-
-
 class NoMatch(Token):
     """
     A token that will never match.
@@ -2312,25 +2412,33 @@
     use :class:`Keyword` or :class:`CaselessKeyword`.
     """
 
+    def __new__(cls, match_string: str = "", *, matchString: str = ""):
+        # Performance tuning: select a subclass with optimized parseImpl
+        if cls is Literal:
+            match_string = matchString or match_string
+            if not match_string:
+                return super().__new__(Empty)
+            if len(match_string) == 1:
+                return super().__new__(_SingleCharLiteral)
+
+        # Default behavior
+        return super().__new__(cls)
+
+    # Needed to make copy.copy() work correctly if we customize __new__
+    def __getnewargs__(self):
+        return (self.match,)
+
     def __init__(self, match_string: str = "", *, matchString: str = ""):
         super().__init__()
         match_string = matchString or match_string
         self.match = match_string
         self.matchLen = len(match_string)
-        try:
-            self.firstMatchChar = match_string[0]
-        except IndexError:
-            raise ValueError("null string passed to Literal; use Empty() instead")
+        self.firstMatchChar = match_string[:1]
         self.errmsg = "Expected " + self.name
         self.mayReturnEmpty = False
         self.mayIndexError = False
 
-        # Performance tuning: modify __class__ to select
-        # a parseImpl optimized for single-character check
-        if self.matchLen == 1 and type(self) is Literal:
-            self.__class__ = _SingleCharLiteral
-
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return repr(self.match)
 
     def parseImpl(self, instring, loc, doActions=True):
@@ -2341,6 +2449,23 @@
         raise ParseException(instring, loc, self.errmsg, self)
 
 
+class Empty(Literal):
+    """
+    An empty token, will always match.
+    """
+
+    def __init__(self, match_string="", *, matchString=""):
+        super().__init__("")
+        self.mayReturnEmpty = True
+        self.mayIndexError = False
+
+    def _generateDefaultName(self) -> str:
+        return "Empty"
+
+    def parseImpl(self, instring, loc, doActions=True):
+        return loc, []
+
+
 class _SingleCharLiteral(Literal):
     def parseImpl(self, instring, loc, doActions=True):
         if instring[loc] == self.firstMatchChar:
@@ -2354,8 +2479,8 @@
 class Keyword(Token):
     """
     Token to exactly match a specified string as a keyword, that is,
-    it must be immediately followed by a non-keyword character.  Compare
-    with :class:`Literal`:
+    it must be immediately preceded and followed by whitespace or
+    non-keyword characters. Compare with :class:`Literal`:
 
     - ``Literal("if")`` will match the leading ``'if'`` in
       ``'ifAndOnlyIf'``.
@@ -2365,7 +2490,7 @@
     Accepts two optional constructor arguments in addition to the
     keyword string:
 
-    - ``identChars`` is a string of characters that would be valid
+    - ``ident_chars`` is a string of characters that would be valid
       identifier characters, defaulting to all alphanumerics + "_" and
       "$"
     - ``caseless`` allows case-insensitive matching, default is ``False``.
@@ -2400,7 +2525,7 @@
             self.firstMatchChar = match_string[0]
         except IndexError:
             raise ValueError("null string passed to Keyword; use Empty() instead")
-        self.errmsg = "Expected {} {}".format(type(self).__name__, self.name)
+        self.errmsg = f"Expected {type(self).__name__} {self.name}"
         self.mayReturnEmpty = False
         self.mayIndexError = False
         self.caseless = caseless
@@ -2409,7 +2534,7 @@
             identChars = identChars.upper()
         self.identChars = set(identChars)
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return repr(self.match)
 
     def parseImpl(self, instring, loc, doActions=True):
@@ -2559,7 +2684,7 @@
     def __init__(
         self,
         match_string: str,
-        max_mismatches: int = None,
+        max_mismatches: typing.Optional[int] = None,
         *,
         maxMismatches: int = 1,
         caseless=False,
@@ -2568,15 +2693,13 @@
         super().__init__()
         self.match_string = match_string
         self.maxMismatches = maxMismatches
-        self.errmsg = "Expected {!r} (with up to {} mismatches)".format(
-            self.match_string, self.maxMismatches
-        )
+        self.errmsg = f"Expected {self.match_string!r} (with up to {self.maxMismatches} mismatches)"
         self.caseless = caseless
         self.mayIndexError = False
         self.mayReturnEmpty = False
 
-    def _generateDefaultName(self):
-        return "{}:{!r}".format(type(self).__name__, self.match_string)
+    def _generateDefaultName(self) -> str:
+        return f"{type(self).__name__}:{self.match_string!r}"
 
     def parseImpl(self, instring, loc, doActions=True):
         start = loc
@@ -2612,7 +2735,9 @@
 
 class Word(Token):
     """Token for matching words composed of allowed character sets.
+
     Parameters:
+
     - ``init_chars`` - string of all characters that should be used to
       match as a word; "ABC" will match "AAA", "ABAB", "CBAC", etc.;
       if ``body_chars`` is also specified, then this is the string of
@@ -2697,26 +2822,24 @@
         super().__init__()
         if not initChars:
             raise ValueError(
-                "invalid {}, initChars cannot be empty string".format(
-                    type(self).__name__
-                )
+                f"invalid {type(self).__name__}, initChars cannot be empty string"
             )
 
-        initChars = set(initChars)
-        self.initChars = initChars
+        initChars_set = set(initChars)
         if excludeChars:
-            excludeChars = set(excludeChars)
-            initChars -= excludeChars
+            excludeChars_set = set(excludeChars)
+            initChars_set -= excludeChars_set
             if bodyChars:
-                bodyChars = set(bodyChars) - excludeChars
-        self.initCharsOrig = "".join(sorted(initChars))
+                bodyChars = "".join(set(bodyChars) - excludeChars_set)
+        self.initChars = initChars_set
+        self.initCharsOrig = "".join(sorted(initChars_set))
 
         if bodyChars:
-            self.bodyCharsOrig = "".join(sorted(bodyChars))
             self.bodyChars = set(bodyChars)
+            self.bodyCharsOrig = "".join(sorted(bodyChars))
         else:
-            self.bodyCharsOrig = "".join(sorted(initChars))
-            self.bodyChars = set(initChars)
+            self.bodyChars = initChars_set
+            self.bodyCharsOrig = self.initCharsOrig
 
         self.maxSpecified = max > 0
 
@@ -2725,6 +2848,11 @@
                 "cannot specify a minimum length < 1; use Opt(Word()) if zero-length word is permitted"
             )
 
+        if self.maxSpecified and min > max:
+            raise ValueError(
+                f"invalid args, if min and max both specified min must be <= max (min={min}, max={max})"
+            )
+
         self.minLen = min
 
         if max > 0:
@@ -2733,62 +2861,66 @@
             self.maxLen = _MAX_INT
 
         if exact > 0:
+            min = max = exact
             self.maxLen = exact
             self.minLen = exact
 
         self.errmsg = "Expected " + self.name
         self.mayIndexError = False
         self.asKeyword = asKeyword
+        if self.asKeyword:
+            self.errmsg += " as a keyword"
 
         # see if we can make a regex for this Word
-        if " " not in self.initChars | self.bodyChars and (min == 1 and exact == 0):
+        if " " not in (self.initChars | self.bodyChars):
+            if len(self.initChars) == 1:
+                re_leading_fragment = re.escape(self.initCharsOrig)
+            else:
+                re_leading_fragment = f"[{_collapse_string_to_ranges(self.initChars)}]"
+
             if self.bodyChars == self.initChars:
                 if max == 0:
                     repeat = "+"
                 elif max == 1:
                     repeat = ""
                 else:
-                    repeat = "{{{},{}}}".format(
-                        self.minLen, "" if self.maxLen == _MAX_INT else self.maxLen
-                    )
-                self.reString = "[{}]{}".format(
-                    _collapse_string_to_ranges(self.initChars),
-                    repeat,
-                )
-            elif len(self.initChars) == 1:
-                if max == 0:
-                    repeat = "*"
-                else:
-                    repeat = "{{0,{}}}".format(max - 1)
-                self.reString = "{}[{}]{}".format(
-                    re.escape(self.initCharsOrig),
-                    _collapse_string_to_ranges(self.bodyChars),
-                    repeat,
-                )
+                    if self.minLen != self.maxLen:
+                        repeat = f"{{{self.minLen},{'' if self.maxLen == _MAX_INT else self.maxLen}}}"
+                    else:
+                        repeat = f"{{{self.minLen}}}"
+                self.reString = f"{re_leading_fragment}{repeat}"
             else:
-                if max == 0:
-                    repeat = "*"
-                elif max == 2:
+                if max == 1:
+                    re_body_fragment = ""
                     repeat = ""
                 else:
-                    repeat = "{{0,{}}}".format(max - 1)
-                self.reString = "[{}][{}]{}".format(
-                    _collapse_string_to_ranges(self.initChars),
-                    _collapse_string_to_ranges(self.bodyChars),
-                    repeat,
+                    re_body_fragment = f"[{_collapse_string_to_ranges(self.bodyChars)}]"
+                    if max == 0:
+                        repeat = "*"
+                    elif max == 2:
+                        repeat = "?" if min <= 1 else ""
+                    else:
+                        if min != max:
+                            repeat = f"{{{min - 1 if min > 0 else 0},{max - 1}}}"
+                        else:
+                            repeat = f"{{{min - 1 if min > 0 else 0}}}"
+
+                self.reString = (
+                    f"{re_leading_fragment}" f"{re_body_fragment}" f"{repeat}"
                 )
+
             if self.asKeyword:
-                self.reString = r"\b" + self.reString + r"\b"
+                self.reString = rf"\b{self.reString}\b"
 
             try:
                 self.re = re.compile(self.reString)
             except re.error:
-                self.re = None
+                self.re = None  # type: ignore[assignment]
             else:
                 self.re_match = self.re.match
-                self.__class__ = _WordRegex
+                self.parseImpl = self.parseImpl_regex  # type: ignore[assignment]
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         def charsAsStr(s):
             max_repr_len = 16
             s = _collapse_string_to_ranges(s, re_escape=False)
@@ -2798,11 +2930,9 @@
                 return s
 
         if self.initChars != self.bodyChars:
-            base = "W:({}, {})".format(
-                charsAsStr(self.initChars), charsAsStr(self.bodyChars)
-            )
+            base = f"W:({charsAsStr(self.initChars)}, {charsAsStr(self.bodyChars)})"
         else:
-            base = "W:({})".format(charsAsStr(self.initChars))
+            base = f"W:({charsAsStr(self.initChars)})"
 
         # add length specification
         if self.minLen > 1 or self.maxLen != _MAX_INT:
@@ -2810,11 +2940,11 @@
                 if self.minLen == 1:
                     return base[2:]
                 else:
-                    return base + "{{{}}}".format(self.minLen)
+                    return base + f"{{{self.minLen}}}"
             elif self.maxLen == _MAX_INT:
-                return base + "{{{},...}}".format(self.minLen)
+                return base + f"{{{self.minLen},...}}"
             else:
-                return base + "{{{},{}}}".format(self.minLen, self.maxLen)
+                return base + f"{{{self.minLen},{self.maxLen}}}"
         return base
 
     def parseImpl(self, instring, loc, doActions=True):
@@ -2849,9 +2979,7 @@
 
         return loc, instring[start:loc]
 
-
-class _WordRegex(Word):
-    def parseImpl(self, instring, loc, doActions=True):
+    def parseImpl_regex(self, instring, loc, doActions=True):
         result = self.re_match(instring, loc)
         if not result:
             raise ParseException(instring, loc, self.errmsg, self)
@@ -2860,7 +2988,7 @@
         return loc, result.group()
 
 
-class Char(_WordRegex):
+class Char(Word):
     """A short-cut class for defining :class:`Word` ``(characters, exact=1)``,
     when defining a match of any single character in a string of
     characters.
@@ -2878,13 +3006,8 @@
         asKeyword = asKeyword or as_keyword
         excludeChars = excludeChars or exclude_chars
         super().__init__(
-            charset, exact=1, asKeyword=asKeyword, excludeChars=excludeChars
+            charset, exact=1, as_keyword=asKeyword, exclude_chars=excludeChars
         )
-        self.reString = "[{}]".format(_collapse_string_to_ranges(self.initChars))
-        if asKeyword:
-            self.reString = r"\b{}\b".format(self.reString)
-        self.re = re.compile(self.reString)
-        self.re_match = self.re.match
 
 
 class Regex(Token):
@@ -2954,9 +3077,9 @@
         self.asGroupList = asGroupList
         self.asMatch = asMatch
         if self.asGroupList:
-            self.parseImpl = self.parseImplAsGroupList
+            self.parseImpl = self.parseImplAsGroupList  # type: ignore [assignment]
         if self.asMatch:
-            self.parseImpl = self.parseImplAsMatch
+            self.parseImpl = self.parseImplAsMatch  # type: ignore [assignment]
 
     @cached_property
     def re(self):
@@ -2966,9 +3089,7 @@
             try:
                 return re.compile(self.pattern, self.flags)
             except re.error:
-                raise ValueError(
-                    "invalid pattern ({!r}) passed to Regex".format(self.pattern)
-                )
+                raise ValueError(f"invalid pattern ({self.pattern!r}) passed to Regex")
 
     @cached_property
     def re_match(self):
@@ -2978,7 +3099,7 @@
     def mayReturnEmpty(self):
         return self.re_match("") is not None
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "Re:({})".format(repr(self.pattern).replace("\\\\", "\\"))
 
     def parseImpl(self, instring, loc, doActions=True):
@@ -3024,10 +3145,12 @@
             # prints "<h1>main title</h1>"
         """
         if self.asGroupList:
-            raise TypeError("cannot use sub() with Regex(asGroupList=True)")
+            raise TypeError("cannot use sub() with Regex(as_group_list=True)")
 
         if self.asMatch and callable(repl):
-            raise TypeError("cannot use sub() with a callable with Regex(asMatch=True)")
+            raise TypeError(
+                "cannot use sub() with a callable with Regex(as_match=True)"
+            )
 
         if self.asMatch:
 
@@ -3081,7 +3204,7 @@
         [['This is the "quote"']]
         [['This is the quote with "embedded" quotes']]
     """
-    ws_map = ((r"\t", "\t"), (r"\n", "\n"), (r"\f", "\f"), (r"\r", "\r"))
+    ws_map = dict(((r"\t", "\t"), (r"\n", "\n"), (r"\f", "\f"), (r"\r", "\r")))
 
     def __init__(
         self,
@@ -3120,57 +3243,54 @@
         else:
             endQuoteChar = endQuoteChar.strip()
             if not endQuoteChar:
-                raise ValueError("endQuoteChar cannot be the empty string")
+                raise ValueError("end_quote_char cannot be the empty string")
 
-        self.quoteChar = quote_char
-        self.quoteCharLen = len(quote_char)
-        self.firstQuoteChar = quote_char[0]
-        self.endQuoteChar = endQuoteChar
-        self.endQuoteCharLen = len(endQuoteChar)
-        self.escChar = escChar
-        self.escQuote = escQuote
-        self.unquoteResults = unquoteResults
-        self.convertWhitespaceEscapes = convertWhitespaceEscapes
+        self.quoteChar: str = quote_char
+        self.quoteCharLen: int = len(quote_char)
+        self.firstQuoteChar: str = quote_char[0]
+        self.endQuoteChar: str = endQuoteChar
+        self.endQuoteCharLen: int = len(endQuoteChar)
+        self.escChar: str = escChar or ""
+        self.escQuote: str = escQuote or ""
+        self.unquoteResults: bool = unquoteResults
+        self.convertWhitespaceEscapes: bool = convertWhitespaceEscapes
+        self.multiline = multiline
 
         sep = ""
         inner_pattern = ""
 
         if escQuote:
-            inner_pattern += r"{}(?:{})".format(sep, re.escape(escQuote))
+            inner_pattern += rf"{sep}(?:{re.escape(escQuote)})"
             sep = "|"
 
         if escChar:
-            inner_pattern += r"{}(?:{}.)".format(sep, re.escape(escChar))
+            inner_pattern += rf"{sep}(?:{re.escape(escChar)}.)"
             sep = "|"
-            self.escCharReplacePattern = re.escape(self.escChar) + "(.)"
+            self.escCharReplacePattern = re.escape(escChar) + "(.)"
 
         if len(self.endQuoteChar) > 1:
             inner_pattern += (
-                "{}(?:".format(sep)
+                f"{sep}(?:"
                 + "|".join(
-                    "(?:{}(?!{}))".format(
-                        re.escape(self.endQuoteChar[:i]),
-                        re.escape(self.endQuoteChar[i:]),
-                    )
+                    f"(?:{re.escape(self.endQuoteChar[:i])}(?!{re.escape(self.endQuoteChar[i:])}))"
                     for i in range(len(self.endQuoteChar) - 1, 0, -1)
                 )
                 + ")"
             )
             sep = "|"
 
+        self.flags = re.RegexFlag(0)
+
         if multiline:
             self.flags = re.MULTILINE | re.DOTALL
-            inner_pattern += r"{}(?:[^{}{}])".format(
-                sep,
-                _escape_regex_range_chars(self.endQuoteChar[0]),
-                (_escape_regex_range_chars(escChar) if escChar is not None else ""),
+            inner_pattern += (
+                rf"{sep}(?:[^{_escape_regex_range_chars(self.endQuoteChar[0])}"
+                rf"{(_escape_regex_range_chars(escChar) if escChar is not None else '')}])"
             )
         else:
-            self.flags = 0
-            inner_pattern += r"{}(?:[^{}\n\r{}])".format(
-                sep,
-                _escape_regex_range_chars(self.endQuoteChar[0]),
-                (_escape_regex_range_chars(escChar) if escChar is not None else ""),
+            inner_pattern += (
+                rf"{sep}(?:[^{_escape_regex_range_chars(self.endQuoteChar[0])}\n\r"
+                rf"{(_escape_regex_range_chars(escChar) if escChar is not None else '')}])"
             )
 
         self.pattern = "".join(
@@ -3183,26 +3303,33 @@
             ]
         )
 
+        if self.unquoteResults:
+            if self.convertWhitespaceEscapes:
+                self.unquote_scan_re = re.compile(
+                    rf"({'|'.join(re.escape(k) for k in self.ws_map)})|({re.escape(self.escChar)}.)|(\n|.)",
+                    flags=self.flags,
+                )
+            else:
+                self.unquote_scan_re = re.compile(
+                    rf"({re.escape(self.escChar)}.)|(\n|.)", flags=self.flags
+                )
+
         try:
             self.re = re.compile(self.pattern, self.flags)
             self.reString = self.pattern
             self.re_match = self.re.match
         except re.error:
-            raise ValueError(
-                "invalid pattern {!r} passed to Regex".format(self.pattern)
-            )
+            raise ValueError(f"invalid pattern {self.pattern!r} passed to Regex")
 
         self.errmsg = "Expected " + self.name
         self.mayIndexError = False
         self.mayReturnEmpty = True
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         if self.quoteChar == self.endQuoteChar and isinstance(self.quoteChar, str_type):
-            return "string enclosed in {!r}".format(self.quoteChar)
+            return f"string enclosed in {self.quoteChar!r}"
 
-        return "quoted string, starting with {} ending with {}".format(
-            self.quoteChar, self.endQuoteChar
-        )
+        return f"quoted string, starting with {self.quoteChar} ending with {self.endQuoteChar}"
 
     def parseImpl(self, instring, loc, doActions=True):
         result = (
@@ -3217,19 +3344,24 @@
         ret = result.group()
 
         if self.unquoteResults:
-
             # strip off quotes
             ret = ret[self.quoteCharLen : -self.endQuoteCharLen]
 
             if isinstance(ret, str_type):
-                # replace escaped whitespace
-                if "\\" in ret and self.convertWhitespaceEscapes:
-                    for wslit, wschar in self.ws_map:
-                        ret = ret.replace(wslit, wschar)
-
-                # replace escaped characters
-                if self.escChar:
-                    ret = re.sub(self.escCharReplacePattern, r"\g<1>", ret)
+                if self.convertWhitespaceEscapes:
+                    ret = "".join(
+                        self.ws_map[match.group(1)]
+                        if match.group(1)
+                        else match.group(2)[-1]
+                        if match.group(2)
+                        else match.group(3)
+                        for match in self.unquote_scan_re.finditer(ret)
+                    )
+                else:
+                    ret = "".join(
+                        match.group(1)[-1] if match.group(1) else match.group(2)
+                        for match in self.unquote_scan_re.finditer(ret)
+                    )
 
                 # replace escaped quotes
                 if self.escQuote:
@@ -3252,7 +3384,7 @@
 
         # define a comma-separated-value as anything that is not a ','
         csv_value = CharsNotIn(',')
-        print(delimited_list(csv_value).parse_string("dkls,lsdkjf,s12 34,@!#,213"))
+        print(DelimitedList(csv_value).parse_string("dkls,lsdkjf,s12 34,@!#,213"))
 
     prints::
 
@@ -3294,12 +3426,12 @@
         self.mayReturnEmpty = self.minLen == 0
         self.mayIndexError = False
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         not_chars_str = _collapse_string_to_ranges(self.notChars)
         if len(not_chars_str) > 16:
-            return "!W:({}...)".format(self.notChars[: 16 - 3])
+            return f"!W:({self.notChars[: 16 - 3]}...)"
         else:
-            return "!W:({})".format(self.notChars)
+            return f"!W:({self.notChars})"
 
     def parseImpl(self, instring, loc, doActions=True):
         notchars = self.notCharsSet
@@ -3376,7 +3508,7 @@
             self.maxLen = exact
             self.minLen = exact
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "".join(White.whiteStrs[c] for c in self.matchWhite)
 
     def parseImpl(self, instring, loc, doActions=True):
@@ -3411,7 +3543,7 @@
         super().__init__()
         self.col = colno
 
-    def preParse(self, instring, loc):
+    def preParse(self, instring: str, loc: int) -> int:
         if col(loc, instring) != self.col:
             instrlen = len(instring)
             if self.ignoreExprs:
@@ -3446,7 +3578,7 @@
         B AAA and definitely not this one
         '''
 
-        for t in (LineStart() + 'AAA' + restOfLine).search_string(test):
+        for t in (LineStart() + 'AAA' + rest_of_line).search_string(test):
             print(t)
 
     prints::
@@ -3464,7 +3596,7 @@
         self.skipper = Empty().set_whitespace_chars(self.whiteChars)
         self.errmsg = "Expected start of line"
 
-    def preParse(self, instring, loc):
+    def preParse(self, instring: str, loc: int) -> int:
         if loc == 0:
             return loc
         else:
@@ -3624,7 +3756,7 @@
                 self.exprs = [exprs]
         self.callPreparse = False
 
-    def recurse(self) -> Sequence[ParserElement]:
+    def recurse(self) -> List[ParserElement]:
         return self.exprs[:]
 
     def append(self, other) -> ParserElement:
@@ -3669,8 +3801,8 @@
                 e.ignore(self.ignoreExprs[-1])
         return self
 
-    def _generateDefaultName(self):
-        return "{}:({})".format(self.__class__.__name__, str(self.exprs))
+    def _generateDefaultName(self) -> str:
+        return f"{self.__class__.__name__}:({str(self.exprs)})"
 
     def streamline(self) -> ParserElement:
         if self.streamlined:
@@ -3714,6 +3846,11 @@
         return self
 
     def validate(self, validateTrace=None) -> None:
+        warnings.warn(
+            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
+            DeprecationWarning,
+            stacklevel=2,
+        )
         tmp = (validateTrace if validateTrace is not None else [])[:] + [self]
         for e in self.exprs:
             e.validate(tmp)
@@ -3721,6 +3858,7 @@
 
     def copy(self) -> ParserElement:
         ret = super().copy()
+        ret = typing.cast(ParseExpression, ret)
         ret.exprs = [e.copy() for e in self.exprs]
         return ret
 
@@ -3750,8 +3888,14 @@
 
         return super()._setResultsName(name, listAllMatches)
 
-    ignoreWhitespace = ignore_whitespace
-    leaveWhitespace = leave_whitespace
+    # Compatibility synonyms
+    # fmt: off
+    @replaced_by_pep8(leave_whitespace)
+    def leaveWhitespace(self): ...
+
+    @replaced_by_pep8(ignore_whitespace)
+    def ignoreWhitespace(self): ...
+    # fmt: on
 
 
 class And(ParseExpression):
@@ -3777,7 +3921,7 @@
             super().__init__(*args, **kwargs)
             self.leave_whitespace()
 
-        def _generateDefaultName(self):
+        def _generateDefaultName(self) -> str:
             return "-"
 
     def __init__(
@@ -3789,7 +3933,9 @@
             for i, expr in enumerate(exprs):
                 if expr is Ellipsis:
                     if i < len(exprs) - 1:
-                        skipto_arg: ParserElement = (Empty() + exprs[i + 1]).exprs[-1]
+                        skipto_arg: ParserElement = typing.cast(
+                            ParseExpression, (Empty() + exprs[i + 1])
+                        ).exprs[-1]
                         tmp.append(SkipTo(skipto_arg)("_skipped*"))
                     else:
                         raise Exception(
@@ -3822,8 +3968,9 @@
                 and isinstance(e.exprs[-1], _PendingSkip)
                 for e in self.exprs[:-1]
             ):
+                deleted_expr_marker = NoMatch()
                 for i, e in enumerate(self.exprs[:-1]):
-                    if e is None:
+                    if e is deleted_expr_marker:
                         continue
                     if (
                         isinstance(e, ParseExpression)
@@ -3831,17 +3978,19 @@
                         and isinstance(e.exprs[-1], _PendingSkip)
                     ):
                         e.exprs[-1] = e.exprs[-1] + self.exprs[i + 1]
-                        self.exprs[i + 1] = None
-                self.exprs = [e for e in self.exprs if e is not None]
+                        self.exprs[i + 1] = deleted_expr_marker
+                self.exprs = [e for e in self.exprs if e is not deleted_expr_marker]
 
         super().streamline()
 
         # link any IndentedBlocks to the prior expression
+        prev: ParserElement
+        cur: ParserElement
         for prev, cur in zip(self.exprs, self.exprs[1:]):
             # traverse cur or any first embedded expr of cur looking for an IndentedBlock
             # (but watch out for recursive grammar)
             seen = set()
-            while cur:
+            while True:
                 if id(cur) in seen:
                     break
                 seen.add(id(cur))
@@ -3853,7 +4002,10 @@
                     )
                     break
                 subs = cur.recurse()
-                cur = next(iter(subs), None)
+                next_first = next(iter(subs), None)
+                if next_first is None:
+                    break
+                cur = typing.cast(ParserElement, next_first)
 
         self.mayReturnEmpty = all(e.mayReturnEmpty for e in self.exprs)
         return self
@@ -3884,13 +4036,14 @@
                     )
             else:
                 loc, exprtokens = e._parse(instring, loc, doActions)
-            if exprtokens or exprtokens.haskeys():
-                resultlist += exprtokens
+            resultlist += exprtokens
         return loc, resultlist
 
     def __iadd__(self, other):
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
+        if not isinstance(other, ParserElement):
+            return NotImplemented
         return self.append(other)  # And([self, other])
 
     def _checkRecursion(self, parseElementList):
@@ -3900,7 +4053,7 @@
             if not e.mayReturnEmpty:
                 break
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         inner = " ".join(str(e) for e in self.exprs)
         # strip off redundant inner {}'s
         while len(inner) > 1 and inner[0 :: len(inner) - 1] == "{}":
@@ -3958,7 +4111,7 @@
                 loc2 = e.try_parse(instring, loc, raise_fatal=True)
             except ParseFatalException as pfe:
                 pfe.__traceback__ = None
-                pfe.parserElement = e
+                pfe.parser_element = e
                 fatals.append(pfe)
                 maxException = None
                 maxExcLoc = -1
@@ -4016,12 +4169,15 @@
             if len(fatals) > 1:
                 fatals.sort(key=lambda e: -e.loc)
                 if fatals[0].loc == fatals[1].loc:
-                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))
+                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parser_element))))
             max_fatal = fatals[0]
             raise max_fatal
 
         if maxException is not None:
-            maxException.msg = self.errmsg
+            # infer from this check that all alternatives failed at the current position
+            # so emit this collective error message instead of any single error message
+            if maxExcLoc == loc:
+                maxException.msg = self.errmsg
             raise maxException
         else:
             raise ParseException(
@@ -4031,9 +4187,11 @@
     def __ixor__(self, other):
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
+        if not isinstance(other, ParserElement):
+            return NotImplemented
         return self.append(other)  # Or([self, other])
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "{" + " ^ ".join(str(e) for e in self.exprs) + "}"
 
     def _setResultsName(self, name, listAllMatches=False):
@@ -4118,7 +4276,7 @@
                 )
             except ParseFatalException as pfe:
                 pfe.__traceback__ = None
-                pfe.parserElement = e
+                pfe.parser_element = e
                 raise
             except ParseException as err:
                 if err.loc > maxExcLoc:
@@ -4132,7 +4290,10 @@
                     maxExcLoc = len(instring)
 
         if maxException is not None:
-            maxException.msg = self.errmsg
+            # infer from this check that all alternatives failed at the current position
+            # so emit this collective error message instead of any individual error message
+            if maxExcLoc == loc:
+                maxException.msg = self.errmsg
             raise maxException
         else:
             raise ParseException(
@@ -4142,9 +4303,11 @@
     def __ior__(self, other):
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
+        if not isinstance(other, ParserElement):
+            return NotImplemented
         return self.append(other)  # MatchFirst([self, other])
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "{" + " | ".join(str(e) for e in self.exprs) + "}"
 
     def _setResultsName(self, name, listAllMatches=False):
@@ -4242,6 +4405,13 @@
         self.initExprGroups = True
         self.saveAsList = True
 
+    def __iand__(self, other):
+        if isinstance(other, str_type):
+            other = self._literalStringClass(other)
+        if not isinstance(other, ParserElement):
+            return NotImplemented
+        return self.append(other)  # Each([self, other])
+
     def streamline(self) -> ParserElement:
         super().streamline()
         if self.exprs:
@@ -4296,7 +4466,7 @@
                     tmpLoc = e.try_parse(instring, tmpLoc, raise_fatal=True)
                 except ParseFatalException as pfe:
                     pfe.__traceback__ = None
-                    pfe.parserElement = e
+                    pfe.parser_element = e
                     fatals.append(pfe)
                     failed.append(e)
                 except ParseException:
@@ -4315,7 +4485,7 @@
             if len(fatals) > 1:
                 fatals.sort(key=lambda e: -e.loc)
                 if fatals[0].loc == fatals[1].loc:
-                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parserElement))))
+                    fatals.sort(key=lambda e: (-e.loc, -len(str(e.parser_element))))
             max_fatal = fatals[0]
             raise max_fatal
 
@@ -4324,7 +4494,7 @@
             raise ParseException(
                 instring,
                 loc,
-                "Missing one or more required elements ({})".format(missing),
+                f"Missing one or more required elements ({missing})",
             )
 
         # add any unmatched Opts, in case they have default values defined
@@ -4337,7 +4507,7 @@
 
         return loc, total_results
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "{" + " & ".join(str(e) for e in self.exprs) + "}"
 
 
@@ -4349,12 +4519,14 @@
     def __init__(self, expr: Union[ParserElement, str], savelist: bool = False):
         super().__init__(savelist)
         if isinstance(expr, str_type):
+            expr_str = typing.cast(str, expr)
             if issubclass(self._literalStringClass, Token):
-                expr = self._literalStringClass(expr)
+                expr = self._literalStringClass(expr_str)  # type: ignore[call-arg]
             elif issubclass(type(self), self._literalStringClass):
-                expr = Literal(expr)
+                expr = Literal(expr_str)
             else:
-                expr = self._literalStringClass(Literal(expr))
+                expr = self._literalStringClass(Literal(expr_str))  # type: ignore[assignment, call-arg]
+        expr = typing.cast(ParserElement, expr)
         self.expr = expr
         if expr is not None:
             self.mayIndexError = expr.mayIndexError
@@ -4367,12 +4539,16 @@
             self.callPreparse = expr.callPreparse
             self.ignoreExprs.extend(expr.ignoreExprs)
 
-    def recurse(self) -> Sequence[ParserElement]:
+    def recurse(self) -> List[ParserElement]:
         return [self.expr] if self.expr is not None else []
 
     def parseImpl(self, instring, loc, doActions=True):
         if self.expr is not None:
-            return self.expr._parse(instring, loc, doActions, callPreParse=False)
+            try:
+                return self.expr._parse(instring, loc, doActions, callPreParse=False)
+            except ParseBaseException as pbe:
+                pbe.msg = self.errmsg
+                raise
         else:
             raise ParseException(instring, loc, "No expression defined", self)
 
@@ -4380,8 +4556,8 @@
         super().leave_whitespace(recursive)
 
         if recursive:
-            self.expr = self.expr.copy()
             if self.expr is not None:
+                self.expr = self.expr.copy()
                 self.expr.leave_whitespace(recursive)
         return self
 
@@ -4389,8 +4565,8 @@
         super().ignore_whitespace(recursive)
 
         if recursive:
-            self.expr = self.expr.copy()
             if self.expr is not None:
+                self.expr = self.expr.copy()
                 self.expr.ignore_whitespace(recursive)
         return self
 
@@ -4420,6 +4596,11 @@
             self.expr._checkRecursion(subRecCheckList)
 
     def validate(self, validateTrace=None) -> None:
+        warnings.warn(
+            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
+            DeprecationWarning,
+            stacklevel=2,
+        )
         if validateTrace is None:
             validateTrace = []
         tmp = validateTrace[:] + [self]
@@ -4427,11 +4608,17 @@
             self.expr.validate(tmp)
         self._checkRecursion([])
 
-    def _generateDefaultName(self):
-        return "{}:({})".format(self.__class__.__name__, str(self.expr))
+    def _generateDefaultName(self) -> str:
+        return f"{self.__class__.__name__}:({str(self.expr)})"
 
-    ignoreWhitespace = ignore_whitespace
-    leaveWhitespace = leave_whitespace
+    # Compatibility synonyms
+    # fmt: off
+    @replaced_by_pep8(leave_whitespace)
+    def leaveWhitespace(self): ...
+
+    @replaced_by_pep8(ignore_whitespace)
+    def ignoreWhitespace(self): ...
+    # fmt: on
 
 
 class IndentedBlock(ParseElementEnhance):
@@ -4443,13 +4630,13 @@
     class _Indent(Empty):
         def __init__(self, ref_col: int):
             super().__init__()
-            self.errmsg = "expected indent at column {}".format(ref_col)
+            self.errmsg = f"expected indent at column {ref_col}"
             self.add_condition(lambda s, l, t: col(l, s) == ref_col)
 
     class _IndentGreater(Empty):
         def __init__(self, ref_col: int):
             super().__init__()
-            self.errmsg = "expected indent at column greater than {}".format(ref_col)
+            self.errmsg = f"expected indent at column greater than {ref_col}"
             self.add_condition(lambda s, l, t: col(l, s) > ref_col)
 
     def __init__(
@@ -4469,7 +4656,7 @@
 
         # see if self.expr matches at the current location - if not it will raise an exception
         # and no further work is necessary
-        self.expr.try_parse(instring, anchor_loc, doActions)
+        self.expr.try_parse(instring, anchor_loc, do_actions=doActions)
 
         indent_col = col(anchor_loc, instring)
         peer_detect_expr = self._Indent(indent_col)
@@ -4532,7 +4719,7 @@
         B AAA and definitely not this one
         '''
 
-        for t in (AtLineStart('AAA') + restOfLine).search_string(test):
+        for t in (AtLineStart('AAA') + rest_of_line).search_string(test):
             print(t)
 
     prints::
@@ -4598,9 +4785,9 @@
 
     Parameters:
 
-    - expr - expression that must match prior to the current parse
+    - ``expr`` - expression that must match prior to the current parse
       location
-    - retreat - (default= ``None``) - (int) maximum number of characters
+    - ``retreat`` - (default= ``None``) - (int) maximum number of characters
       to lookbehind prior to the current parse location
 
     If the lookbehind expression is a string, :class:`Literal`,
@@ -4627,6 +4814,7 @@
         self.mayIndexError = False
         self.exact = False
         if isinstance(expr, str_type):
+            expr = typing.cast(str, expr)
             retreat = len(expr)
             self.exact = True
         elif isinstance(expr, (Literal, Keyword)):
@@ -4746,18 +4934,18 @@
         self.errmsg = "Found unwanted token, " + str(self.expr)
 
     def parseImpl(self, instring, loc, doActions=True):
-        if self.expr.can_parse_next(instring, loc):
+        if self.expr.can_parse_next(instring, loc, do_actions=doActions):
             raise ParseException(instring, loc, self.errmsg, self)
         return loc, []
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "~{" + str(self.expr) + "}"
 
 
 class _MultipleMatch(ParseElementEnhance):
     def __init__(
         self,
-        expr: ParserElement,
+        expr: Union[str, ParserElement],
         stop_on: typing.Optional[Union[ParserElement, str]] = None,
         *,
         stopOn: typing.Optional[Union[ParserElement, str]] = None,
@@ -4781,7 +4969,7 @@
         self_skip_ignorables = self._skipIgnorables
         check_ender = self.not_ender is not None
         if check_ender:
-            try_not_ender = self.not_ender.tryParse
+            try_not_ender = self.not_ender.try_parse
 
         # must be at least one (but first see if we are the stopOn sentinel;
         # if so, fail)
@@ -4798,8 +4986,7 @@
                 else:
                     preloc = loc
                 loc, tmptokens = self_expr_parse(instring, preloc, doActions)
-                if tmptokens or tmptokens.haskeys():
-                    tokens += tmptokens
+                tokens += tmptokens
         except (ParseException, IndexError):
             pass
 
@@ -4837,10 +5024,11 @@
     Repetition of one or more of the given expression.
 
     Parameters:
-    - expr - expression that must match one or more times
-    - stop_on - (default= ``None``) - expression for a terminating sentinel
-         (only required if the sentinel would ordinarily match the repetition
-         expression)
+
+    - ``expr`` - expression that must match one or more times
+    - ``stop_on`` - (default= ``None``) - expression for a terminating sentinel
+      (only required if the sentinel would ordinarily match the repetition
+      expression)
 
     Example::
 
@@ -4859,7 +5047,7 @@
         (attr_expr * (1,)).parse_string(text).pprint()
     """
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "{" + str(self.expr) + "}..."
 
 
@@ -4868,6 +5056,7 @@
     Optional repetition of zero or more of the given expression.
 
     Parameters:
+
     - ``expr`` - expression that must match zero or more times
     - ``stop_on`` - expression for a terminating sentinel
       (only required if the sentinel would ordinarily match the repetition
@@ -4878,7 +5067,7 @@
 
     def __init__(
         self,
-        expr: ParserElement,
+        expr: Union[str, ParserElement],
         stop_on: typing.Optional[Union[ParserElement, str]] = None,
         *,
         stopOn: typing.Optional[Union[ParserElement, str]] = None,
@@ -4892,10 +5081,75 @@
         except (ParseException, IndexError):
             return loc, ParseResults([], name=self.resultsName)
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         return "[" + str(self.expr) + "]..."
 
 
+class DelimitedList(ParseElementEnhance):
+    def __init__(
+        self,
+        expr: Union[str, ParserElement],
+        delim: Union[str, ParserElement] = ",",
+        combine: bool = False,
+        min: typing.Optional[int] = None,
+        max: typing.Optional[int] = None,
+        *,
+        allow_trailing_delim: bool = False,
+    ):
+        """Helper to define a delimited list of expressions - the delimiter
+        defaults to ','. By default, the list elements and delimiters can
+        have intervening whitespace, and comments, but this can be
+        overridden by passing ``combine=True`` in the constructor. If
+        ``combine`` is set to ``True``, the matching tokens are
+        returned as a single token string, with the delimiters included;
+        otherwise, the matching tokens are returned as a list of tokens,
+        with the delimiters suppressed.
+
+        If ``allow_trailing_delim`` is set to True, then the list may end with
+        a delimiter.
+
+        Example::
+
+            DelimitedList(Word(alphas)).parse_string("aa,bb,cc") # -> ['aa', 'bb', 'cc']
+            DelimitedList(Word(hexnums), delim=':', combine=True).parse_string("AA:BB:CC:DD:EE") # -> ['AA:BB:CC:DD:EE']
+        """
+        if isinstance(expr, str_type):
+            expr = ParserElement._literalStringClass(expr)
+        expr = typing.cast(ParserElement, expr)
+
+        if min is not None:
+            if min < 1:
+                raise ValueError("min must be greater than 0")
+        if max is not None:
+            if min is not None and max < min:
+                raise ValueError("max must be greater than, or equal to min")
+
+        self.content = expr
+        self.raw_delim = str(delim)
+        self.delim = delim
+        self.combine = combine
+        if not combine:
+            self.delim = Suppress(delim)
+        self.min = min or 1
+        self.max = max
+        self.allow_trailing_delim = allow_trailing_delim
+
+        delim_list_expr = self.content + (self.delim + self.content) * (
+            self.min - 1,
+            None if self.max is None else self.max - 1,
+        )
+        if self.allow_trailing_delim:
+            delim_list_expr += Opt(self.delim)
+
+        if self.combine:
+            delim_list_expr = Combine(delim_list_expr)
+
+        super().__init__(delim_list_expr, savelist=True)
+
+    def _generateDefaultName(self) -> str:
+        return "{0} [{1} {0}]...".format(self.content.streamline(), self.raw_delim)
+
+
 class _NullToken:
     def __bool__(self):
         return False
@@ -4909,6 +5163,7 @@
     Optional matching of the given expression.
 
     Parameters:
+
     - ``expr`` - expression that must match zero or more times
     - ``default`` (optional) - value to be returned if the optional expression is not found.
 
@@ -4969,7 +5224,7 @@
                 tokens = []
         return loc, tokens
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         inner = str(self.expr)
         # strip off redundant inner {}'s
         while len(inner) > 1 and inner[0 :: len(inner) - 1] == "{}":
@@ -4986,6 +5241,7 @@
     expression is found.
 
     Parameters:
+
     - ``expr`` - target expression marking the end of the data to be skipped
     - ``include`` - if ``True``, the target expression is also parsed
       (the skipped text and target expression are returned as a 2-element
@@ -5045,14 +5301,15 @@
         self,
         other: Union[ParserElement, str],
         include: bool = False,
-        ignore: bool = None,
+        ignore: typing.Optional[Union[ParserElement, str]] = None,
         fail_on: typing.Optional[Union[ParserElement, str]] = None,
         *,
-        failOn: Union[ParserElement, str] = None,
+        failOn: typing.Optional[Union[ParserElement, str]] = None,
     ):
         super().__init__(other)
         failOn = failOn or fail_on
-        self.ignoreExpr = ignore
+        if ignore is not None:
+            self.ignore(ignore)
         self.mayReturnEmpty = True
         self.mayIndexError = False
         self.includeMatch = include
@@ -5070,9 +5327,7 @@
         self_failOn_canParseNext = (
             self.failOn.canParseNext if self.failOn is not None else None
         )
-        self_ignoreExpr_tryParse = (
-            self.ignoreExpr.tryParse if self.ignoreExpr is not None else None
-        )
+        self_preParse = self.preParse if self.callPreparse else None
 
         tmploc = loc
         while tmploc <= instrlen:
@@ -5081,13 +5336,9 @@
                 if self_failOn_canParseNext(instring, tmploc):
                     break
 
-            if self_ignoreExpr_tryParse is not None:
-                # advance past ignore expressions
-                while 1:
-                    try:
-                        tmploc = self_ignoreExpr_tryParse(instring, tmploc)
-                    except ParseBaseException:
-                        break
+            if self_preParse is not None:
+                # skip grammar-ignored expressions
+                tmploc = self_preParse(instring, tmploc)
 
             try:
                 self_expr_parse(instring, tmploc, doActions=False, callPreParse=False)
@@ -5145,15 +5396,20 @@
 
     def __init__(self, other: typing.Optional[Union[ParserElement, str]] = None):
         self.caller_frame = traceback.extract_stack(limit=2)[0]
-        super().__init__(other, savelist=False)
+        super().__init__(other, savelist=False)  # type: ignore[arg-type]
         self.lshift_line = None
 
-    def __lshift__(self, other):
+    def __lshift__(self, other) -> "Forward":
         if hasattr(self, "caller_frame"):
             del self.caller_frame
         if isinstance(other, str_type):
             other = self._literalStringClass(other)
+
+        if not isinstance(other, ParserElement):
+            return NotImplemented
+
         self.expr = other
+        self.streamlined = other.streamlined
         self.mayIndexError = self.expr.mayIndexError
         self.mayReturnEmpty = self.expr.mayReturnEmpty
         self.set_whitespace_chars(
@@ -5162,13 +5418,16 @@
         self.skipWhitespace = self.expr.skipWhitespace
         self.saveAsList = self.expr.saveAsList
         self.ignoreExprs.extend(self.expr.ignoreExprs)
-        self.lshift_line = traceback.extract_stack(limit=2)[-2]
+        self.lshift_line = traceback.extract_stack(limit=2)[-2]  # type: ignore[assignment]
         return self
 
-    def __ilshift__(self, other):
+    def __ilshift__(self, other) -> "Forward":
+        if not isinstance(other, ParserElement):
+            return NotImplemented
+
         return self << other
 
-    def __or__(self, other):
+    def __or__(self, other) -> "ParserElement":
         caller_line = traceback.extract_stack(limit=2)[-2]
         if (
             __diag__.warn_on_match_first_with_lshift_operator
@@ -5205,12 +5464,12 @@
             not in self.suppress_warnings_
         ):
             # walk stack until parse_string, scan_string, search_string, or transform_string is found
-            parse_fns = [
+            parse_fns = (
                 "parse_string",
                 "scan_string",
                 "search_string",
                 "transform_string",
-            ]
+            )
             tb = traceback.extract_stack(limit=200)
             for i, frm in enumerate(reversed(tb), start=1):
                 if frm.name in parse_fns:
@@ -5308,6 +5567,11 @@
         return self
 
     def validate(self, validateTrace=None) -> None:
+        warnings.warn(
+            "ParserElement.validate() is deprecated, and should not be used to check for left recursion",
+            DeprecationWarning,
+            stacklevel=2,
+        )
         if validateTrace is None:
             validateTrace = []
 
@@ -5317,7 +5581,7 @@
                 self.expr.validate(tmp)
         self._checkRecursion([])
 
-    def _generateDefaultName(self):
+    def _generateDefaultName(self) -> str:
         # Avoid infinite recursion by setting a temporary _defaultName
         self._defaultName = ": ..."
 
@@ -5356,8 +5620,14 @@
 
         return super()._setResultsName(name, list_all_matches)
 
-    ignoreWhitespace = ignore_whitespace
-    leaveWhitespace = leave_whitespace
+    # Compatibility synonyms
+    # fmt: off
+    @replaced_by_pep8(leave_whitespace)
+    def leaveWhitespace(self): ...
+
+    @replaced_by_pep8(ignore_whitespace)
+    def ignoreWhitespace(self): ...
+    # fmt: on
 
 
 class TokenConverter(ParseElementEnhance):
@@ -5439,11 +5709,11 @@
         ident = Word(alphas)
         num = Word(nums)
         term = ident | num
-        func = ident + Opt(delimited_list(term))
+        func = ident + Opt(DelimitedList(term))
         print(func.parse_string("fn a, b, 100"))
         # -> ['fn', 'a', 'b', '100']
 
-        func = ident + Group(Opt(delimited_list(term)))
+        func = ident + Group(Opt(DelimitedList(term)))
         print(func.parse_string("fn a, b, 100"))
         # -> ['fn', ['a', 'b', '100']]
     """
@@ -5579,7 +5849,7 @@
         ['a', 'b', 'c', 'd']
         ['START', 'relevant text ', 'END']
 
-    (See also :class:`delimited_list`.)
+    (See also :class:`DelimitedList`.)
     """
 
     def __init__(self, expr: Union[ParserElement, str], savelist: bool = False):
@@ -5638,15 +5908,13 @@
         s, l, t = paArgs[-3:]
         if len(paArgs) > 3:
             thisFunc = paArgs[0].__class__.__name__ + "." + thisFunc
-        sys.stderr.write(
-            ">>entering {}(line: {!r}, {}, {!r})\n".format(thisFunc, line(l, s), l, t)
-        )
+        sys.stderr.write(f">>entering {thisFunc}(line: {line(l, s)!r}, {l}, {t!r})\n")
         try:
             ret = f(*paArgs)
         except Exception as exc:
-            sys.stderr.write("<<leaving {} (exception: {})\n".format(thisFunc, exc))
+            sys.stderr.write(f"<<leaving {thisFunc} (exception: {exc})\n")
             raise
-        sys.stderr.write("<<leaving {} (ret: {!r})\n".format(thisFunc, ret))
+        sys.stderr.write(f"<<leaving {thisFunc} (ret: {ret!r})\n")
         return ret
 
     z.__name__ = f.__name__
@@ -5660,7 +5928,7 @@
 string_start = StringStart().set_name("string_start")
 string_end = StringEnd().set_name("string_end")
 
-_escapedPunc = Word(_bslash, r"\[]-*.$+^?()~ ", exact=2).set_parse_action(
+_escapedPunc = Regex(r"\\[\\[\]\/\-\*\.\$\+\^\?()~ ]").set_parse_action(
     lambda s, l, t: t[0][1]
 )
 _escapedHexChar = Regex(r"\\0?[xX][0-9a-fA-F]+").set_parse_action(
@@ -5677,7 +5945,7 @@
     Literal("[")
     + Opt("^").set_results_name("negate")
     + Group(OneOrMore(_charRange | _singleChar)).set_results_name("body")
-    + "]"
+    + Literal("]")
 )
 
 
@@ -5714,7 +5982,7 @@
     )
     try:
         return "".join(_expanded(part) for part in _reBracketExpr.parse_string(s).body)
-    except Exception:
+    except Exception as e:
         return ""
 
 
@@ -5769,7 +6037,11 @@
     Utility to simplify mass-naming of parser elements, for
     generating railroad diagram with named subdiagrams.
     """
-    for name, var in sys._getframe().f_back.f_locals.items():
+    calling_frame = sys._getframe().f_back
+    if calling_frame is None:
+        return
+    calling_frame = typing.cast(types.FrameType, calling_frame)
+    for name, var in calling_frame.f_locals.items():
         if isinstance(var, ParserElement) and not var.customName:
             var.set_name(name)
 
@@ -5783,9 +6055,28 @@
 ).set_name("string enclosed in single quotes")
 
 quoted_string = Combine(
-    Regex(r'"(?:[^"\n\r\\]|(?:"")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*') + '"'
-    | Regex(r"'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*") + "'"
-).set_name("quotedString using single or double quotes")
+    (Regex(r'"(?:[^"\n\r\\]|(?:"")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*') + '"').set_name(
+        "double quoted string"
+    )
+    | (Regex(r"'(?:[^'\n\r\\]|(?:'')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*") + "'").set_name(
+        "single quoted string"
+    )
+).set_name("quoted string using single or double quotes")
+
+python_quoted_string = Combine(
+    (Regex(r'"""(?:[^"\\]|""(?!")|"(?!"")|\\.)*', flags=re.MULTILINE) + '"""').set_name(
+        "multiline double quoted string"
+    )
+    ^ (
+        Regex(r"'''(?:[^'\\]|''(?!')|'(?!'')|\\.)*", flags=re.MULTILINE) + "'''"
+    ).set_name("multiline single quoted string")
+    ^ (Regex(r'"(?:[^"\n\r\\]|(?:\\")|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*') + '"').set_name(
+        "double quoted string"
+    )
+    ^ (Regex(r"'(?:[^'\n\r\\]|(?:\\')|(?:\\(?:[^x]|x[0-9a-fA-F]+)))*") + "'").set_name(
+        "single quoted string"
+    )
+).set_name("Python quoted string")
 
 unicode_string = Combine("u" + quoted_string.copy()).set_name("unicode string literal")
 
@@ -5800,9 +6091,7 @@
 ]
 
 # backward compatibility names
-tokenMap = token_map
-conditionAsParseAction = condition_as_parse_action
-nullDebugAction = null_debug_action
+# fmt: off
 sglQuotedString = sgl_quoted_string
 dblQuotedString = dbl_quoted_string
 quotedString = quoted_string
@@ -5811,4 +6100,16 @@
 lineEnd = line_end
 stringStart = string_start
 stringEnd = string_end
-traceParseAction = trace_parse_action
+
+@replaced_by_pep8(null_debug_action)
+def nullDebugAction(): ...
+
+@replaced_by_pep8(trace_parse_action)
+def traceParseAction(): ...
+
+@replaced_by_pep8(condition_as_parse_action)
+def conditionAsParseAction(): ...
+
+@replaced_by_pep8(token_map)
+def tokenMap(): ...
+# fmt: on
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/diagram/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/diagram/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/diagram/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/diagram/__init__.py	2023-07-15 11:52:27.575474434 -0400
@@ -1,3 +1,4 @@
+# mypy: ignore-errors
 import railroad
 from pip._vendor import pyparsing
 import typing
@@ -17,11 +18,13 @@
 
 
 jinja2_template_source = """\
+{% if not embed %}
 <!DOCTYPE html>
 <html>
 <head>
+{% endif %}
     {% if not head %}
-        <style type="text/css">
+        <style>
             .railroad-heading {
                 font-family: monospace;
             }
@@ -29,8 +32,10 @@
     {% else %}
         {{ head | safe }}
     {% endif %}
+{% if not embed %}
 </head>
 <body>
+{% endif %}
 {{ body | safe }}
 {% for diagram in diagrams %}
     <div class="railroad-group">
@@ -41,8 +46,10 @@
         </div>
     </div>
 {% endfor %}
+{% if not embed %}
 </body>
 </html>
+{% endif %}
 """
 
 template = Template(jinja2_template_source)
@@ -127,7 +134,7 @@
         return self.func(*args, **kwargs)
 
 
-def railroad_to_html(diagrams: List[NamedDiagram], **kwargs) -> str:
+def railroad_to_html(diagrams: List[NamedDiagram], embed=False, **kwargs) -> str:
     """
     Given a list of NamedDiagram, produce a single HTML string that visualises those diagrams
     :params kwargs: kwargs to be passed in to the template
@@ -137,13 +144,17 @@
         if diagram.diagram is None:
             continue
         io = StringIO()
-        diagram.diagram.writeSvg(io.write)
+        try:
+            css = kwargs.get('css')
+            diagram.diagram.writeStandalone(io.write, css=css)
+        except AttributeError:
+            diagram.diagram.writeSvg(io.write)
         title = diagram.name
         if diagram.index == 0:
             title += " (root)"
         data.append({"title": title, "text": "", "svg": io.getvalue()})
 
-    return template.render(diagrams=data, **kwargs)
+    return template.render(diagrams=data, embed=embed, **kwargs)
 
 
 def resolve_partial(partial: "EditablePartial[T]") -> T:
@@ -398,7 +409,6 @@
         show_results_names: bool = False,
         show_groups: bool = False,
     ) -> typing.Optional[EditablePartial]:
-
         ret = fn(
             element,
             parent,
@@ -555,9 +565,11 @@
         else:
             ret = EditablePartial.from_call(railroad.Group, label="", item="")
     elif isinstance(element, pyparsing.TokenConverter):
-        ret = EditablePartial.from_call(
-            AnnotatedItem, label=type(element).__name__.lower(), item=""
-        )
+        label = type(element).__name__.lower()
+        if label == "tokenconverter":
+            ret = EditablePartial.from_call(railroad.Sequence, items=[])
+        else:
+            ret = EditablePartial.from_call(AnnotatedItem, label=label, item="")
     elif isinstance(element, pyparsing.Opt):
         ret = EditablePartial.from_call(railroad.Optional, item="")
     elif isinstance(element, pyparsing.OneOrMore):
@@ -571,10 +583,12 @@
     elif isinstance(element, pyparsing.Empty) and not element.customName:
         # Skip unnamed "Empty" elements
         ret = None
-    elif len(exprs) > 1:
+    elif isinstance(element, pyparsing.ParseElementEnhance):
         ret = EditablePartial.from_call(railroad.Sequence, items=[])
     elif len(exprs) > 0 and not element_results_name:
         ret = EditablePartial.from_call(railroad.Group, item="", label=name)
+    elif len(exprs) > 0:
+        ret = EditablePartial.from_call(railroad.Sequence, items=[])
     else:
         terminal = EditablePartial.from_call(railroad.Terminal, element.defaultName)
         ret = terminal
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/exceptions.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/exceptions.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/exceptions.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/exceptions.py	2023-07-15 11:52:27.574474434 -0400
@@ -4,7 +4,13 @@
 import sys
 import typing
 
-from .util import col, line, lineno, _collapse_string_to_ranges
+from .util import (
+    col,
+    line,
+    lineno,
+    _collapse_string_to_ranges,
+    replaced_by_pep8,
+)
 from .unicode import pyparsing_unicode as ppu
 
 
@@ -19,6 +25,20 @@
 class ParseBaseException(Exception):
     """base exception class for all parsing runtime exceptions"""
 
+    loc: int
+    msg: str
+    pstr: str
+    parser_element: typing.Any  # "ParserElement"
+    args: typing.Tuple[str, int, typing.Optional[str]]
+
+    __slots__ = (
+        "loc",
+        "msg",
+        "pstr",
+        "parser_element",
+        "args",
+    )
+
     # Performance tuning: we construct a *lot* of these, so keep this
     # constructor as small and fast as possible
     def __init__(
@@ -35,7 +55,7 @@
         else:
             self.msg = msg
             self.pstr = pstr
-        self.parser_element = self.parserElement = elem
+        self.parser_element = elem
         self.args = (pstr, loc, msg)
 
     @staticmethod
@@ -64,7 +84,7 @@
         if isinstance(exc, ParseBaseException):
             ret.append(exc.line)
             ret.append(" " * (exc.column - 1) + "^")
-        ret.append("{}: {}".format(type(exc).__name__, exc))
+        ret.append(f"{type(exc).__name__}: {exc}")
 
         if depth > 0:
             callers = inspect.getinnerframes(exc.__traceback__, context=depth)
@@ -74,7 +94,9 @@
 
                 f_self = frm.f_locals.get("self", None)
                 if isinstance(f_self, ParserElement):
-                    if frm.f_code.co_name not in ("parseImpl", "_parseNoCache"):
+                    if not frm.f_code.co_name.startswith(
+                        ("parseImpl", "_parseNoCache")
+                    ):
                         continue
                     if id(f_self) in seen:
                         continue
@@ -82,21 +104,19 @@
 
                     self_type = type(f_self)
                     ret.append(
-                        "{}.{} - {}".format(
-                            self_type.__module__, self_type.__name__, f_self
-                        )
+                        f"{self_type.__module__}.{self_type.__name__} - {f_self}"
                     )
 
                 elif f_self is not None:
                     self_type = type(f_self)
-                    ret.append("{}.{}".format(self_type.__module__, self_type.__name__))
+                    ret.append(f"{self_type.__module__}.{self_type.__name__}")
 
                 else:
                     code = frm.f_code
                     if code.co_name in ("wrapper", "<module>"):
                         continue
 
-                    ret.append("{}".format(code.co_name))
+                    ret.append(code.co_name)
 
                 depth -= 1
                 if not depth:
@@ -110,7 +130,7 @@
         internal factory method to simplify creating one type of ParseException
         from another - avoids having __init__ signature conflicts among subclasses
         """
-        return cls(pe.pstr, pe.loc, pe.msg, pe.parserElement)
+        return cls(pe.pstr, pe.loc, pe.msg, pe.parser_element)
 
     @property
     def line(self) -> str:
@@ -140,6 +160,15 @@
         """
         return col(self.loc, self.pstr)
 
+    # pre-PEP8 compatibility
+    @property
+    def parserElement(self):
+        return self.parser_element
+
+    @parserElement.setter
+    def parserElement(self, elem):
+        self.parser_element = elem
+
     def __str__(self) -> str:
         if self.pstr:
             if self.loc >= len(self.pstr):
@@ -154,14 +183,14 @@
                 foundstr = (", found %r" % found).replace(r"\\", "\\")
         else:
             foundstr = ""
-        return "{}{}  (at char {}), (line:{}, col:{})".format(
-            self.msg, foundstr, self.loc, self.lineno, self.column
-        )
+        return f"{self.msg}{foundstr}  (at char {self.loc}), (line:{self.lineno}, col:{self.column})"
 
     def __repr__(self):
         return str(self)
 
-    def mark_input_line(self, marker_string: str = None, *, markerString=">!<") -> str:
+    def mark_input_line(
+        self, marker_string: typing.Optional[str] = None, *, markerString: str = ">!<"
+    ) -> str:
         """
         Extracts the exception line from the input string, and marks
         the location of the exception with a special symbol.
@@ -214,7 +243,10 @@
         """
         return self.explain_exception(self, depth)
 
-    markInputline = mark_input_line
+    # fmt: off
+    @replaced_by_pep8(mark_input_line)
+    def markInputline(self): ...
+    # fmt: on
 
 
 class ParseException(ParseBaseException):
@@ -264,4 +296,4 @@
         self.parseElementTrace = parseElementList
 
     def __str__(self) -> str:
-        return "RecursiveGrammarException: {}".format(self.parseElementTrace)
+        return f"RecursiveGrammarException: {self.parseElementTrace}"
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/helpers.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/helpers.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/helpers.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/helpers.py	2023-07-15 11:52:27.574474434 -0400
@@ -1,73 +1,22 @@
 # helpers.py
 import html.entities
 import re
+import sys
 import typing
 
 from . import __diag__
 from .core import *
-from .util import _bslash, _flatten, _escape_regex_range_chars
+from .util import (
+    _bslash,
+    _flatten,
+    _escape_regex_range_chars,
+    replaced_by_pep8,
+)
 
 
 #
 # global helpers
 #
-def delimited_list(
-    expr: Union[str, ParserElement],
-    delim: Union[str, ParserElement] = ",",
-    combine: bool = False,
-    min: typing.Optional[int] = None,
-    max: typing.Optional[int] = None,
-    *,
-    allow_trailing_delim: bool = False,
-) -> ParserElement:
-    """Helper to define a delimited list of expressions - the delimiter
-    defaults to ','. By default, the list elements and delimiters can
-    have intervening whitespace, and comments, but this can be
-    overridden by passing ``combine=True`` in the constructor. If
-    ``combine`` is set to ``True``, the matching tokens are
-    returned as a single token string, with the delimiters included;
-    otherwise, the matching tokens are returned as a list of tokens,
-    with the delimiters suppressed.
-
-    If ``allow_trailing_delim`` is set to True, then the list may end with
-    a delimiter.
-
-    Example::
-
-        delimited_list(Word(alphas)).parse_string("aa,bb,cc") # -> ['aa', 'bb', 'cc']
-        delimited_list(Word(hexnums), delim=':', combine=True).parse_string("AA:BB:CC:DD:EE") # -> ['AA:BB:CC:DD:EE']
-    """
-    if isinstance(expr, str_type):
-        expr = ParserElement._literalStringClass(expr)
-
-    dlName = "{expr} [{delim} {expr}]...{end}".format(
-        expr=str(expr.copy().streamline()),
-        delim=str(delim),
-        end=" [{}]".format(str(delim)) if allow_trailing_delim else "",
-    )
-
-    if not combine:
-        delim = Suppress(delim)
-
-    if min is not None:
-        if min < 1:
-            raise ValueError("min must be greater than 0")
-        min -= 1
-    if max is not None:
-        if min is not None and max <= min:
-            raise ValueError("max must be greater than, or equal to min")
-        max -= 1
-    delimited_list_expr = expr + (delim + expr)[min, max]
-
-    if allow_trailing_delim:
-        delimited_list_expr += Opt(delim)
-
-    if combine:
-        return Combine(delimited_list_expr).set_name(dlName)
-    else:
-        return delimited_list_expr.set_name(dlName)
-
-
 def counted_array(
     expr: ParserElement,
     int_expr: typing.Optional[ParserElement] = None,
@@ -187,7 +136,7 @@
             theseTokens = _flatten(t.as_list())
             if theseTokens != matchTokens:
                 raise ParseException(
-                    s, l, "Expected {}, found{}".format(matchTokens, theseTokens)
+                    s, l, f"Expected {matchTokens}, found{theseTokens}"
                 )
 
         rep.set_parse_action(must_match_these_tokens, callDuringTry=True)
@@ -218,7 +167,7 @@
     - ``caseless`` - treat all literals as caseless - (default= ``False``)
     - ``use_regex`` - as an optimization, will
       generate a :class:`Regex` object; otherwise, will generate
-      a :class:`MatchFirst` object (if ``caseless=True`` or ``asKeyword=True``, or if
+      a :class:`MatchFirst` object (if ``caseless=True`` or ``as_keyword=True``, or if
       creating a :class:`Regex` raises an exception) - (default= ``True``)
     - ``as_keyword`` - enforce :class:`Keyword`-style matching on the
       generated expressions - (default= ``False``)
@@ -262,6 +211,7 @@
 
     symbols: List[str] = []
     if isinstance(strs, str_type):
+        strs = typing.cast(str, strs)
         symbols = strs.split()
     elif isinstance(strs, Iterable):
         symbols = list(strs)
@@ -293,15 +243,13 @@
         try:
             if all(len(sym) == 1 for sym in symbols):
                 # symbols are just single characters, create range regex pattern
-                patt = "[{}]".format(
-                    "".join(_escape_regex_range_chars(sym) for sym in symbols)
-                )
+                patt = f"[{''.join(_escape_regex_range_chars(sym) for sym in symbols)}]"
             else:
                 patt = "|".join(re.escape(sym) for sym in symbols)
 
             # wrap with \b word break markers if defining as keywords
             if asKeyword:
-                patt = r"\b(?:{})\b".format(patt)
+                patt = rf"\b(?:{patt})\b"
 
             ret = Regex(patt, flags=re_flags).set_name(" | ".join(symbols))
 
@@ -371,7 +319,7 @@
     expression.  Useful to restore the parsed fields of an HTML start
     tag into the raw tag text itself, or to revert separate tokens with
     intervening whitespace back to the original matching input text. By
-    default, returns astring containing the original parsed text.
+    default, returns a string containing the original parsed text.
 
     If the optional ``as_string`` argument is passed as
     ``False``, then the return value is
@@ -390,7 +338,7 @@
         src = "this is test <b> bold <i>text</i> </b> normal text "
         for tag in ("b", "i"):
             opener, closer = make_html_tags(tag)
-            patt = original_text_for(opener + SkipTo(closer) + closer)
+            patt = original_text_for(opener + ... + closer)
             print(patt.search_string(src)[0])
 
     prints::
@@ -426,7 +374,7 @@
 
 def locatedExpr(expr: ParserElement) -> ParserElement:
     """
-    (DEPRECATED - future code should use the Located class)
+    (DEPRECATED - future code should use the :class:`Located` class)
     Helper to decorate a returned token with its starting and ending
     locations in the input string.
 
@@ -437,12 +385,12 @@
     - ``value`` - the actual parsed results
 
     Be careful if the input text contains ``<TAB>`` characters, you
-    may want to call :class:`ParserElement.parseWithTabs`
+    may want to call :class:`ParserElement.parse_with_tabs`
 
     Example::
 
         wd = Word(alphas)
-        for match in locatedExpr(wd).searchString("ljsdf123lksdjjf123lkkjj1222"):
+        for match in locatedExpr(wd).search_string("ljsdf123lksdjjf123lkkjj1222"):
             print(match)
 
     prints::
@@ -471,6 +419,7 @@
     closing delimiters (``"("`` and ``")"`` are the default).
 
     Parameters:
+
     - ``opener`` - opening character for a nested list
       (default= ``"("``); can also be a pyparsing expression
     - ``closer`` - closing character for a nested list
@@ -507,7 +456,7 @@
 
         c_function = (decl_data_type("type")
                       + ident("name")
-                      + LPAR + Opt(delimited_list(arg), [])("args") + RPAR
+                      + LPAR + Opt(DelimitedList(arg), [])("args") + RPAR
                       + code_body("body"))
         c_function.ignore(c_style_comment)
 
@@ -539,6 +488,8 @@
         raise ValueError("opening and closing strings cannot be the same")
     if content is None:
         if isinstance(opener, str_type) and isinstance(closer, str_type):
+            opener = typing.cast(str, opener)
+            closer = typing.cast(str, closer)
             if len(opener) == 1 and len(closer) == 1:
                 if ignoreExpr is not None:
                     content = Combine(
@@ -695,12 +646,15 @@
 )
 
 
-def replace_html_entity(t):
+def replace_html_entity(s, l, t):
     """Helper parser action to replace common HTML entities with their special characters"""
     return _htmlEntityMap.get(t.entity)
 
 
 class OpAssoc(Enum):
+    """Enumeration of operator associativity
+    - used in constructing InfixNotationOperatorSpec for :class:`infix_notation`"""
+
     LEFT = 1
     RIGHT = 2
 
@@ -742,6 +696,7 @@
     improve your parser performance.
 
     Parameters:
+
     - ``base_expr`` - expression representing the most basic operand to
       be used in the expression
     - ``op_list`` - list of tuples, one for each operator precedence level
@@ -764,11 +719,11 @@
         ``set_parse_action(*fn)``
         (:class:`ParserElement.set_parse_action`)
     - ``lpar`` - expression for matching left-parentheses; if passed as a
-      str, then will be parsed as Suppress(lpar). If lpar is passed as
+      str, then will be parsed as ``Suppress(lpar)``. If lpar is passed as
       an expression (such as ``Literal('(')``), then it will be kept in
       the parsed results, and grouped with them. (default= ``Suppress('(')``)
     - ``rpar`` - expression for matching right-parentheses; if passed as a
-      str, then will be parsed as Suppress(rpar). If rpar is passed as
+      str, then will be parsed as ``Suppress(rpar)``. If rpar is passed as
       an expression (such as ``Literal(')')``), then it will be kept in
       the parsed results, and grouped with them. (default= ``Suppress(')')``)
 
@@ -800,9 +755,13 @@
         (5+3)*6
         [[[5, '+', 3], '*', 6]]
 
+        (5+x)*y
+        [[[5, '+', 'x'], '*', 'y']]
+
         -2--11
         [[['-', 2], '-', ['-', 11]]]
     """
+
     # captive version of FollowedBy that does not do parse actions or capture results names
     class _FB(FollowedBy):
         def parseImpl(self, instring, loc, doActions=True):
@@ -823,19 +782,25 @@
     else:
         lastExpr = base_expr | (lpar + ret + rpar)
 
+    arity: int
+    rightLeftAssoc: opAssoc
+    pa: typing.Optional[ParseAction]
+    opExpr1: ParserElement
+    opExpr2: ParserElement
     for i, operDef in enumerate(op_list):
-        opExpr, arity, rightLeftAssoc, pa = (operDef + (None,))[:4]
+        opExpr, arity, rightLeftAssoc, pa = (operDef + (None,))[:4]  # type: ignore[assignment]
         if isinstance(opExpr, str_type):
             opExpr = ParserElement._literalStringClass(opExpr)
+        opExpr = typing.cast(ParserElement, opExpr)
         if arity == 3:
             if not isinstance(opExpr, (tuple, list)) or len(opExpr) != 2:
                 raise ValueError(
                     "if numterms=3, opExpr must be a tuple or list of two expressions"
                 )
             opExpr1, opExpr2 = opExpr
-            term_name = "{}{} term".format(opExpr1, opExpr2)
+            term_name = f"{opExpr1}{opExpr2} term"
         else:
-            term_name = "{} term".format(opExpr)
+            term_name = f"{opExpr} term"
 
         if not 1 <= arity <= 3:
             raise ValueError("operator must be unary (1), binary (2), or ternary (3)")
@@ -843,7 +808,8 @@
         if rightLeftAssoc not in (OpAssoc.LEFT, OpAssoc.RIGHT):
             raise ValueError("operator must indicate right or left associativity")
 
-        thisExpr: Forward = Forward().set_name(term_name)
+        thisExpr: ParserElement = Forward().set_name(term_name)
+        thisExpr = typing.cast(Forward, thisExpr)
         if rightLeftAssoc is OpAssoc.LEFT:
             if arity == 1:
                 matchExpr = _FB(lastExpr + opExpr) + Group(lastExpr + opExpr[1, ...])
@@ -890,7 +856,7 @@
 
 def indentedBlock(blockStatementExpr, indentStack, indent=True, backup_stacks=[]):
     """
-    (DEPRECATED - use IndentedBlock class instead)
+    (DEPRECATED - use :class:`IndentedBlock` class instead)
     Helper method for defining space-delimited indentation blocks,
     such as those used to define block statements in Python source code.
 
@@ -1063,22 +1029,28 @@
 ]
 
 
+# compatibility function, superseded by DelimitedList class
+def delimited_list(
+    expr: Union[str, ParserElement],
+    delim: Union[str, ParserElement] = ",",
+    combine: bool = False,
+    min: typing.Optional[int] = None,
+    max: typing.Optional[int] = None,
+    *,
+    allow_trailing_delim: bool = False,
+) -> ParserElement:
+    """(DEPRECATED - use :class:`DelimitedList` class)"""
+    return DelimitedList(
+        expr, delim, combine, min, max, allow_trailing_delim=allow_trailing_delim
+    )
+
+
 # pre-PEP8 compatible names
-delimitedList = delimited_list
-countedArray = counted_array
-matchPreviousLiteral = match_previous_literal
-matchPreviousExpr = match_previous_expr
-oneOf = one_of
-dictOf = dict_of
-originalTextFor = original_text_for
-nestedExpr = nested_expr
-makeHTMLTags = make_html_tags
-makeXMLTags = make_xml_tags
-anyOpenTag, anyCloseTag = any_open_tag, any_close_tag
-commonHTMLEntity = common_html_entity
-replaceHTMLEntity = replace_html_entity
+# fmt: off
 opAssoc = OpAssoc
-infixNotation = infix_notation
+anyOpenTag = any_open_tag
+anyCloseTag = any_close_tag
+commonHTMLEntity = common_html_entity
 cStyleComment = c_style_comment
 htmlComment = html_comment
 restOfLine = rest_of_line
@@ -1086,3 +1058,43 @@
 cppStyleComment = cpp_style_comment
 javaStyleComment = java_style_comment
 pythonStyleComment = python_style_comment
+
+@replaced_by_pep8(DelimitedList)
+def delimitedList(): ...
+
+@replaced_by_pep8(DelimitedList)
+def delimited_list(): ...
+
+@replaced_by_pep8(counted_array)
+def countedArray(): ...
+
+@replaced_by_pep8(match_previous_literal)
+def matchPreviousLiteral(): ...
+
+@replaced_by_pep8(match_previous_expr)
+def matchPreviousExpr(): ...
+
+@replaced_by_pep8(one_of)
+def oneOf(): ...
+
+@replaced_by_pep8(dict_of)
+def dictOf(): ...
+
+@replaced_by_pep8(original_text_for)
+def originalTextFor(): ...
+
+@replaced_by_pep8(nested_expr)
+def nestedExpr(): ...
+
+@replaced_by_pep8(make_html_tags)
+def makeHTMLTags(): ...
+
+@replaced_by_pep8(make_xml_tags)
+def makeXMLTags(): ...
+
+@replaced_by_pep8(replace_html_entity)
+def replaceHTMLEntity(): ...
+
+@replaced_by_pep8(infix_notation)
+def infixNotation(): ...
+# fmt: on
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/__init__.py	2023-07-15 11:52:27.574474434 -0400
@@ -56,7 +56,7 @@
 :class:`'|'<MatchFirst>`, :class:`'^'<Or>` and :class:`'&'<Each>` operators.
 
 The :class:`ParseResults` object returned from
-:class:`ParserElement.parseString` can be
+:class:`ParserElement.parse_string` can be
 accessed as a nested list, a dictionary, or an object with named
 attributes.
 
@@ -85,11 +85,11 @@
    and :class:`'&'<Each>` operators to combine simple expressions into
    more complex ones
  - associate names with your parsed results using
-   :class:`ParserElement.setResultsName`
+   :class:`ParserElement.set_results_name`
  - access the parsed data, which is returned as a :class:`ParseResults`
    object
- - find some helpful expression short-cuts like :class:`delimitedList`
-   and :class:`oneOf`
+ - find some helpful expression short-cuts like :class:`DelimitedList`
+   and :class:`one_of`
  - find more useful common expressions in the :class:`pyparsing_common`
    namespace class
 """
@@ -106,30 +106,22 @@
     @property
     def __version__(self):
         return (
-            "{}.{}.{}".format(self.major, self.minor, self.micro)
+            f"{self.major}.{self.minor}.{self.micro}"
             + (
-                "{}{}{}".format(
-                    "r" if self.releaselevel[0] == "c" else "",
-                    self.releaselevel[0],
-                    self.serial,
-                ),
+                f"{'r' if self.releaselevel[0] == 'c' else ''}{self.releaselevel[0]}{self.serial}",
                 "",
             )[self.releaselevel == "final"]
         )
 
     def __str__(self):
-        return "{} {} / {}".format(__name__, self.__version__, __version_time__)
+        return f"{__name__} {self.__version__} / {__version_time__}"
 
     def __repr__(self):
-        return "{}.{}({})".format(
-            __name__,
-            type(self).__name__,
-            ", ".join("{}={!r}".format(*nv) for nv in zip(self._fields, self)),
-        )
+        return f"{__name__}.{type(self).__name__}({', '.join('{}={!r}'.format(*nv) for nv in zip(self._fields, self))})"
 
 
-__version_info__ = version_info(3, 0, 9, "final", 0)
-__version_time__ = "05 May 2022 07:02 UTC"
+__version_info__ = version_info(3, 1, 0, "final", 1)
+__version_time__ = "18 Jun 2023 14:05 UTC"
 __version__ = __version_info__.__version__
 __versionTime__ = __version_time__
 __author__ = "Paul McGuire <ptmcg.gm+pyparsing@gmail.com>"
@@ -139,9 +131,9 @@
 from .actions import *
 from .core import __diag__, __compat__
 from .results import *
-from .core import *
+from .core import *  # type: ignore[misc, assignment]
 from .core import _builtin_exprs as core_builtin_exprs
-from .helpers import *
+from .helpers import *  # type: ignore[misc, assignment]
 from .helpers import _builtin_exprs as helper_builtin_exprs
 
 from .unicode import unicode_set, UnicodeRangeList, pyparsing_unicode as unicode
@@ -153,11 +145,11 @@
 
 # define backward compat synonyms
 if "pyparsing_unicode" not in globals():
-    pyparsing_unicode = unicode
+    pyparsing_unicode = unicode  # type: ignore[misc]
 if "pyparsing_common" not in globals():
-    pyparsing_common = common
+    pyparsing_common = common  # type: ignore[misc]
 if "pyparsing_test" not in globals():
-    pyparsing_test = testing
+    pyparsing_test = testing  # type: ignore[misc]
 
 core_builtin_exprs += common_builtin_exprs + helper_builtin_exprs
 
@@ -174,7 +166,9 @@
     "CaselessKeyword",
     "CaselessLiteral",
     "CharsNotIn",
+    "CloseMatch",
     "Combine",
+    "DelimitedList",
     "Dict",
     "Each",
     "Empty",
@@ -227,9 +221,11 @@
     "alphas8bit",
     "any_close_tag",
     "any_open_tag",
+    "autoname_elements",
     "c_style_comment",
     "col",
     "common_html_entity",
+    "condition_as_parse_action",
     "counted_array",
     "cpp_style_comment",
     "dbl_quoted_string",
@@ -241,6 +237,7 @@
     "html_comment",
     "identchars",
     "identbodychars",
+    "infix_notation",
     "java_style_comment",
     "line",
     "line_end",
@@ -255,8 +252,12 @@
     "null_debug_action",
     "nums",
     "one_of",
+    "original_text_for",
     "printables",
     "punc8bit",
+    "pyparsing_common",
+    "pyparsing_test",
+    "pyparsing_unicode",
     "python_style_comment",
     "quoted_string",
     "remove_quotes",
@@ -267,28 +268,20 @@
     "srange",
     "string_end",
     "string_start",
+    "token_map",
     "trace_parse_action",
+    "ungroup",
+    "unicode_set",
     "unicode_string",
     "with_attribute",
-    "indentedBlock",
-    "original_text_for",
-    "ungroup",
-    "infix_notation",
-    "locatedExpr",
     "with_class",
-    "CloseMatch",
-    "token_map",
-    "pyparsing_common",
-    "pyparsing_unicode",
-    "unicode_set",
-    "condition_as_parse_action",
-    "pyparsing_test",
     # pre-PEP8 compatibility names
     "__versionTime__",
     "anyCloseTag",
     "anyOpenTag",
     "cStyleComment",
     "commonHTMLEntity",
+    "conditionAsParseAction",
     "countedArray",
     "cppStyleComment",
     "dblQuotedString",
@@ -296,9 +289,12 @@
     "delimitedList",
     "dictOf",
     "htmlComment",
+    "indentedBlock",
+    "infixNotation",
     "javaStyleComment",
     "lineEnd",
     "lineStart",
+    "locatedExpr",
     "makeHTMLTags",
     "makeXMLTags",
     "matchOnlyAtCol",
@@ -308,6 +304,7 @@
     "nullDebugAction",
     "oneOf",
     "opAssoc",
+    "originalTextFor",
     "pythonStyleComment",
     "quotedString",
     "removeQuotes",
@@ -317,15 +314,9 @@
     "sglQuotedString",
     "stringEnd",
     "stringStart",
+    "tokenMap",
     "traceParseAction",
     "unicodeString",
     "withAttribute",
-    "indentedBlock",
-    "originalTextFor",
-    "infixNotation",
-    "locatedExpr",
     "withClass",
-    "tokenMap",
-    "conditionAsParseAction",
-    "autoname_elements",
 ]
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/results.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/results.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/results.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/results.py	2023-07-15 11:52:27.574474434 -0400
@@ -1,18 +1,25 @@
 # results.py
-from collections.abc import MutableMapping, Mapping, MutableSequence, Iterator
+from collections.abc import (
+    MutableMapping,
+    Mapping,
+    MutableSequence,
+    Iterator,
+    Sequence,
+    Container,
+)
 import pprint
-from weakref import ref as wkref
-from typing import Tuple, Any
+from typing import Tuple, Any, Dict, Set, List
 
 str_type: Tuple[type, ...] = (str, bytes)
 _generator_type = type((_ for _ in ()))
 
 
 class _ParseResultsWithOffset:
+    tup: Tuple["ParseResults", int]
     __slots__ = ["tup"]
 
-    def __init__(self, p1, p2):
-        self.tup = (p1, p2)
+    def __init__(self, p1: "ParseResults", p2: int):
+        self.tup: Tuple[ParseResults, int] = (p1, p2)
 
     def __getitem__(self, i):
         return self.tup[i]
@@ -47,7 +54,7 @@
         result = date_str.parse_string("1999/12/31")
 
         def test(s, fn=repr):
-            print("{} -> {}".format(s, fn(eval(s))))
+            print(f"{s} -> {fn(eval(s))}")
         test("list(result)")
         test("result[0]")
         test("result['month']")
@@ -70,27 +77,33 @@
         - year: '1999'
     """
 
-    _null_values: Tuple[Any, ...] = (None, [], "", ())
+    _null_values: Tuple[Any, ...] = (None, [], ())
 
-    __slots__ = [
+    _name: str
+    _parent: "ParseResults"
+    _all_names: Set[str]
+    _modal: bool
+    _toklist: List[Any]
+    _tokdict: Dict[str, Any]
+
+    __slots__ = (
         "_name",
         "_parent",
         "_all_names",
         "_modal",
         "_toklist",
         "_tokdict",
-        "__weakref__",
-    ]
+    )
 
     class List(list):
         """
         Simple wrapper class to distinguish parsed list results that should be preserved
-        as actual Python lists, instead of being converted to :class:`ParseResults`:
+        as actual Python lists, instead of being converted to :class:`ParseResults`::
 
             LBRACK, RBRACK = map(pp.Suppress, "[]")
             element = pp.Forward()
             item = ppc.integer
-            element_list = LBRACK + pp.delimited_list(element) + RBRACK
+            element_list = LBRACK + pp.DelimitedList(element) + RBRACK
 
             # add parse actions to convert from ParseResults to actual Python collection types
             def as_python_list(t):
@@ -107,7 +120,7 @@
                 (2,3,4)
                 ''', post_parse=lambda s, r: (r[0], type(r[0])))
 
-        prints:
+        prints::
 
             100
             (100, <class 'int'>)
@@ -127,8 +140,7 @@
 
             if not isinstance(contained, list):
                 raise TypeError(
-                    "{} may only be constructed with a list,"
-                    " not {}".format(cls.__name__, type(contained).__name__)
+                    f"{cls.__name__} may only be constructed with a list, not {type(contained).__name__}"
                 )
 
             return list.__new__(cls)
@@ -159,6 +171,7 @@
     def __init__(
         self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance
     ):
+        self._tokdict: Dict[str, _ParseResultsWithOffset]
         self._modal = modal
         if name is not None and name != "":
             if isinstance(name, int):
@@ -210,7 +223,7 @@
             ]
             sub = v
         if isinstance(sub, ParseResults):
-            sub._parent = wkref(self)
+            sub._parent = self
 
     def __delitem__(self, i):
         if isinstance(i, (int, slice)):
@@ -263,7 +276,7 @@
         """
         Since ``keys()`` returns an iterator, this method is helpful in bypassing
         code that looks for the existence of any defined results names."""
-        return bool(self._tokdict)
+        return not not self._tokdict
 
     def pop(self, *args, **kwargs):
         """
@@ -311,9 +324,7 @@
             if k == "default":
                 args = (args[0], v)
             else:
-                raise TypeError(
-                    "pop() got an unexpected keyword argument {!r}".format(k)
-                )
+                raise TypeError(f"pop() got an unexpected keyword argument {k!r}")
         if isinstance(args[0], int) or len(args) == 1 or args[0] in self:
             index = args[0]
             ret = self[index]
@@ -423,12 +434,15 @@
                 raise AttributeError(name)
             return ""
 
-    def __add__(self, other) -> "ParseResults":
+    def __add__(self, other: "ParseResults") -> "ParseResults":
         ret = self.copy()
         ret += other
         return ret
 
-    def __iadd__(self, other) -> "ParseResults":
+    def __iadd__(self, other: "ParseResults") -> "ParseResults":
+        if not other:
+            return self
+
         if other._tokdict:
             offset = len(self._toklist)
             addoffset = lambda a: offset if a < 0 else a + offset
@@ -441,7 +455,7 @@
             for k, v in otherdictitems:
                 self[k] = v
                 if isinstance(v[0], ParseResults):
-                    v[0]._parent = wkref(self)
+                    v[0]._parent = self
 
         self._toklist += other._toklist
         self._all_names |= other._all_names
@@ -456,7 +470,7 @@
             return other + self
 
     def __repr__(self) -> str:
-        return "{}({!r}, {})".format(type(self).__name__, self._toklist, self.as_dict())
+        return f"{type(self).__name__}({self._toklist!r}, {self.as_dict()})"
 
     def __str__(self) -> str:
         return (
@@ -532,7 +546,10 @@
 
     def copy(self) -> "ParseResults":
         """
-        Returns a new copy of a :class:`ParseResults` object.
+        Returns a new shallow copy of a :class:`ParseResults` object. `ParseResults`
+        items contained within the source are shared with the copy. Use
+        :class:`ParseResults.deepcopy()` to create a copy with its own separate
+        content values.
         """
         ret = ParseResults(self._toklist)
         ret._tokdict = self._tokdict.copy()
@@ -541,6 +558,27 @@
         ret._name = self._name
         return ret
 
+    def deepcopy(self) -> "ParseResults":
+        """
+        Returns a new deep copy of a :class:`ParseResults` object.
+        """
+        ret = self.copy()
+        # replace values with copies if they are of known mutable types
+        for i, obj in enumerate(self._toklist):
+            if isinstance(obj, ParseResults):
+                self._toklist[i] = obj.deepcopy()
+            elif isinstance(obj, (str, bytes)):
+                pass
+            elif isinstance(obj, MutableMapping):
+                self._toklist[i] = dest = type(obj)()
+                for k, v in obj.items():
+                    dest[k] = v.deepcopy() if isinstance(v, ParseResults) else v
+            elif isinstance(obj, Container):
+                self._toklist[i] = type(obj)(
+                    v.deepcopy() if isinstance(v, ParseResults) else v for v in obj
+                )
+        return ret
+
     def get_name(self):
         r"""
         Returns the results name for this token expression. Useful when several
@@ -569,20 +607,17 @@
         if self._name:
             return self._name
         elif self._parent:
-            par = self._parent()
-
-            def find_in_parent(sub):
-                return next(
-                    (
-                        k
-                        for k, vlist in par._tokdict.items()
-                        for v, loc in vlist
-                        if sub is v
-                    ),
-                    None,
-                )
-
-            return find_in_parent(self) if par else None
+            par: "ParseResults" = self._parent
+            parent_tokdict_items = par._tokdict.items()
+            return next(
+                (
+                    k
+                    for k, vlist in parent_tokdict_items
+                    for v, loc in vlist
+                    if v is self
+                ),
+                None,
+            )
         elif (
             len(self) == 1
             and len(self._tokdict) == 1
@@ -623,7 +658,7 @@
                 for k, v in items:
                     if out:
                         out.append(NL)
-                    out.append("{}{}- {}: ".format(indent, ("  " * _depth), k))
+                    out.append(f"{indent}{('  ' * _depth)}- {k}: ")
                     if isinstance(v, ParseResults):
                         if v:
                             out.append(
@@ -685,7 +720,7 @@
             num = Word(nums)
             func = Forward()
             term = ident | num | Group('(' + func + ')')
-            func <<= ident + Group(Optional(delimited_list(term)))
+            func <<= ident + Group(Optional(DelimitedList(term)))
             result = func.parse_string("fna a,b,(fnb c,d,200),100")
             result.pprint(width=40)
 
@@ -705,7 +740,7 @@
             self._toklist,
             (
                 self._tokdict.copy(),
-                self._parent is not None and self._parent() or None,
+                None,
                 self._all_names,
                 self._name,
             ),
@@ -714,10 +749,7 @@
     def __setstate__(self, state):
         self._toklist, (self._tokdict, par, inAccumNames, self._name) = state
         self._all_names = set(inAccumNames)
-        if par is not None:
-            self._parent = wkref(par)
-        else:
-            self._parent = None
+        self._parent = None
 
     def __getnewargs__(self):
         return self._toklist, self._name
@@ -738,6 +770,7 @@
                 iter(obj)
             except Exception:
                 return False
+            # str's are iterable, but in pyparsing, we don't want to iterate over them
             else:
                 return not isinstance(obj, str_type)
 
@@ -752,8 +785,11 @@
         return ret
 
     asList = as_list
+    """Deprecated - use :class:`as_list`"""
     asDict = as_dict
+    """Deprecated - use :class:`as_dict`"""
     getName = get_name
+    """Deprecated - use :class:`get_name`"""
 
 
 MutableMapping.register(ParseResults)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/testing.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/testing.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/testing.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/testing.py	2023-07-15 11:52:27.574474434 -0400
@@ -222,7 +222,7 @@
                             )
                         else:
                             # warning here maybe?
-                            print("no validation for {!r}".format(test_string))
+                            print(f"no validation for {test_string!r}")
 
             # do this last, in case some specific test results can be reported instead
             self.assertTrue(
@@ -265,15 +265,18 @@
         if expand_tabs:
             s = s.expandtabs()
         if mark_control is not None:
+            mark_control = typing.cast(str, mark_control)
             if mark_control == "unicode":
-                tbl = str.maketrans(
-                    {c: u for c, u in zip(range(0, 33), range(0x2400, 0x2433))}
-                    | {127: 0x2421}
-                )
+                transtable_map = {
+                    c: u for c, u in zip(range(0, 33), range(0x2400, 0x2433))
+                }
+                transtable_map[127] = 0x2421
+                tbl = str.maketrans(transtable_map)
                 eol_mark = ""
             else:
+                ord_mark_control = ord(mark_control)
                 tbl = str.maketrans(
-                    {c: mark_control for c in list(range(0, 32)) + [127]}
+                    {c: ord_mark_control for c in list(range(0, 32)) + [127]}
                 )
             s = s.translate(tbl)
         if mark_spaces is not None and mark_spaces != " ":
@@ -303,7 +306,7 @@
             header0 = (
                 lead
                 + "".join(
-                    "{}{}".format(" " * 99, (i + 1) % 100)
+                    f"{' ' * 99}{(i + 1) % 100}"
                     for i in range(max(max_line_len // 100, 1))
                 )
                 + "\n"
@@ -313,10 +316,7 @@
         header1 = (
             header0
             + lead
-            + "".join(
-                "         {}".format((i + 1) % 10)
-                for i in range(-(-max_line_len // 10))
-            )
+            + "".join(f"         {(i + 1) % 10}" for i in range(-(-max_line_len // 10)))
             + "\n"
         )
         header2 = lead + "1234567890" * (-(-max_line_len // 10)) + "\n"
@@ -324,7 +324,7 @@
             header1
             + header2
             + "\n".join(
-                "{:{}d}:{}{}".format(i, lineno_width, line, eol_mark)
+                f"{i:{lineno_width}d}:{line}{eol_mark}"
                 for i, line in enumerate(s_lines, start=start_line)
             )
             + "\n"
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/unicode.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/unicode.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/unicode.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/unicode.py	2023-07-15 11:52:27.574474434 -0400
@@ -64,27 +64,27 @@
 
     @_lazyclassproperty
     def printables(cls):
-        "all non-whitespace characters in this range"
+        """all non-whitespace characters in this range"""
         return "".join(filterfalse(str.isspace, cls._chars_for_ranges))
 
     @_lazyclassproperty
     def alphas(cls):
-        "all alphabetic characters in this range"
+        """all alphabetic characters in this range"""
         return "".join(filter(str.isalpha, cls._chars_for_ranges))
 
     @_lazyclassproperty
     def nums(cls):
-        "all numeric digit characters in this range"
+        """all numeric digit characters in this range"""
         return "".join(filter(str.isdigit, cls._chars_for_ranges))
 
     @_lazyclassproperty
     def alphanums(cls):
-        "all alphanumeric characters in this range"
+        """all alphanumeric characters in this range"""
         return cls.alphas + cls.nums
 
     @_lazyclassproperty
     def identchars(cls):
-        "all characters in this range that are valid identifier characters, plus underscore '_'"
+        """all characters in this range that are valid identifier characters, plus underscore '_'"""
         return "".join(
             sorted(
                 set(
@@ -100,13 +100,13 @@
     def identbodychars(cls):
         """
         all characters in this range that are valid identifier body characters,
-        plus the digits 0-9
+        plus the digits 0-9, and · (Unicode MIDDLE DOT)
         """
         return "".join(
             sorted(
                 set(
                     cls.identchars
-                    + "0123456789"
+                    + "0123456789·"
                     + "".join(
                         [c for c in cls._chars_for_ranges if ("_" + c).isidentifier()]
                     )
@@ -114,6 +114,16 @@
             )
         )
 
+    @_lazyclassproperty
+    def identifier(cls):
+        """
+        a pyparsing Word expression for an identifier using this range's definitions for
+        identchars and identbodychars
+        """
+        from pip._vendor.pyparsing import Word
+
+        return Word(cls.identchars, cls.identbodychars)
+
 
 class pyparsing_unicode(unicode_set):
     """
@@ -128,32 +138,32 @@
     ]
 
     class BasicMultilingualPlane(unicode_set):
-        "Unicode set for the Basic Multilingual Plane"
+        """Unicode set for the Basic Multilingual Plane"""
         _ranges: UnicodeRangeList = [
             (0x0020, 0xFFFF),
         ]
 
     class Latin1(unicode_set):
-        "Unicode set for Latin-1 Unicode Character Range"
+        """Unicode set for Latin-1 Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0020, 0x007E),
             (0x00A0, 0x00FF),
         ]
 
     class LatinA(unicode_set):
-        "Unicode set for Latin-A Unicode Character Range"
+        """Unicode set for Latin-A Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0100, 0x017F),
         ]
 
     class LatinB(unicode_set):
-        "Unicode set for Latin-B Unicode Character Range"
+        """Unicode set for Latin-B Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0180, 0x024F),
         ]
 
     class Greek(unicode_set):
-        "Unicode set for Greek Unicode Character Ranges"
+        """Unicode set for Greek Unicode Character Ranges"""
         _ranges: UnicodeRangeList = [
             (0x0342, 0x0345),
             (0x0370, 0x0377),
@@ -193,7 +203,7 @@
         ]
 
     class Cyrillic(unicode_set):
-        "Unicode set for Cyrillic Unicode Character Range"
+        """Unicode set for Cyrillic Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0400, 0x052F),
             (0x1C80, 0x1C88),
@@ -206,7 +216,7 @@
         ]
 
     class Chinese(unicode_set):
-        "Unicode set for Chinese Unicode Character Range"
+        """Unicode set for Chinese Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x2E80, 0x2E99),
             (0x2E9B, 0x2EF3),
@@ -229,8 +239,7 @@
         ]
 
     class Japanese(unicode_set):
-        "Unicode set for Japanese Unicode Character Range, combining Kanji, Hiragana, and Katakana ranges"
-        _ranges: UnicodeRangeList = []
+        """Unicode set for Japanese Unicode Character Range, combining Kanji, Hiragana, and Katakana ranges"""
 
         class Kanji(unicode_set):
             "Unicode set for Kanji Unicode Character Range"
@@ -240,7 +249,7 @@
             ]
 
         class Hiragana(unicode_set):
-            "Unicode set for Hiragana Unicode Character Range"
+            """Unicode set for Hiragana Unicode Character Range"""
             _ranges: UnicodeRangeList = [
                 (0x3041, 0x3096),
                 (0x3099, 0x30A0),
@@ -252,7 +261,7 @@
             ]
 
         class Katakana(unicode_set):
-            "Unicode set for Katakana  Unicode Character Range"
+            """Unicode set for Katakana  Unicode Character Range"""
             _ranges: UnicodeRangeList = [
                 (0x3099, 0x309C),
                 (0x30A0, 0x30FF),
@@ -265,8 +274,18 @@
                 (0x1F213,),
             ]
 
+        漢字 = Kanji
+        カタカナ = Katakana
+        ひらがな = Hiragana
+
+        _ranges = (
+            Kanji._ranges
+            + Hiragana._ranges
+            + Katakana._ranges
+        )
+
     class Hangul(unicode_set):
-        "Unicode set for Hangul (Korean) Unicode Character Range"
+        """Unicode set for Hangul (Korean) Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x1100, 0x11FF),
             (0x302E, 0x302F),
@@ -288,17 +307,17 @@
     Korean = Hangul
 
     class CJK(Chinese, Japanese, Hangul):
-        "Unicode set for combined Chinese, Japanese, and Korean (CJK) Unicode Character Range"
+        """Unicode set for combined Chinese, Japanese, and Korean (CJK) Unicode Character Range"""
 
     class Thai(unicode_set):
-        "Unicode set for Thai Unicode Character Range"
+        """Unicode set for Thai Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0E01, 0x0E3A),
             (0x0E3F, 0x0E5B)
         ]
 
     class Arabic(unicode_set):
-        "Unicode set for Arabic Unicode Character Range"
+        """Unicode set for Arabic Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0600, 0x061B),
             (0x061E, 0x06FF),
@@ -306,7 +325,7 @@
         ]
 
     class Hebrew(unicode_set):
-        "Unicode set for Hebrew Unicode Character Range"
+        """Unicode set for Hebrew Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0591, 0x05C7),
             (0x05D0, 0x05EA),
@@ -320,33 +339,23 @@
         ]
 
     class Devanagari(unicode_set):
-        "Unicode set for Devanagari Unicode Character Range"
+        """Unicode set for Devanagari Unicode Character Range"""
         _ranges: UnicodeRangeList = [
             (0x0900, 0x097F),
             (0xA8E0, 0xA8FF)
         ]
 
-    # fmt: on
+    BMP = BasicMultilingualPlane
 
+    # add language identifiers using language Unicode
+    العربية = Arabic
+    中文 = Chinese
+    кириллица = Cyrillic
+    Ελληνικά = Greek
+    עִברִית = Hebrew
+    日本語 = Japanese
+    한국어 = Korean
+    ไทย = Thai
+    देवनागरी = Devanagari
 
-pyparsing_unicode.Japanese._ranges = (
-    pyparsing_unicode.Japanese.Kanji._ranges
-    + pyparsing_unicode.Japanese.Hiragana._ranges
-    + pyparsing_unicode.Japanese.Katakana._ranges
-)
-
-pyparsing_unicode.BMP = pyparsing_unicode.BasicMultilingualPlane
-
-# add language identifiers using language Unicode
-pyparsing_unicode.العربية = pyparsing_unicode.Arabic
-pyparsing_unicode.中文 = pyparsing_unicode.Chinese
-pyparsing_unicode.кириллица = pyparsing_unicode.Cyrillic
-pyparsing_unicode.Ελληνικά = pyparsing_unicode.Greek
-pyparsing_unicode.עִברִית = pyparsing_unicode.Hebrew
-pyparsing_unicode.日本語 = pyparsing_unicode.Japanese
-pyparsing_unicode.Japanese.漢字 = pyparsing_unicode.Japanese.Kanji
-pyparsing_unicode.Japanese.カタカナ = pyparsing_unicode.Japanese.Katakana
-pyparsing_unicode.Japanese.ひらがな = pyparsing_unicode.Japanese.Hiragana
-pyparsing_unicode.한국어 = pyparsing_unicode.Korean
-pyparsing_unicode.ไทย = pyparsing_unicode.Thai
-pyparsing_unicode.देवनागरी = pyparsing_unicode.Devanagari
+    # fmt: on
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/util.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/util.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/pyparsing/util.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/pyparsing/util.py	2023-07-15 11:52:27.574474434 -0400
@@ -1,12 +1,14 @@
 # util.py
+import inspect
 import warnings
 import types
 import collections
 import itertools
-from functools import lru_cache
-from typing import List, Union, Iterable
+from functools import lru_cache, wraps
+from typing import Callable, List, Union, Iterable, TypeVar, cast
 
 _bslash = chr(92)
+C = TypeVar("C", bound=Callable)
 
 
 class __config_flags:
@@ -20,18 +22,15 @@
     def _set(cls, dname, value):
         if dname in cls._fixed_names:
             warnings.warn(
-                "{}.{} {} is {} and cannot be overridden".format(
-                    cls.__name__,
-                    dname,
-                    cls._type_desc,
-                    str(getattr(cls, dname)).upper(),
-                )
+                f"{cls.__name__}.{dname} {cls._type_desc} is {str(getattr(cls, dname)).upper()}"
+                f" and cannot be overridden",
+                stacklevel=3,
             )
             return
         if dname in cls._all_names:
             setattr(cls, dname, value)
         else:
-            raise ValueError("no such {} {!r}".format(cls._type_desc, dname))
+            raise ValueError(f"no such {cls._type_desc} {dname!r}")
 
     enable = classmethod(lambda cls, name: cls._set(name, True))
     disable = classmethod(lambda cls, name: cls._set(name, False))
@@ -45,7 +44,7 @@
 
     Note: the default parsing behavior is to expand tabs in the input string
     before starting the parsing process.  See
-    :class:`ParserElement.parseString` for more
+    :class:`ParserElement.parse_string` for more
     information on parsing strings containing ``<TAB>`` s, and suggested
     methods to maintain a consistent view of the parsed string, the parse
     location, and line and column positions within the parsed string.
@@ -60,7 +59,7 @@
     The first line is number 1.
 
     Note - the default parsing behavior is to expand tabs in the input string
-    before starting the parsing process.  See :class:`ParserElement.parseString`
+    before starting the parsing process.  See :class:`ParserElement.parse_string`
     for more information on parsing strings containing ``<TAB>`` s, and
     suggested methods to maintain a consistent view of the parsed string, the
     parse location, and line and column positions within the parsed string.
@@ -102,19 +101,24 @@
 class _FifoCache:
     def __init__(self, size):
         self.not_in_cache = not_in_cache = object()
-        cache = collections.OrderedDict()
+        cache = {}
+        keyring = [object()] * size
         cache_get = cache.get
+        cache_pop = cache.pop
+        keyiter = itertools.cycle(range(size))
 
         def get(_, key):
             return cache_get(key, not_in_cache)
 
         def set_(_, key, value):
             cache[key] = value
-            while len(cache) > size:
-                cache.popitem(last=False)
+            i = next(keyiter)
+            cache_pop(keyring[i], None)
+            keyring[i] = key
 
         def clear(_):
             cache.clear()
+            keyring[:] = [object()] * size
 
         self.size = size
         self.get = types.MethodType(get, self)
@@ -189,9 +193,9 @@
             is_consecutive.value = next(is_consecutive.counter)
         return is_consecutive.value
 
-    is_consecutive.prev = 0
-    is_consecutive.counter = itertools.count()
-    is_consecutive.value = -1
+    is_consecutive.prev = 0  # type: ignore [attr-defined]
+    is_consecutive.counter = itertools.count()  # type: ignore [attr-defined]
+    is_consecutive.value = -1  # type: ignore [attr-defined]
 
     def escape_re_range_char(c):
         return "\\" + c if c in r"\^-][" else c
@@ -215,9 +219,7 @@
             else:
                 sep = "" if ord(last) == ord(first) + 1 else "-"
                 ret.append(
-                    "{}{}{}".format(
-                        escape_re_range_char(first), sep, escape_re_range_char(last)
-                    )
+                    f"{escape_re_range_char(first)}{sep}{escape_re_range_char(last)}"
                 )
     else:
         ret = [escape_re_range_char(c) for c in s]
@@ -233,3 +235,50 @@
         else:
             ret.append(i)
     return ret
+
+
+def _make_synonym_function(compat_name: str, fn: C) -> C:
+    # In a future version, uncomment the code in the internal _inner() functions
+    # to begin emitting DeprecationWarnings.
+
+    # Unwrap staticmethod/classmethod
+    fn = getattr(fn, "__func__", fn)
+
+    # (Presence of 'self' arg in signature is used by explain_exception() methods, so we take
+    # some extra steps to add it if present in decorated function.)
+    if "self" == list(inspect.signature(fn).parameters)[0]:
+
+        @wraps(fn)
+        def _inner(self, *args, **kwargs):
+            # warnings.warn(
+            #     f"Deprecated - use {fn.__name__}", DeprecationWarning, stacklevel=3
+            # )
+            return fn(self, *args, **kwargs)
+
+    else:
+
+        @wraps(fn)
+        def _inner(*args, **kwargs):
+            # warnings.warn(
+            #     f"Deprecated - use {fn.__name__}", DeprecationWarning, stacklevel=3
+            # )
+            return fn(*args, **kwargs)
+
+    _inner.__doc__ = f"""Deprecated - use :class:`{fn.__name__}`"""
+    _inner.__name__ = compat_name
+    _inner.__annotations__ = fn.__annotations__
+    if isinstance(fn, types.FunctionType):
+        _inner.__kwdefaults__ = fn.__kwdefaults__
+    elif isinstance(fn, type) and hasattr(fn, "__init__"):
+        _inner.__kwdefaults__ = fn.__init__.__kwdefaults__
+    else:
+        _inner.__kwdefaults__ = None
+    _inner.__qualname__ = fn.__qualname__
+    return cast(C, _inner)
+
+
+def replaced_by_pep8(fn: C) -> Callable[[Callable], C]:
+    """
+    Decorator for pre-PEP8 compatibility synonyms, to link them to the new function.
+    """
+    return lambda other: _make_synonym_function(other.__name__, fn)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/adapters.py	2023-07-15 11:52:27.562474434 -0400
@@ -22,7 +22,6 @@
 from pip._vendor.urllib3.exceptions import ReadTimeoutError, ResponseError
 from pip._vendor.urllib3.exceptions import SSLError as _SSLError
 from pip._vendor.urllib3.poolmanager import PoolManager, proxy_from_url
-from pip._vendor.urllib3.response import HTTPResponse
 from pip._vendor.urllib3.util import Timeout as TimeoutSauce
 from pip._vendor.urllib3.util import parse_url
 from pip._vendor.urllib3.util.retry import Retry
@@ -194,7 +193,6 @@
             num_pools=connections,
             maxsize=maxsize,
             block=block,
-            strict=True,
             **pool_kwargs,
         )
 
@@ -485,63 +483,19 @@
             timeout = TimeoutSauce(connect=timeout, read=timeout)
 
         try:
-            if not chunked:
-                resp = conn.urlopen(
-                    method=request.method,
-                    url=url,
-                    body=request.body,
-                    headers=request.headers,
-                    redirect=False,
-                    assert_same_host=False,
-                    preload_content=False,
-                    decode_content=False,
-                    retries=self.max_retries,
-                    timeout=timeout,
-                )
-
-            # Send the request.
-            else:
-                if hasattr(conn, "proxy_pool"):
-                    conn = conn.proxy_pool
-
-                low_conn = conn._get_conn(timeout=DEFAULT_POOL_TIMEOUT)
-
-                try:
-                    skip_host = "Host" in request.headers
-                    low_conn.putrequest(
-                        request.method,
-                        url,
-                        skip_accept_encoding=True,
-                        skip_host=skip_host,
-                    )
-
-                    for header, value in request.headers.items():
-                        low_conn.putheader(header, value)
-
-                    low_conn.endheaders()
-
-                    for i in request.body:
-                        low_conn.send(hex(len(i))[2:].encode("utf-8"))
-                        low_conn.send(b"\r\n")
-                        low_conn.send(i)
-                        low_conn.send(b"\r\n")
-                    low_conn.send(b"0\r\n\r\n")
-
-                    # Receive the response from the server
-                    r = low_conn.getresponse()
-
-                    resp = HTTPResponse.from_httplib(
-                        r,
-                        pool=conn,
-                        connection=low_conn,
-                        preload_content=False,
-                        decode_content=False,
-                    )
-                except Exception:
-                    # If we hit any problems here, clean up the connection.
-                    # Then, raise so that we can handle the actual exception.
-                    low_conn.close()
-                    raise
+            resp = conn.urlopen(
+                method=request.method,
+                url=url,
+                body=request.body,
+                headers=request.headers,
+                redirect=False,
+                assert_same_host=False,
+                preload_content=False,
+                decode_content=False,
+                retries=self.max_retries,
+                timeout=timeout,
+                chunked=chunked,
+            )
 
         except (ProtocolError, OSError) as err:
             raise ConnectionError(err, request=request)
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/api.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/api.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/api.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/api.py	2023-07-15 11:52:27.562474434 -0400
@@ -106,7 +106,7 @@
     :param url: URL for the new :class:`Request` object.
     :param data: (optional) Dictionary, list of tuples, bytes, or file-like
         object to send in the body of the :class:`Request`.
-    :param json: (optional) json data to send in the body of the :class:`Request`.
+    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
     :param \*\*kwargs: Optional arguments that ``request`` takes.
     :return: :class:`Response <Response>` object
     :rtype: requests.Response
@@ -121,7 +121,7 @@
     :param url: URL for the new :class:`Request` object.
     :param data: (optional) Dictionary, list of tuples, bytes, or file-like
         object to send in the body of the :class:`Request`.
-    :param json: (optional) json data to send in the body of the :class:`Request`.
+    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
     :param \*\*kwargs: Optional arguments that ``request`` takes.
     :return: :class:`Response <Response>` object
     :rtype: requests.Response
@@ -136,7 +136,7 @@
     :param url: URL for the new :class:`Request` object.
     :param data: (optional) Dictionary, list of tuples, bytes, or file-like
         object to send in the body of the :class:`Request`.
-    :param json: (optional) json data to send in the body of the :class:`Request`.
+    :param json: (optional) A JSON serializable Python object to send in the body of the :class:`Request`.
     :param \*\*kwargs: Optional arguments that ``request`` takes.
     :return: :class:`Response <Response>` object
     :rtype: requests.Response
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/__init__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/__init__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/__init__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/__init__.py	2023-07-15 11:52:27.562474434 -0400
@@ -63,10 +63,10 @@
     # Check urllib3 for compatibility.
     major, minor, patch = urllib3_version  # noqa: F811
     major, minor, patch = int(major), int(minor), int(patch)
-    # urllib3 >= 1.21.1, <= 1.26
-    assert major == 1
-    assert minor >= 21
-    assert minor <= 26
+    # urllib3 >= 1.21.1
+    assert major >= 1
+    if major == 1:
+        assert minor >= 21
 
     # Check charset_normalizer for compatibility.
     if chardet_version:
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/_internal_utils.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/_internal_utils.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/_internal_utils.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/_internal_utils.py	2023-07-15 11:52:27.562474434 -0400
@@ -14,9 +14,11 @@
 _VALID_HEADER_VALUE_RE_BYTE = re.compile(rb"^\S[^\r\n]*$|^$")
 _VALID_HEADER_VALUE_RE_STR = re.compile(r"^\S[^\r\n]*$|^$")
 
+_HEADER_VALIDATORS_STR = (_VALID_HEADER_NAME_RE_STR, _VALID_HEADER_VALUE_RE_STR)
+_HEADER_VALIDATORS_BYTE = (_VALID_HEADER_NAME_RE_BYTE, _VALID_HEADER_VALUE_RE_BYTE)
 HEADER_VALIDATORS = {
-    bytes: (_VALID_HEADER_NAME_RE_BYTE, _VALID_HEADER_VALUE_RE_BYTE),
-    str: (_VALID_HEADER_NAME_RE_STR, _VALID_HEADER_VALUE_RE_STR),
+    bytes: _HEADER_VALIDATORS_BYTE,
+    str: _HEADER_VALIDATORS_STR,
 }
 
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/sessions.py	2023-07-15 11:52:27.563474434 -0400
@@ -324,7 +324,9 @@
         except KeyError:
             username, password = None, None
 
-        if username and password:
+        # urllib3 handles proxy authorization for us in the standard adapter.
+        # Avoid appending this to TLS tunneled requests where it may be leaked.
+        if not scheme.startswith('https') and username and password:
             headers["Proxy-Authorization"] = _basic_auth_str(username, password)
 
         return new_proxies
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/utils.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/utils.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/utils.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/utils.py	2023-07-15 11:52:27.563474434 -0400
@@ -25,7 +25,12 @@
 from .__version__ import __version__
 
 # to_native_string is unused here, but imported here for backwards compatibility
-from ._internal_utils import HEADER_VALIDATORS, to_native_string  # noqa: F401
+from ._internal_utils import (  # noqa: F401
+    _HEADER_VALIDATORS_BYTE,
+    _HEADER_VALIDATORS_STR,
+    HEADER_VALIDATORS,
+    to_native_string,
+)
 from .compat import (
     Mapping,
     basestring,
@@ -1031,20 +1036,23 @@
     :param header: tuple, in the format (name, value).
     """
     name, value = header
+    _validate_header_part(header, name, 0)
+    _validate_header_part(header, value, 1)
 
-    for part in header:
-        if type(part) not in HEADER_VALIDATORS:
-            raise InvalidHeader(
-                f"Header part ({part!r}) from {{{name!r}: {value!r}}} must be "
-                f"of type str or bytes, not {type(part)}"
-            )
-
-    _validate_header_part(name, "name", HEADER_VALIDATORS[type(name)][0])
-    _validate_header_part(value, "value", HEADER_VALIDATORS[type(value)][1])
 
+def _validate_header_part(header, header_part, header_validator_index):
+    if isinstance(header_part, str):
+        validator = _HEADER_VALIDATORS_STR[header_validator_index]
+    elif isinstance(header_part, bytes):
+        validator = _HEADER_VALIDATORS_BYTE[header_validator_index]
+    else:
+        raise InvalidHeader(
+            f"Header part ({header_part!r}) from {header} "
+            f"must be of type str or bytes, not {type(header_part)}"
+        )
 
-def _validate_header_part(header_part, header_kind, validator):
     if not validator.match(header_part):
+        header_kind = "name" if header_validator_index == 0 else "value"
         raise InvalidHeader(
             f"Invalid leading whitespace, reserved character(s), or return"
             f"character(s) in header {header_kind}: {header_part!r}"
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/__version__.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/__version__.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/requests/__version__.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/requests/__version__.py	2023-07-15 11:52:27.562474434 -0400
@@ -5,8 +5,8 @@
 __title__ = "requests"
 __description__ = "Python HTTP for Humans."
 __url__ = "https://requests.readthedocs.io"
-__version__ = "2.28.2"
-__build__ = 0x022802
+__version__ = "2.31.0"
+__build__ = 0x023100
 __author__ = "Kenneth Reitz"
 __author_email__ = "me@kennethreitz.org"
 __license__ = "Apache 2.0"
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/rich/console.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/rich/console.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/rich/console.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/rich/console.py	2023-07-15 11:52:27.569474434 -0400
@@ -952,6 +952,7 @@
         force_color = self._environ.get("FORCE_COLOR")
         if force_color is not None:
             self._force_terminal = True
+            return True
 
         isatty: Optional[Callable[[], bool]] = getattr(self.file, "isatty", None)
         try:
@@ -2000,7 +2001,6 @@
                     self._record_buffer.extend(self._buffer[:])
 
             if self._buffer_index == 0:
-
                 if self.is_jupyter:  # pragma: no cover
                     from .jupyter import display
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/rich/syntax.py	2023-07-15 11:52:27.570474434 -0400
@@ -590,7 +590,6 @@
     def __rich_measure__(
         self, console: "Console", options: "ConsoleOptions"
     ) -> "Measurement":
-
         _, right, _, left = Padding.unpack(self.padding)
         padding = left + right
         if self.code_width is not None:
@@ -688,7 +687,7 @@
             lines = (
                 Text("\n")
                 .join(lines)
-                .with_indent_guides(self.tab_size, style=style)
+                .with_indent_guides(self.tab_size, style=style + Style(italic=False))
                 .split("\n", allow_blank=True)
             )
 
@@ -830,7 +829,6 @@
 
 
 if __name__ == "__main__":  # pragma: no cover
-
     import argparse
     import sys
 
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/typing_extensions.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/typing_extensions.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/typing_extensions.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/typing_extensions.py	2023-07-15 11:52:27.564474434 -0400
@@ -9,7 +9,6 @@
 import typing
 import warnings
 
-
 __all__ = [
     # Super-special typing primitives.
     'Any',
@@ -33,6 +32,7 @@
     'Coroutine',
     'AsyncGenerator',
     'AsyncContextManager',
+    'Buffer',
     'ChainMap',
 
     # Concrete collection types.
@@ -45,7 +45,13 @@
     'TypedDict',
 
     # Structural checks, a.k.a. protocols.
+    'SupportsAbs',
+    'SupportsBytes',
+    'SupportsComplex',
+    'SupportsFloat',
     'SupportsIndex',
+    'SupportsInt',
+    'SupportsRound',
 
     # One-off things.
     'Annotated',
@@ -58,8 +64,11 @@
     'final',
     'get_args',
     'get_origin',
+    'get_original_bases',
+    'get_protocol_members',
     'get_type_hints',
     'IntVar',
+    'is_protocol',
     'is_typeddict',
     'Literal',
     'NewType',
@@ -71,12 +80,52 @@
     'runtime_checkable',
     'Text',
     'TypeAlias',
+    'TypeAliasType',
     'TypeGuard',
     'TYPE_CHECKING',
     'Never',
     'NoReturn',
     'Required',
     'NotRequired',
+
+    # Pure aliases, have always been in typing
+    'AbstractSet',
+    'AnyStr',
+    'BinaryIO',
+    'Callable',
+    'Collection',
+    'Container',
+    'Dict',
+    'ForwardRef',
+    'FrozenSet',
+    'Generator',
+    'Generic',
+    'Hashable',
+    'IO',
+    'ItemsView',
+    'Iterable',
+    'Iterator',
+    'KeysView',
+    'List',
+    'Mapping',
+    'MappingView',
+    'Match',
+    'MutableMapping',
+    'MutableSequence',
+    'MutableSet',
+    'Optional',
+    'Pattern',
+    'Reversible',
+    'Sequence',
+    'Set',
+    'Sized',
+    'TextIO',
+    'Tuple',
+    'Union',
+    'ValuesView',
+    'cast',
+    'no_type_check',
+    'no_type_check_decorator',
 ]
 
 # for backward compatibility
@@ -86,7 +135,13 @@
 # The functions below are modified copies of typing internal helpers.
 # They are needed by _ProtocolMeta and they provide support for PEP 646.
 
-_marker = object()
+
+class _Sentinel:
+    def __repr__(self):
+        return "<sentinel>"
+
+
+_marker = _Sentinel()
 
 
 def _check_generic(cls, parameters, elen=_marker):
@@ -187,17 +242,19 @@
 
 ClassVar = typing.ClassVar
 
+
+class _ExtensionsSpecialForm(typing._SpecialForm, _root=True):
+    def __repr__(self):
+        return 'typing_extensions.' + self._name
+
+
 # On older versions of typing there is an internal class named "Final".
 # 3.8+
 if hasattr(typing, 'Final') and sys.version_info[:2] >= (3, 7):
     Final = typing.Final
 # 3.7
 else:
-    class _FinalForm(typing._SpecialForm, _root=True):
-
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
+    class _FinalForm(_ExtensionsSpecialForm, _root=True):
         def __getitem__(self, parameters):
             item = typing._type_check(parameters,
                                       f'{self._name} accepts only a single type.')
@@ -260,21 +317,67 @@
     return typing.TypeVar(name)
 
 
-# 3.8+:
-if hasattr(typing, 'Literal'):
+# A Literal bug was fixed in 3.11.0, 3.10.1 and 3.9.8
+if sys.version_info >= (3, 10, 1):
     Literal = typing.Literal
-# 3.7:
 else:
-    class _LiteralForm(typing._SpecialForm, _root=True):
+    def _flatten_literal_params(parameters):
+        """An internal helper for Literal creation: flatten Literals among parameters"""
+        params = []
+        for p in parameters:
+            if isinstance(p, _LiteralGenericAlias):
+                params.extend(p.__args__)
+            else:
+                params.append(p)
+        return tuple(params)
 
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
+    def _value_and_type_iter(params):
+        for p in params:
+            yield p, type(p)
+
+    class _LiteralGenericAlias(typing._GenericAlias, _root=True):
+        def __eq__(self, other):
+            if not isinstance(other, _LiteralGenericAlias):
+                return NotImplemented
+            these_args_deduped = set(_value_and_type_iter(self.__args__))
+            other_args_deduped = set(_value_and_type_iter(other.__args__))
+            return these_args_deduped == other_args_deduped
+
+        def __hash__(self):
+            return hash(frozenset(_value_and_type_iter(self.__args__)))
+
+    class _LiteralForm(_ExtensionsSpecialForm, _root=True):
+        def __init__(self, doc: str):
+            self._name = 'Literal'
+            self._doc = self.__doc__ = doc
 
         def __getitem__(self, parameters):
-            return typing._GenericAlias(self, parameters)
+            if not isinstance(parameters, tuple):
+                parameters = (parameters,)
+
+            parameters = _flatten_literal_params(parameters)
 
-    Literal = _LiteralForm('Literal',
-                           doc="""A type that can be used to indicate to type checkers
+            val_type_pairs = list(_value_and_type_iter(parameters))
+            try:
+                deduped_pairs = set(val_type_pairs)
+            except TypeError:
+                # unhashable parameters
+                pass
+            else:
+                # similar logic to typing._deduplicate on Python 3.9+
+                if len(deduped_pairs) < len(val_type_pairs):
+                    new_parameters = []
+                    for pair in val_type_pairs:
+                        if pair in deduped_pairs:
+                            new_parameters.append(pair[0])
+                            deduped_pairs.remove(pair)
+                    assert not deduped_pairs, deduped_pairs
+                    parameters = tuple(new_parameters)
+
+            return _LiteralGenericAlias(self, parameters)
+
+    Literal = _LiteralForm(doc="""\
+                           A type that can be used to indicate to type checkers
                            that the corresponding value has a value literally equivalent
                            to the provided parameter. For example:
 
@@ -288,7 +391,7 @@
                            instead of a type.""")
 
 
-_overload_dummy = typing._overload_dummy  # noqa
+_overload_dummy = typing._overload_dummy
 
 
 if hasattr(typing, "get_overloads"):  # 3.11+
@@ -383,40 +486,55 @@
 Counter = typing.Counter
 ChainMap = typing.ChainMap
 AsyncGenerator = typing.AsyncGenerator
-NewType = typing.NewType
 Text = typing.Text
 TYPE_CHECKING = typing.TYPE_CHECKING
 
 
-_PROTO_WHITELIST = ['Callable', 'Awaitable',
-                    'Iterable', 'Iterator', 'AsyncIterable', 'AsyncIterator',
-                    'Hashable', 'Sized', 'Container', 'Collection', 'Reversible',
-                    'ContextManager', 'AsyncContextManager']
+_PROTO_ALLOWLIST = {
+    'collections.abc': [
+        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',
+        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible', 'Buffer',
+    ],
+    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],
+    'typing_extensions': ['Buffer'],
+}
+
+
+_EXCLUDED_ATTRS = {
+    "__abstractmethods__", "__annotations__", "__weakref__", "_is_protocol",
+    "_is_runtime_protocol", "__dict__", "__slots__", "__parameters__",
+    "__orig_bases__", "__module__", "_MutableMapping__marker", "__doc__",
+    "__subclasshook__", "__orig_class__", "__init__", "__new__",
+    "__protocol_attrs__", "__callable_proto_members_only__",
+}
+
+if sys.version_info < (3, 8):
+    _EXCLUDED_ATTRS |= {
+        "_gorg", "__next_in_mro__", "__extra__", "__tree_hash__", "__args__",
+        "__origin__"
+    }
+
+if sys.version_info >= (3, 9):
+    _EXCLUDED_ATTRS.add("__class_getitem__")
+
+if sys.version_info >= (3, 12):
+    _EXCLUDED_ATTRS.add("__type_params__")
+
+_EXCLUDED_ATTRS = frozenset(_EXCLUDED_ATTRS)
 
 
 def _get_protocol_attrs(cls):
     attrs = set()
     for base in cls.__mro__[:-1]:  # without object
-        if base.__name__ in ('Protocol', 'Generic'):
+        if base.__name__ in {'Protocol', 'Generic'}:
             continue
         annotations = getattr(base, '__annotations__', {})
-        for attr in list(base.__dict__.keys()) + list(annotations.keys()):
-            if (not attr.startswith('_abc_') and attr not in (
-                    '__abstractmethods__', '__annotations__', '__weakref__',
-                    '_is_protocol', '_is_runtime_protocol', '__dict__',
-                    '__args__', '__slots__',
-                    '__next_in_mro__', '__parameters__', '__origin__',
-                    '__orig_bases__', '__extra__', '__tree_hash__',
-                    '__doc__', '__subclasshook__', '__init__', '__new__',
-                    '__module__', '_MutableMapping__marker', '_gorg')):
+        for attr in (*base.__dict__, *annotations):
+            if (not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS):
                 attrs.add(attr)
     return attrs
 
 
-def _is_callable_members_only(cls):
-    return all(callable(getattr(cls, attr, None)) for attr in _get_protocol_attrs(cls))
-
-
 def _maybe_adjust_parameters(cls):
     """Helper function used in Protocol.__init_subclass__ and _TypedDictMeta.__new__.
 
@@ -426,7 +544,7 @@
     """
     tvars = []
     if '__orig_bases__' in cls.__dict__:
-        tvars = typing._collect_type_vars(cls.__orig_bases__)
+        tvars = _collect_type_vars(cls.__orig_bases__)
         # Look for Generic[T1, ..., Tn] or Protocol[T1, ..., Tn].
         # If found, tvars must be a subset of it.
         # If not found, tvars is it.
@@ -457,168 +575,284 @@
     cls.__parameters__ = tuple(tvars)
 
 
-# 3.8+
-if hasattr(typing, 'Protocol'):
+def _caller(depth=2):
+    try:
+        return sys._getframe(depth).f_globals.get('__name__', '__main__')
+    except (AttributeError, ValueError):  # For platforms without _getframe()
+        return None
+
+
+# The performance of runtime-checkable protocols is significantly improved on Python 3.12,
+# so we backport the 3.12 version of Protocol to Python <=3.11
+if sys.version_info >= (3, 12):
     Protocol = typing.Protocol
-# 3.7
 else:
+    def _allow_reckless_class_checks(depth=3):
+        """Allow instance and class checks for special stdlib modules.
+        The abc and functools modules indiscriminately call isinstance() and
+        issubclass() on the whole MRO of a user class, which may contain protocols.
+        """
+        return _caller(depth) in {'abc', 'functools', None}
 
     def _no_init(self, *args, **kwargs):
         if type(self)._is_protocol:
             raise TypeError('Protocols cannot be instantiated')
 
-    class _ProtocolMeta(abc.ABCMeta):  # noqa: B024
-        # This metaclass is a bit unfortunate and exists only because of the lack
-        # of __instancehook__.
+    if sys.version_info >= (3, 8):
+        # Inheriting from typing._ProtocolMeta isn't actually desirable,
+        # but is necessary to allow typing.Protocol and typing_extensions.Protocol
+        # to mix without getting TypeErrors about "metaclass conflict"
+        _typing_Protocol = typing.Protocol
+        _ProtocolMetaBase = type(_typing_Protocol)
+    else:
+        _typing_Protocol = _marker
+        _ProtocolMetaBase = abc.ABCMeta
+
+    class _ProtocolMeta(_ProtocolMetaBase):
+        # This metaclass is somewhat unfortunate,
+        # but is necessary for several reasons...
+        #
+        # NOTE: DO NOT call super() in any methods in this class
+        # That would call the methods on typing._ProtocolMeta on Python 3.8-3.11
+        # and those are slow
+        def __new__(mcls, name, bases, namespace, **kwargs):
+            if name == "Protocol" and len(bases) < 2:
+                pass
+            elif {Protocol, _typing_Protocol} & set(bases):
+                for base in bases:
+                    if not (
+                        base in {object, typing.Generic, Protocol, _typing_Protocol}
+                        or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__, [])
+                        or is_protocol(base)
+                    ):
+                        raise TypeError(
+                            f"Protocols can only inherit from other protocols, "
+                            f"got {base!r}"
+                        )
+            return abc.ABCMeta.__new__(mcls, name, bases, namespace, **kwargs)
+
+        def __init__(cls, *args, **kwargs):
+            abc.ABCMeta.__init__(cls, *args, **kwargs)
+            if getattr(cls, "_is_protocol", False):
+                cls.__protocol_attrs__ = _get_protocol_attrs(cls)
+                # PEP 544 prohibits using issubclass()
+                # with protocols that have non-method members.
+                cls.__callable_proto_members_only__ = all(
+                    callable(getattr(cls, attr, None)) for attr in cls.__protocol_attrs__
+                )
+
+        def __subclasscheck__(cls, other):
+            if cls is Protocol:
+                return type.__subclasscheck__(cls, other)
+            if (
+                getattr(cls, '_is_protocol', False)
+                and not _allow_reckless_class_checks()
+            ):
+                if not isinstance(other, type):
+                    # Same error message as for issubclass(1, int).
+                    raise TypeError('issubclass() arg 1 must be a class')
+                if (
+                    not cls.__callable_proto_members_only__
+                    and cls.__dict__.get("__subclasshook__") is _proto_hook
+                ):
+                    raise TypeError(
+                        "Protocols with non-method members don't support issubclass()"
+                    )
+                if not getattr(cls, '_is_runtime_protocol', False):
+                    raise TypeError(
+                        "Instance and class checks can only be used with "
+                        "@runtime_checkable protocols"
+                    )
+            return abc.ABCMeta.__subclasscheck__(cls, other)
+
         def __instancecheck__(cls, instance):
             # We need this method for situations where attributes are
             # assigned in __init__.
-            if ((not getattr(cls, '_is_protocol', False) or
-                 _is_callable_members_only(cls)) and
-                    issubclass(instance.__class__, cls)):
-                return True
-            if cls._is_protocol:
-                if all(hasattr(instance, attr) and
-                       (not callable(getattr(cls, attr, None)) or
-                        getattr(instance, attr) is not None)
-                       for attr in _get_protocol_attrs(cls)):
-                    return True
-            return super().__instancecheck__(instance)
-
-    class Protocol(metaclass=_ProtocolMeta):
-        # There is quite a lot of overlapping code with typing.Generic.
-        # Unfortunately it is hard to avoid this while these live in two different
-        # modules. The duplicated code will be removed when Protocol is moved to typing.
-        """Base class for protocol classes. Protocol classes are defined as::
-
-            class Proto(Protocol):
-                def meth(self) -> int:
-                    ...
-
-        Such classes are primarily used with static type checkers that recognize
-        structural subtyping (static duck-typing), for example::
+            if cls is Protocol:
+                return type.__instancecheck__(cls, instance)
+            if not getattr(cls, "_is_protocol", False):
+                # i.e., it's a concrete subclass of a protocol
+                return abc.ABCMeta.__instancecheck__(cls, instance)
+
+            if (
+                not getattr(cls, '_is_runtime_protocol', False) and
+                not _allow_reckless_class_checks()
+            ):
+                raise TypeError("Instance and class checks can only be used with"
+                                " @runtime_checkable protocols")
 
-            class C:
-                def meth(self) -> int:
-                    return 0
+            if abc.ABCMeta.__instancecheck__(cls, instance):
+                return True
 
-            def func(x: Proto) -> int:
-                return x.meth()
+            for attr in cls.__protocol_attrs__:
+                try:
+                    val = inspect.getattr_static(instance, attr)
+                except AttributeError:
+                    break
+                if val is None and callable(getattr(cls, attr, None)):
+                    break
+            else:
+                return True
 
-            func(C())  # Passes static type check
+            return False
 
-        See PEP 544 for details. Protocol classes decorated with
-        @typing_extensions.runtime act as simple-minded runtime protocol that checks
-        only the presence of given attributes, ignoring their type signatures.
+        def __eq__(cls, other):
+            # Hack so that typing.Generic.__class_getitem__
+            # treats typing_extensions.Protocol
+            # as equivalent to typing.Protocol on Python 3.8+
+            if abc.ABCMeta.__eq__(cls, other) is True:
+                return True
+            return (
+                cls is Protocol and other is getattr(typing, "Protocol", object())
+            )
 
-        Protocol classes can be generic, they are defined as::
+        # This has to be defined, or the abc-module cache
+        # complains about classes with this metaclass being unhashable,
+        # if we define only __eq__!
+        def __hash__(cls) -> int:
+            return type.__hash__(cls)
+
+    @classmethod
+    def _proto_hook(cls, other):
+        if not cls.__dict__.get('_is_protocol', False):
+            return NotImplemented
+
+        for attr in cls.__protocol_attrs__:
+            for base in other.__mro__:
+                # Check if the members appears in the class dictionary...
+                if attr in base.__dict__:
+                    if base.__dict__[attr] is None:
+                        return NotImplemented
+                    break
 
-            class GenProto(Protocol[T]):
-                def meth(self) -> T:
-                    ...
-        """
-        __slots__ = ()
-        _is_protocol = True
+                # ...or in annotations, if it is a sub-protocol.
+                annotations = getattr(base, '__annotations__', {})
+                if (
+                    isinstance(annotations, collections.abc.Mapping)
+                    and attr in annotations
+                    and is_protocol(other)
+                ):
+                    break
+            else:
+                return NotImplemented
+        return True
 
-        def __new__(cls, *args, **kwds):
-            if cls is Protocol:
-                raise TypeError("Type Protocol cannot be instantiated; "
-                                "it can only be used as a base class")
-            return super().__new__(cls)
+    if sys.version_info >= (3, 8):
+        class Protocol(typing.Generic, metaclass=_ProtocolMeta):
+            __doc__ = typing.Protocol.__doc__
+            __slots__ = ()
+            _is_protocol = True
+            _is_runtime_protocol = False
+
+            def __init_subclass__(cls, *args, **kwargs):
+                super().__init_subclass__(*args, **kwargs)
+
+                # Determine if this is a protocol or a concrete subclass.
+                if not cls.__dict__.get('_is_protocol', False):
+                    cls._is_protocol = any(b is Protocol for b in cls.__bases__)
+
+                # Set (or override) the protocol subclass hook.
+                if '__subclasshook__' not in cls.__dict__:
+                    cls.__subclasshook__ = _proto_hook
+
+                # Prohibit instantiation for protocol classes
+                if cls._is_protocol and cls.__init__ is Protocol.__init__:
+                    cls.__init__ = _no_init
 
-        @typing._tp_cache
-        def __class_getitem__(cls, params):
-            if not isinstance(params, tuple):
-                params = (params,)
-            if not params and cls is not typing.Tuple:
-                raise TypeError(
-                    f"Parameter list to {cls.__qualname__}[...] cannot be empty")
-            msg = "Parameters to generic types must be types."
-            params = tuple(typing._type_check(p, msg) for p in params)  # noqa
-            if cls is Protocol:
-                # Generic can only be subscripted with unique type variables.
-                if not all(isinstance(p, typing.TypeVar) for p in params):
-                    i = 0
-                    while isinstance(params[i], typing.TypeVar):
-                        i += 1
-                    raise TypeError(
-                        "Parameters to Protocol[...] must all be type variables."
-                        f" Parameter {i + 1} is {params[i]}")
-                if len(set(params)) != len(params):
+    else:
+        class Protocol(metaclass=_ProtocolMeta):
+            # There is quite a lot of overlapping code with typing.Generic.
+            # Unfortunately it is hard to avoid this on Python <3.8,
+            # as the typing module on Python 3.7 doesn't let us subclass typing.Generic!
+            """Base class for protocol classes. Protocol classes are defined as::
+
+                class Proto(Protocol):
+                    def meth(self) -> int:
+                        ...
+
+            Such classes are primarily used with static type checkers that recognize
+            structural subtyping (static duck-typing), for example::
+
+                class C:
+                    def meth(self) -> int:
+                        return 0
+
+                def func(x: Proto) -> int:
+                    return x.meth()
+
+                func(C())  # Passes static type check
+
+            See PEP 544 for details. Protocol classes decorated with
+            @typing_extensions.runtime_checkable act
+            as simple-minded runtime-checkable protocols that check
+            only the presence of given attributes, ignoring their type signatures.
+
+            Protocol classes can be generic, they are defined as::
+
+                class GenProto(Protocol[T]):
+                    def meth(self) -> T:
+                        ...
+            """
+            __slots__ = ()
+            _is_protocol = True
+            _is_runtime_protocol = False
+
+            def __new__(cls, *args, **kwds):
+                if cls is Protocol:
+                    raise TypeError("Type Protocol cannot be instantiated; "
+                                    "it can only be used as a base class")
+                return super().__new__(cls)
+
+            @typing._tp_cache
+            def __class_getitem__(cls, params):
+                if not isinstance(params, tuple):
+                    params = (params,)
+                if not params and cls is not typing.Tuple:
                     raise TypeError(
-                        "Parameters to Protocol[...] must all be unique")
-            else:
-                # Subscripting a regular Generic subclass.
-                _check_generic(cls, params, len(cls.__parameters__))
-            return typing._GenericAlias(cls, params)
-
-        def __init_subclass__(cls, *args, **kwargs):
-            if '__orig_bases__' in cls.__dict__:
-                error = typing.Generic in cls.__orig_bases__
-            else:
-                error = typing.Generic in cls.__bases__
-            if error:
-                raise TypeError("Cannot inherit from plain Generic")
-            _maybe_adjust_parameters(cls)
-
-            # Determine if this is a protocol or a concrete subclass.
-            if not cls.__dict__.get('_is_protocol', None):
-                cls._is_protocol = any(b is Protocol for b in cls.__bases__)
+                        f"Parameter list to {cls.__qualname__}[...] cannot be empty")
+                msg = "Parameters to generic types must be types."
+                params = tuple(typing._type_check(p, msg) for p in params)
+                if cls is Protocol:
+                    # Generic can only be subscripted with unique type variables.
+                    if not all(isinstance(p, typing.TypeVar) for p in params):
+                        i = 0
+                        while isinstance(params[i], typing.TypeVar):
+                            i += 1
+                        raise TypeError(
+                            "Parameters to Protocol[...] must all be type variables."
+                            f" Parameter {i + 1} is {params[i]}")
+                    if len(set(params)) != len(params):
+                        raise TypeError(
+                            "Parameters to Protocol[...] must all be unique")
+                else:
+                    # Subscripting a regular Generic subclass.
+                    _check_generic(cls, params, len(cls.__parameters__))
+                return typing._GenericAlias(cls, params)
+
+            def __init_subclass__(cls, *args, **kwargs):
+                if '__orig_bases__' in cls.__dict__:
+                    error = typing.Generic in cls.__orig_bases__
+                else:
+                    error = typing.Generic in cls.__bases__
+                if error:
+                    raise TypeError("Cannot inherit from plain Generic")
+                _maybe_adjust_parameters(cls)
 
-            # Set (or override) the protocol subclass hook.
-            def _proto_hook(other):
+                # Determine if this is a protocol or a concrete subclass.
                 if not cls.__dict__.get('_is_protocol', None):
-                    return NotImplemented
-                if not getattr(cls, '_is_runtime_protocol', False):
-                    if sys._getframe(2).f_globals['__name__'] in ['abc', 'functools']:
-                        return NotImplemented
-                    raise TypeError("Instance and class checks can only be used with"
-                                    " @runtime protocols")
-                if not _is_callable_members_only(cls):
-                    if sys._getframe(2).f_globals['__name__'] in ['abc', 'functools']:
-                        return NotImplemented
-                    raise TypeError("Protocols with non-method members"
-                                    " don't support issubclass()")
-                if not isinstance(other, type):
-                    # Same error as for issubclass(1, int)
-                    raise TypeError('issubclass() arg 1 must be a class')
-                for attr in _get_protocol_attrs(cls):
-                    for base in other.__mro__:
-                        if attr in base.__dict__:
-                            if base.__dict__[attr] is None:
-                                return NotImplemented
-                            break
-                        annotations = getattr(base, '__annotations__', {})
-                        if (isinstance(annotations, typing.Mapping) and
-                                attr in annotations and
-                                isinstance(other, _ProtocolMeta) and
-                                other._is_protocol):
-                            break
-                    else:
-                        return NotImplemented
-                return True
-            if '__subclasshook__' not in cls.__dict__:
-                cls.__subclasshook__ = _proto_hook
+                    cls._is_protocol = any(b is Protocol for b in cls.__bases__)
 
-            # We have nothing more to do for non-protocols.
-            if not cls._is_protocol:
-                return
+                # Set (or override) the protocol subclass hook.
+                if '__subclasshook__' not in cls.__dict__:
+                    cls.__subclasshook__ = _proto_hook
 
-            # Check consistency of bases.
-            for base in cls.__bases__:
-                if not (base in (object, typing.Generic) or
-                        base.__module__ == 'collections.abc' and
-                        base.__name__ in _PROTO_WHITELIST or
-                        isinstance(base, _ProtocolMeta) and base._is_protocol):
-                    raise TypeError('Protocols can only inherit from other'
-                                    f' protocols, got {repr(base)}')
-            cls.__init__ = _no_init
+                # Prohibit instantiation for protocol classes
+                if cls._is_protocol and cls.__init__ is Protocol.__init__:
+                    cls.__init__ = _no_init
 
 
-# 3.8+
-if hasattr(typing, 'runtime_checkable'):
+if sys.version_info >= (3, 8):
     runtime_checkable = typing.runtime_checkable
-# 3.7
 else:
     def runtime_checkable(cls):
         """Mark a protocol class as a runtime protocol, so that it
@@ -628,7 +862,10 @@
         This allows a simple-minded structural check very similar to the
         one-offs in collections.abc such as Hashable.
         """
-        if not isinstance(cls, _ProtocolMeta) or not cls._is_protocol:
+        if not (
+            (isinstance(cls, _ProtocolMeta) or issubclass(cls, typing.Generic))
+            and getattr(cls, "_is_protocol", False)
+        ):
             raise TypeError('@runtime_checkable can be only applied to protocol classes,'
                             f' got {cls!r}')
         cls._is_runtime_protocol = True
@@ -639,12 +876,53 @@
 runtime = runtime_checkable
 
 
-# 3.8+
-if hasattr(typing, 'SupportsIndex'):
+# Our version of runtime-checkable protocols is faster on Python 3.7-3.11
+if sys.version_info >= (3, 12):
+    SupportsInt = typing.SupportsInt
+    SupportsFloat = typing.SupportsFloat
+    SupportsComplex = typing.SupportsComplex
+    SupportsBytes = typing.SupportsBytes
     SupportsIndex = typing.SupportsIndex
-# 3.7
+    SupportsAbs = typing.SupportsAbs
+    SupportsRound = typing.SupportsRound
 else:
     @runtime_checkable
+    class SupportsInt(Protocol):
+        """An ABC with one abstract method __int__."""
+        __slots__ = ()
+
+        @abc.abstractmethod
+        def __int__(self) -> int:
+            pass
+
+    @runtime_checkable
+    class SupportsFloat(Protocol):
+        """An ABC with one abstract method __float__."""
+        __slots__ = ()
+
+        @abc.abstractmethod
+        def __float__(self) -> float:
+            pass
+
+    @runtime_checkable
+    class SupportsComplex(Protocol):
+        """An ABC with one abstract method __complex__."""
+        __slots__ = ()
+
+        @abc.abstractmethod
+        def __complex__(self) -> complex:
+            pass
+
+    @runtime_checkable
+    class SupportsBytes(Protocol):
+        """An ABC with one abstract method __bytes__."""
+        __slots__ = ()
+
+        @abc.abstractmethod
+        def __bytes__(self) -> bytes:
+            pass
+
+    @runtime_checkable
     class SupportsIndex(Protocol):
         __slots__ = ()
 
@@ -652,8 +930,45 @@
         def __index__(self) -> int:
             pass
 
+    @runtime_checkable
+    class SupportsAbs(Protocol[T_co]):
+        """
+        An ABC with one abstract method __abs__ that is covariant in its return type.
+        """
+        __slots__ = ()
 
-if hasattr(typing, "Required"):
+        @abc.abstractmethod
+        def __abs__(self) -> T_co:
+            pass
+
+    @runtime_checkable
+    class SupportsRound(Protocol[T_co]):
+        """
+        An ABC with one abstract method __round__ that is covariant in its return type.
+        """
+        __slots__ = ()
+
+        @abc.abstractmethod
+        def __round__(self, ndigits: int = 0) -> T_co:
+            pass
+
+
+def _ensure_subclassable(mro_entries):
+    def inner(func):
+        if sys.implementation.name == "pypy" and sys.version_info < (3, 9):
+            cls_dict = {
+                "__call__": staticmethod(func),
+                "__mro_entries__": staticmethod(mro_entries)
+            }
+            t = type(func.__name__, (), cls_dict)
+            return functools.update_wrapper(t(), func)
+        else:
+            func.__mro_entries__ = mro_entries
+            return func
+    return inner
+
+
+if sys.version_info >= (3, 13):
     # The standard library TypedDict in Python 3.8 does not store runtime information
     # about which (if any) keys are optional.  See https://bugs.python.org/issue38834
     # The standard library TypedDict in Python 3.9.0/1 does not honour the "total"
@@ -661,108 +976,63 @@
     # The standard library TypedDict below Python 3.11 does not store runtime
     # information about optional and required keys when using Required or NotRequired.
     # Generic TypedDicts are also impossible using typing.TypedDict on Python <3.11.
+    # Aaaand on 3.12 we add __orig_bases__ to TypedDict
+    # to enable better runtime introspection.
+    # On 3.13 we deprecate some odd ways of creating TypedDicts.
     TypedDict = typing.TypedDict
     _TypedDictMeta = typing._TypedDictMeta
     is_typeddict = typing.is_typeddict
 else:
-    def _check_fails(cls, other):
-        try:
-            if sys._getframe(1).f_globals['__name__'] not in ['abc',
-                                                              'functools',
-                                                              'typing']:
-                # Typed dicts are only for static structural subtyping.
-                raise TypeError('TypedDict does not support instance and class checks')
-        except (AttributeError, ValueError):
-            pass
-        return False
-
-    def _dict_new(*args, **kwargs):
-        if not args:
-            raise TypeError('TypedDict.__new__(): not enough arguments')
-        _, args = args[0], args[1:]  # allow the "cls" keyword be passed
-        return dict(*args, **kwargs)
-
-    _dict_new.__text_signature__ = '($cls, _typename, _fields=None, /, **kwargs)'
-
-    def _typeddict_new(*args, total=True, **kwargs):
-        if not args:
-            raise TypeError('TypedDict.__new__(): not enough arguments')
-        _, args = args[0], args[1:]  # allow the "cls" keyword be passed
-        if args:
-            typename, args = args[0], args[1:]  # allow the "_typename" keyword be passed
-        elif '_typename' in kwargs:
-            typename = kwargs.pop('_typename')
-            import warnings
-            warnings.warn("Passing '_typename' as keyword argument is deprecated",
-                          DeprecationWarning, stacklevel=2)
-        else:
-            raise TypeError("TypedDict.__new__() missing 1 required positional "
-                            "argument: '_typename'")
-        if args:
-            try:
-                fields, = args  # allow the "_fields" keyword be passed
-            except ValueError:
-                raise TypeError('TypedDict.__new__() takes from 2 to 3 '
-                                f'positional arguments but {len(args) + 2} '
-                                'were given')
-        elif '_fields' in kwargs and len(kwargs) == 1:
-            fields = kwargs.pop('_fields')
-            import warnings
-            warnings.warn("Passing '_fields' as keyword argument is deprecated",
-                          DeprecationWarning, stacklevel=2)
-        else:
-            fields = None
-
-        if fields is None:
-            fields = kwargs
-        elif kwargs:
-            raise TypeError("TypedDict takes either a dict or keyword arguments,"
-                            " but not both")
+    # 3.10.0 and later
+    _TAKES_MODULE = "module" in inspect.signature(typing._type_check).parameters
 
-        ns = {'__annotations__': dict(fields)}
-        try:
-            # Setting correct module is necessary to make typed dict classes pickleable.
-            ns['__module__'] = sys._getframe(1).f_globals.get('__name__', '__main__')
-        except (AttributeError, ValueError):
-            pass
+    if sys.version_info >= (3, 8):
+        _fake_name = "Protocol"
+    else:
+        _fake_name = "_Protocol"
 
-        return _TypedDictMeta(typename, (), ns, total=total)
+    class _TypedDictMeta(type):
+        def __new__(cls, name, bases, ns, total=True):
+            """Create new typed dict class object.
 
-    _typeddict_new.__text_signature__ = ('($cls, _typename, _fields=None,'
-                                         ' /, *, total=True, **kwargs)')
+            This method is called when TypedDict is subclassed,
+            or when TypedDict is instantiated. This way
+            TypedDict supports all three syntax forms described in its docstring.
+            Subclasses and instances of TypedDict return actual dictionaries.
+            """
+            for base in bases:
+                if type(base) is not _TypedDictMeta and base is not typing.Generic:
+                    raise TypeError('cannot inherit from both a TypedDict type '
+                                    'and a non-TypedDict base class')
 
-    _TAKES_MODULE = "module" in inspect.signature(typing._type_check).parameters
+            if any(issubclass(b, typing.Generic) for b in bases):
+                generic_base = (typing.Generic,)
+            else:
+                generic_base = ()
 
-    class _TypedDictMeta(type):
-        def __init__(cls, name, bases, ns, total=True):
-            super().__init__(name, bases, ns)
+            # typing.py generally doesn't let you inherit from plain Generic, unless
+            # the name of the class happens to be "Protocol" (or "_Protocol" on 3.7).
+            tp_dict = type.__new__(_TypedDictMeta, _fake_name, (*generic_base, dict), ns)
+            tp_dict.__name__ = name
+            if tp_dict.__qualname__ == _fake_name:
+                tp_dict.__qualname__ = name
 
-        def __new__(cls, name, bases, ns, total=True):
-            # Create new typed dict class object.
-            # This method is called directly when TypedDict is subclassed,
-            # or via _typeddict_new when TypedDict is instantiated. This way
-            # TypedDict supports all three syntaxes described in its docstring.
-            # Subclasses and instances of TypedDict return actual dictionaries
-            # via _dict_new.
-            ns['__new__'] = _typeddict_new if name == 'TypedDict' else _dict_new
-            # Don't insert typing.Generic into __bases__ here,
-            # or Generic.__init_subclass__ will raise TypeError
-            # in the super().__new__() call.
-            # Instead, monkey-patch __bases__ onto the class after it's been created.
-            tp_dict = super().__new__(cls, name, (dict,), ns)
-
-            if any(issubclass(base, typing.Generic) for base in bases):
-                tp_dict.__bases__ = (typing.Generic, dict)
-                _maybe_adjust_parameters(tp_dict)
+            if not hasattr(tp_dict, '__orig_bases__'):
+                tp_dict.__orig_bases__ = bases
 
             annotations = {}
             own_annotations = ns.get('__annotations__', {})
             msg = "TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type"
-            kwds = {"module": tp_dict.__module__} if _TAKES_MODULE else {}
-            own_annotations = {
-                n: typing._type_check(tp, msg, **kwds)
-                for n, tp in own_annotations.items()
-            }
+            if _TAKES_MODULE:
+                own_annotations = {
+                    n: typing._type_check(tp, msg, module=tp_dict.__module__)
+                    for n, tp in own_annotations.items()
+                }
+            else:
+                own_annotations = {
+                    n: typing._type_check(tp, msg)
+                    for n, tp in own_annotations.items()
+                }
             required_keys = set()
             optional_keys = set()
 
@@ -796,17 +1066,25 @@
                 tp_dict.__total__ = total
             return tp_dict
 
-        __instancecheck__ = __subclasscheck__ = _check_fails
+        __call__ = dict  # static method
+
+        def __subclasscheck__(cls, other):
+            # Typed dicts are only for static structural subtyping.
+            raise TypeError('TypedDict does not support instance and class checks')
+
+        __instancecheck__ = __subclasscheck__
 
-    TypedDict = _TypedDictMeta('TypedDict', (dict,), {})
-    TypedDict.__module__ = __name__
-    TypedDict.__doc__ = \
-        """A simple typed name space. At runtime it is equivalent to a plain dict.
+    _TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})
 
-        TypedDict creates a dictionary type that expects all of its
-        instances to have a certain set of keys, with each key
+    @_ensure_subclassable(lambda bases: (_TypedDict,))
+    def TypedDict(__typename, __fields=_marker, *, total=True, **kwargs):
+        """A simple typed namespace. At runtime it is equivalent to a plain dict.
+
+        TypedDict creates a dictionary type such that a type checker will expect all
+        instances to have a certain set of keys, where each key is
         associated with a value of a consistent type. This expectation
-        is not checked at runtime but is only enforced by type checkers.
+        is not checked at runtime.
+
         Usage::
 
             class Point2D(TypedDict):
@@ -821,14 +1099,66 @@
 
         The type info can be accessed via the Point2D.__annotations__ dict, and
         the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.
-        TypedDict supports two additional equivalent forms::
+        TypedDict supports an additional equivalent form::
 
-            Point2D = TypedDict('Point2D', x=int, y=int, label=str)
             Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})
 
-        The class syntax is only supported in Python 3.6+, while two other
-        syntax forms work for Python 2.7 and 3.2+
+        By default, all keys must be present in a TypedDict. It is possible
+        to override this by specifying totality::
+
+            class Point2D(TypedDict, total=False):
+                x: int
+                y: int
+
+        This means that a Point2D TypedDict can have any of the keys omitted. A type
+        checker is only expected to support a literal False or True as the value of
+        the total argument. True is the default, and makes all items defined in the
+        class body be required.
+
+        The Required and NotRequired special forms can also be used to mark
+        individual keys as being required or not required::
+
+            class Point2D(TypedDict):
+                x: int  # the "x" key must always be present (Required is the default)
+                y: NotRequired[int]  # the "y" key can be omitted
+
+        See PEP 655 for more details on Required and NotRequired.
         """
+        if __fields is _marker or __fields is None:
+            if __fields is _marker:
+                deprecated_thing = "Failing to pass a value for the 'fields' parameter"
+            else:
+                deprecated_thing = "Passing `None` as the 'fields' parameter"
+
+            example = f"`{__typename} = TypedDict({__typename!r}, {{}})`"
+            deprecation_msg = (
+                f"{deprecated_thing} is deprecated and will be disallowed in "
+                "Python 3.15. To create a TypedDict class with 0 fields "
+                "using the functional syntax, pass an empty dictionary, e.g. "
+            ) + example + "."
+            warnings.warn(deprecation_msg, DeprecationWarning, stacklevel=2)
+            __fields = kwargs
+        elif kwargs:
+            raise TypeError("TypedDict takes either a dict or keyword arguments,"
+                            " but not both")
+        if kwargs:
+            warnings.warn(
+                "The kwargs-based syntax for TypedDict definitions is deprecated "
+                "in Python 3.11, will be removed in Python 3.13, and may not be "
+                "understood by third-party type checkers.",
+                DeprecationWarning,
+                stacklevel=2,
+            )
+
+        ns = {'__annotations__': dict(__fields)}
+        module = _caller()
+        if module is not None:
+            # Setting correct module is necessary to make typed dict classes pickleable.
+            ns['__module__'] = module
+
+        td = _TypedDictMeta(__typename, (), ns, total=total)
+        td.__orig_bases__ = (TypedDict,)
+        return td
 
     if hasattr(typing, "_TypedDictMeta"):
         _TYPEDDICT_TYPES = (typing._TypedDictMeta, _TypedDictMeta)
@@ -846,7 +1176,10 @@
             is_typeddict(Film)  # => True
             is_typeddict(Union[list, str])  # => False
         """
-        return isinstance(tp, tuple(_TYPEDDICT_TYPES))
+        # On 3.8, this would otherwise return True
+        if hasattr(typing, "TypedDict") and tp is typing.TypedDict:
+            return False
+        return isinstance(tp, _TYPEDDICT_TYPES)
 
 
 if hasattr(typing, "assert_type"):
@@ -872,9 +1205,6 @@
 if hasattr(typing, "Required"):
     get_type_hints = typing.get_type_hints
 else:
-    import functools
-    import types
-
     # replaces _strip_annotations()
     def _strip_extras(t):
         """Strips Annotated, Required and NotRequired from a given type."""
@@ -887,12 +1217,12 @@
             if stripped_args == t.__args__:
                 return t
             return t.copy_with(stripped_args)
-        if hasattr(types, "GenericAlias") and isinstance(t, types.GenericAlias):
+        if hasattr(_types, "GenericAlias") and isinstance(t, _types.GenericAlias):
             stripped_args = tuple(_strip_extras(a) for a in t.__args__)
             if stripped_args == t.__args__:
                 return t
-            return types.GenericAlias(t.__origin__, stripped_args)
-        if hasattr(types, "UnionType") and isinstance(t, types.UnionType):
+            return _types.GenericAlias(t.__origin__, stripped_args)
+        if hasattr(_types, "UnionType") and isinstance(t, _types.UnionType):
             stripped_args = tuple(_strip_extras(a) for a in t.__args__)
             if stripped_args == t.__args__:
                 return t
@@ -1119,11 +1449,7 @@
     TypeAlias = typing.TypeAlias
 # 3.9
 elif sys.version_info[:2] >= (3, 9):
-    class _TypeAliasForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
-    @_TypeAliasForm
+    @_ExtensionsSpecialForm
     def TypeAlias(self, parameters):
         """Special marker indicating that an assignment should
         be recognized as a proper type alias definition by type
@@ -1138,59 +1464,77 @@
         raise TypeError(f"{self} is not subscriptable")
 # 3.7-3.8
 else:
-    class _TypeAliasForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
+    TypeAlias = _ExtensionsSpecialForm(
+        'TypeAlias',
+        doc="""Special marker indicating that an assignment should
+        be recognized as a proper type alias definition by type
+        checkers.
 
-    TypeAlias = _TypeAliasForm('TypeAlias',
-                               doc="""Special marker indicating that an assignment should
-                               be recognized as a proper type alias definition by type
-                               checkers.
+        For example::
 
-                               For example::
+            Predicate: TypeAlias = Callable[..., bool]
+
+        It's invalid when used anywhere except as in the example
+        above."""
+    )
 
-                                   Predicate: TypeAlias = Callable[..., bool]
 
-                               It's invalid when used anywhere except as in the example
-                               above.""")
+def _set_default(type_param, default):
+    if isinstance(default, (tuple, list)):
+        type_param.__default__ = tuple((typing._type_check(d, "Default must be a type")
+                                        for d in default))
+    elif default != _marker:
+        type_param.__default__ = typing._type_check(default, "Default must be a type")
+    else:
+        type_param.__default__ = None
+
+
+def _set_module(typevarlike):
+    # for pickling:
+    def_mod = _caller(depth=3)
+    if def_mod != 'typing_extensions':
+        typevarlike.__module__ = def_mod
 
 
 class _DefaultMixin:
     """Mixin for TypeVarLike defaults."""
 
     __slots__ = ()
+    __init__ = _set_default
 
-    def __init__(self, default):
-        if isinstance(default, (tuple, list)):
-            self.__default__ = tuple((typing._type_check(d, "Default must be a type")
-                                      for d in default))
-        elif default != _marker:
-            self.__default__ = typing._type_check(default, "Default must be a type")
-        else:
-            self.__default__ = None
+
+# Classes using this metaclass must provide a _backported_typevarlike ClassVar
+class _TypeVarLikeMeta(type):
+    def __instancecheck__(cls, __instance: Any) -> bool:
+        return isinstance(__instance, cls._backported_typevarlike)
 
 
 # Add default and infer_variance parameters from PEP 696 and 695
-class TypeVar(typing.TypeVar, _DefaultMixin, _root=True):
+class TypeVar(metaclass=_TypeVarLikeMeta):
     """Type variable."""
 
-    __module__ = 'typing'
+    _backported_typevarlike = typing.TypeVar
 
-    def __init__(self, name, *constraints, bound=None,
-                 covariant=False, contravariant=False,
-                 default=_marker, infer_variance=False):
-        super().__init__(name, *constraints, bound=bound, covariant=covariant,
-                         contravariant=contravariant)
-        _DefaultMixin.__init__(self, default)
-        self.__infer_variance__ = infer_variance
+    def __new__(cls, name, *constraints, bound=None,
+                covariant=False, contravariant=False,
+                default=_marker, infer_variance=False):
+        if hasattr(typing, "TypeAliasType"):
+            # PEP 695 implemented, can pass infer_variance to typing.TypeVar
+            typevar = typing.TypeVar(name, *constraints, bound=bound,
+                                     covariant=covariant, contravariant=contravariant,
+                                     infer_variance=infer_variance)
+        else:
+            typevar = typing.TypeVar(name, *constraints, bound=bound,
+                                     covariant=covariant, contravariant=contravariant)
+            if infer_variance and (covariant or contravariant):
+                raise ValueError("Variance cannot be specified with infer_variance.")
+            typevar.__infer_variance__ = infer_variance
+        _set_default(typevar, default)
+        _set_module(typevar)
+        return typevar
 
-        # for pickling:
-        try:
-            def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')
-        except (AttributeError, ValueError):
-            def_mod = None
-        if def_mod != 'typing_extensions':
-            self.__module__ = def_mod
+    def __init_subclass__(cls) -> None:
+        raise TypeError(f"type '{__name__}.TypeVar' is not an acceptable base type")
 
 
 # Python 3.10+ has PEP 612
@@ -1258,25 +1602,33 @@
 # 3.10+
 if hasattr(typing, 'ParamSpec'):
 
-    # Add default Parameter - PEP 696
-    class ParamSpec(typing.ParamSpec, _DefaultMixin, _root=True):
-        """Parameter specification variable."""
-
-        __module__ = 'typing'
-
-        def __init__(self, name, *, bound=None, covariant=False, contravariant=False,
-                     default=_marker):
-            super().__init__(name, bound=bound, covariant=covariant,
-                             contravariant=contravariant)
-            _DefaultMixin.__init__(self, default)
+    # Add default parameter - PEP 696
+    class ParamSpec(metaclass=_TypeVarLikeMeta):
+        """Parameter specification."""
+
+        _backported_typevarlike = typing.ParamSpec
+
+        def __new__(cls, name, *, bound=None,
+                    covariant=False, contravariant=False,
+                    infer_variance=False, default=_marker):
+            if hasattr(typing, "TypeAliasType"):
+                # PEP 695 implemented, can pass infer_variance to typing.TypeVar
+                paramspec = typing.ParamSpec(name, bound=bound,
+                                             covariant=covariant,
+                                             contravariant=contravariant,
+                                             infer_variance=infer_variance)
+            else:
+                paramspec = typing.ParamSpec(name, bound=bound,
+                                             covariant=covariant,
+                                             contravariant=contravariant)
+                paramspec.__infer_variance__ = infer_variance
+
+            _set_default(paramspec, default)
+            _set_module(paramspec)
+            return paramspec
 
-            # for pickling:
-            try:
-                def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')
-            except (AttributeError, ValueError):
-                def_mod = None
-            if def_mod != 'typing_extensions':
-                self.__module__ = def_mod
+        def __init_subclass__(cls) -> None:
+            raise TypeError(f"type '{__name__}.ParamSpec' is not an acceptable base type")
 
 # 3.7-3.9
 else:
@@ -1341,11 +1693,12 @@
             return ParamSpecKwargs(self)
 
         def __init__(self, name, *, bound=None, covariant=False, contravariant=False,
-                     default=_marker):
+                     infer_variance=False, default=_marker):
             super().__init__([self])
             self.__name__ = name
             self.__covariant__ = bool(covariant)
             self.__contravariant__ = bool(contravariant)
+            self.__infer_variance__ = bool(infer_variance)
             if bound:
                 self.__bound__ = typing._type_check(bound, 'Bound must be a type.')
             else:
@@ -1353,15 +1706,14 @@
             _DefaultMixin.__init__(self, default)
 
             # for pickling:
-            try:
-                def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')
-            except (AttributeError, ValueError):
-                def_mod = None
+            def_mod = _caller()
             if def_mod != 'typing_extensions':
                 self.__module__ = def_mod
 
         def __repr__(self):
-            if self.__covariant__:
+            if self.__infer_variance__:
+                prefix = ''
+            elif self.__covariant__:
                 prefix = '+'
             elif self.__contravariant__:
                 prefix = '-'
@@ -1436,10 +1788,10 @@
 # 3.10+
 if hasattr(typing, 'Concatenate'):
     Concatenate = typing.Concatenate
-    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias # noqa
+    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias  # noqa: F811
 # 3.9
 elif sys.version_info[:2] >= (3, 9):
-    @_TypeAliasForm
+    @_ExtensionsSpecialForm
     def Concatenate(self, parameters):
         """Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a
         higher order function which adds, removes or transforms parameters of a
@@ -1454,10 +1806,7 @@
         return _concatenate_getitem(self, parameters)
 # 3.7-8
 else:
-    class _ConcatenateForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
+    class _ConcatenateForm(_ExtensionsSpecialForm, _root=True):
         def __getitem__(self, parameters):
             return _concatenate_getitem(self, parameters)
 
@@ -1479,11 +1828,7 @@
     TypeGuard = typing.TypeGuard
 # 3.9
 elif sys.version_info[:2] >= (3, 9):
-    class _TypeGuardForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
-    @_TypeGuardForm
+    @_ExtensionsSpecialForm
     def TypeGuard(self, parameters):
         """Special typing form used to annotate the return type of a user-defined
         type guard function.  ``TypeGuard`` only accepts a single type argument.
@@ -1531,11 +1876,7 @@
         return typing._GenericAlias(self, (item,))
 # 3.7-3.8
 else:
-    class _TypeGuardForm(typing._SpecialForm, _root=True):
-
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
+    class _TypeGuardForm(_ExtensionsSpecialForm, _root=True):
         def __getitem__(self, parameters):
             item = typing._type_check(parameters,
                                       f'{self._name} accepts only a single type')
@@ -1709,10 +2050,6 @@
     Required = typing.Required
     NotRequired = typing.NotRequired
 elif sys.version_info[:2] >= (3, 9):
-    class _ExtensionsSpecialForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
     @_ExtensionsSpecialForm
     def Required(self, parameters):
         """A special typing construct to mark a key of a total=False TypedDict
@@ -1751,10 +2088,7 @@
         return typing._GenericAlias(self, (item,))
 
 else:
-    class _RequiredForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
+    class _RequiredForm(_ExtensionsSpecialForm, _root=True):
         def __getitem__(self, parameters):
             item = typing._type_check(parameters,
                                       f'{self._name} accepts only a single type.')
@@ -1793,28 +2127,65 @@
         """)
 
 
-if hasattr(typing, "Unpack"):  # 3.11+
+_UNPACK_DOC = """\
+Type unpack operator.
+
+The type unpack operator takes the child types from some container type,
+such as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'. For
+example:
+
+  # For some generic class `Foo`:
+  Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]
+
+  Ts = TypeVarTuple('Ts')
+  # Specifies that `Bar` is generic in an arbitrary number of types.
+  # (Think of `Ts` as a tuple of an arbitrary number of individual
+  #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the
+  #  `Generic[]`.)
+  class Bar(Generic[Unpack[Ts]]): ...
+  Bar[int]  # Valid
+  Bar[int, str]  # Also valid
+
+From Python 3.11, this can also be done using the `*` operator:
+
+    Foo[*tuple[int, str]]
+    class Bar(Generic[*Ts]): ...
+
+The operator can also be used along with a `TypedDict` to annotate
+`**kwargs` in a function signature. For instance:
+
+  class Movie(TypedDict):
+    name: str
+    year: int
+
+  # This function expects two keyword arguments - *name* of type `str` and
+  # *year* of type `int`.
+  def foo(**kwargs: Unpack[Movie]): ...
+
+Note that there is only some runtime checking of this operator. Not
+everything the runtime allows may be accepted by static type checkers.
+
+For more information, see PEP 646 and PEP 692.
+"""
+
+
+if sys.version_info >= (3, 12):  # PEP 692 changed the repr of Unpack[]
     Unpack = typing.Unpack
+
+    def _is_unpack(obj):
+        return get_origin(obj) is Unpack
+
 elif sys.version_info[:2] >= (3, 9):
-    class _UnpackSpecialForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
+    class _UnpackSpecialForm(_ExtensionsSpecialForm, _root=True):
+        def __init__(self, getitem):
+            super().__init__(getitem)
+            self.__doc__ = _UNPACK_DOC
 
     class _UnpackAlias(typing._GenericAlias, _root=True):
         __class__ = typing.TypeVar
 
     @_UnpackSpecialForm
     def Unpack(self, parameters):
-        """A special typing construct to unpack a variadic type. For example:
-
-            Shape = TypeVarTuple('Shape')
-            Batch = NewType('Batch', int)
-
-            def add_batch_axis(
-                x: Array[Unpack[Shape]]
-            ) -> Array[Batch, Unpack[Shape]]: ...
-
-        """
         item = typing._type_check(parameters, f'{self._name} accepts only a single type.')
         return _UnpackAlias(self, (item,))
 
@@ -1825,27 +2196,13 @@
     class _UnpackAlias(typing._GenericAlias, _root=True):
         __class__ = typing.TypeVar
 
-    class _UnpackForm(typing._SpecialForm, _root=True):
-        def __repr__(self):
-            return 'typing_extensions.' + self._name
-
+    class _UnpackForm(_ExtensionsSpecialForm, _root=True):
         def __getitem__(self, parameters):
             item = typing._type_check(parameters,
                                       f'{self._name} accepts only a single type.')
             return _UnpackAlias(self, (item,))
 
-    Unpack = _UnpackForm(
-        'Unpack',
-        doc="""A special typing construct to unpack a variadic type. For example:
-
-            Shape = TypeVarTuple('Shape')
-            Batch = NewType('Batch', int)
-
-            def add_batch_axis(
-                x: Array[Unpack[Shape]]
-            ) -> Array[Batch, Unpack[Shape]]: ...
-
-        """)
+    Unpack = _UnpackForm('Unpack', doc=_UNPACK_DOC)
 
     def _is_unpack(obj):
         return isinstance(obj, _UnpackAlias)
@@ -1853,21 +2210,20 @@
 
 if hasattr(typing, "TypeVarTuple"):  # 3.11+
 
-    # Add default Parameter - PEP 696
-    class TypeVarTuple(typing.TypeVarTuple, _DefaultMixin, _root=True):
+    # Add default parameter - PEP 696
+    class TypeVarTuple(metaclass=_TypeVarLikeMeta):
         """Type variable tuple."""
 
-        def __init__(self, name, *, default=_marker):
-            super().__init__(name)
-            _DefaultMixin.__init__(self, default)
+        _backported_typevarlike = typing.TypeVarTuple
 
-            # for pickling:
-            try:
-                def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')
-            except (AttributeError, ValueError):
-                def_mod = None
-            if def_mod != 'typing_extensions':
-                self.__module__ = def_mod
+        def __new__(cls, name, *, default=_marker):
+            tvt = typing.TypeVarTuple(name)
+            _set_default(tvt, default)
+            _set_module(tvt)
+            return tvt
+
+        def __init_subclass__(self, *args, **kwds):
+            raise TypeError("Cannot subclass special typing classes")
 
 else:
     class TypeVarTuple(_DefaultMixin):
@@ -1925,10 +2281,7 @@
             _DefaultMixin.__init__(self, default)
 
             # for pickling:
-            try:
-                def_mod = sys._getframe(1).f_globals.get('__name__', '__main__')
-            except (AttributeError, ValueError):
-                def_mod = None
+            def_mod = _caller()
             if def_mod != 'typing_extensions':
                 self.__module__ = def_mod
 
@@ -2163,7 +2516,15 @@
         When this decorator is applied to an object, the type checker
         will generate a diagnostic on usage of the deprecated object.
 
-        No runtime warning is issued. The decorator sets the ``__deprecated__``
+        The warning specified by ``category`` will be emitted on use
+        of deprecated objects. For functions, that happens on calls;
+        for classes, on instantiation. If the ``category`` is ``None``,
+        no warning is emitted. The ``stacklevel`` determines where the
+        warning is emitted. If it is ``1`` (the default), the warning
+        is emitted at the direct caller of the deprecated object; if it
+        is higher, it is emitted further up the stack.
+
+        The decorator sets the ``__deprecated__``
         attribute on the decorated object to the deprecation message
         passed to the decorator. If applied to an overload, the decorator
         must be after the ``@overload`` decorator for the attribute to
@@ -2183,11 +2544,11 @@
                 @functools.wraps(original_new)
                 def __new__(cls, *args, **kwargs):
                     warnings.warn(__msg, category=category, stacklevel=stacklevel + 1)
-                    # Mirrors a similar check in object.__new__.
-                    if not has_init and (args or kwargs):
-                        raise TypeError(f"{cls.__name__}() takes no arguments")
                     if original_new is not object.__new__:
                         return original_new(cls, *args, **kwargs)
+                    # Mirrors a similar check in object.__new__.
+                    elif not has_init and (args or kwargs):
+                        raise TypeError(f"{cls.__name__}() takes no arguments")
                     else:
                         return original_new(cls)
 
@@ -2223,18 +2584,14 @@
     typing._check_generic = _check_generic
 
 
-# Backport typing.NamedTuple as it exists in Python 3.11.
+# Backport typing.NamedTuple as it exists in Python 3.12.
 # In 3.11, the ability to define generic `NamedTuple`s was supported.
 # This was explicitly disallowed in 3.9-3.10, and only half-worked in <=3.8.
-if sys.version_info >= (3, 11):
+# On 3.12, we added __orig_bases__ to call-based NamedTuples
+# On 3.13, we deprecated kwargs-based NamedTuples
+if sys.version_info >= (3, 13):
     NamedTuple = typing.NamedTuple
 else:
-    def _caller():
-        try:
-            return sys._getframe(2).f_globals.get('__name__', '__main__')
-        except (AttributeError, ValueError):  # For platforms without _getframe()
-            return None
-
     def _make_nmtuple(name, types, module, defaults=()):
         fields = [n for n, t in types]
         annotations = {n: typing._type_check(t, f"field {n} annotation must be a type")
@@ -2276,8 +2633,11 @@
             )
             nm_tpl.__bases__ = bases
             if typing.Generic in bases:
-                class_getitem = typing.Generic.__class_getitem__.__func__
-                nm_tpl.__class_getitem__ = classmethod(class_getitem)
+                if hasattr(typing, '_generic_class_getitem'):  # 3.12+
+                    nm_tpl.__class_getitem__ = classmethod(typing._generic_class_getitem)
+                else:
+                    class_getitem = typing.Generic.__class_getitem__.__func__
+                    nm_tpl.__class_getitem__ = classmethod(class_getitem)
             # update from user namespace without overriding special namedtuple attributes
             for key in ns:
                 if key in _prohibited_namedtuple_fields:
@@ -2288,25 +2648,425 @@
                 nm_tpl.__init_subclass__()
             return nm_tpl
 
-    def NamedTuple(__typename, __fields=None, **kwargs):
-        if __fields is None:
-            __fields = kwargs.items()
+    _NamedTuple = type.__new__(_NamedTupleMeta, 'NamedTuple', (), {})
+
+    def _namedtuple_mro_entries(bases):
+        assert NamedTuple in bases
+        return (_NamedTuple,)
+
+    @_ensure_subclassable(_namedtuple_mro_entries)
+    def NamedTuple(__typename, __fields=_marker, **kwargs):
+        """Typed version of namedtuple.
+
+        Usage::
+
+            class Employee(NamedTuple):
+                name: str
+                id: int
+
+        This is equivalent to::
+
+            Employee = collections.namedtuple('Employee', ['name', 'id'])
+
+        The resulting class has an extra __annotations__ attribute, giving a
+        dict that maps field names to types.  (The field names are also in
+        the _fields attribute, which is part of the namedtuple API.)
+        An alternative equivalent functional syntax is also accepted::
+
+            Employee = NamedTuple('Employee', [('name', str), ('id', int)])
+        """
+        if __fields is _marker:
+            if kwargs:
+                deprecated_thing = "Creating NamedTuple classes using keyword arguments"
+                deprecation_msg = (
+                    "{name} is deprecated and will be disallowed in Python {remove}. "
+                    "Use the class-based or functional syntax instead."
+                )
+            else:
+                deprecated_thing = "Failing to pass a value for the 'fields' parameter"
+                example = f"`{__typename} = NamedTuple({__typename!r}, [])`"
+                deprecation_msg = (
+                    "{name} is deprecated and will be disallowed in Python {remove}. "
+                    "To create a NamedTuple class with 0 fields "
+                    "using the functional syntax, "
+                    "pass an empty list, e.g. "
+                ) + example + "."
+        elif __fields is None:
+            if kwargs:
+                raise TypeError(
+                    "Cannot pass `None` as the 'fields' parameter "
+                    "and also specify fields using keyword arguments"
+                )
+            else:
+                deprecated_thing = "Passing `None` as the 'fields' parameter"
+                example = f"`{__typename} = NamedTuple({__typename!r}, [])`"
+                deprecation_msg = (
+                    "{name} is deprecated and will be disallowed in Python {remove}. "
+                    "To create a NamedTuple class with 0 fields "
+                    "using the functional syntax, "
+                    "pass an empty list, e.g. "
+                ) + example + "."
         elif kwargs:
             raise TypeError("Either list of fields or keywords"
                             " can be provided to NamedTuple, not both")
-        return _make_nmtuple(__typename, __fields, module=_caller())
-
-    NamedTuple.__doc__ = typing.NamedTuple.__doc__
-    _NamedTuple = type.__new__(_NamedTupleMeta, 'NamedTuple', (), {})
+        if __fields is _marker or __fields is None:
+            warnings.warn(
+                deprecation_msg.format(name=deprecated_thing, remove="3.15"),
+                DeprecationWarning,
+                stacklevel=2,
+            )
+            __fields = kwargs.items()
+        nt = _make_nmtuple(__typename, __fields, module=_caller())
+        nt.__orig_bases__ = (NamedTuple,)
+        return nt
 
     # On 3.8+, alter the signature so that it matches typing.NamedTuple.
     # The signature of typing.NamedTuple on >=3.8 is invalid syntax in Python 3.7,
     # so just leave the signature as it is on 3.7.
     if sys.version_info >= (3, 8):
-        NamedTuple.__text_signature__ = '(typename, fields=None, /, **kwargs)'
+        _new_signature = '(typename, fields=None, /, **kwargs)'
+        if isinstance(NamedTuple, _types.FunctionType):
+            NamedTuple.__text_signature__ = _new_signature
+        else:
+            NamedTuple.__call__.__text_signature__ = _new_signature
 
-    def _namedtuple_mro_entries(bases):
-        assert NamedTuple in bases
-        return (_NamedTuple,)
 
-    NamedTuple.__mro_entries__ = _namedtuple_mro_entries
+if hasattr(collections.abc, "Buffer"):
+    Buffer = collections.abc.Buffer
+else:
+    class Buffer(abc.ABC):
+        """Base class for classes that implement the buffer protocol.
+
+        The buffer protocol allows Python objects to expose a low-level
+        memory buffer interface. Before Python 3.12, it is not possible
+        to implement the buffer protocol in pure Python code, or even
+        to check whether a class implements the buffer protocol. In
+        Python 3.12 and higher, the ``__buffer__`` method allows access
+        to the buffer protocol from Python code, and the
+        ``collections.abc.Buffer`` ABC allows checking whether a class
+        implements the buffer protocol.
+
+        To indicate support for the buffer protocol in earlier versions,
+        inherit from this ABC, either in a stub file or at runtime,
+        or use ABC registration. This ABC provides no methods, because
+        there is no Python-accessible methods shared by pre-3.12 buffer
+        classes. It is useful primarily for static checks.
+
+        """
+
+    # As a courtesy, register the most common stdlib buffer classes.
+    Buffer.register(memoryview)
+    Buffer.register(bytearray)
+    Buffer.register(bytes)
+
+
+# Backport of types.get_original_bases, available on 3.12+ in CPython
+if hasattr(_types, "get_original_bases"):
+    get_original_bases = _types.get_original_bases
+else:
+    def get_original_bases(__cls):
+        """Return the class's "original" bases prior to modification by `__mro_entries__`.
+
+        Examples::
+
+            from typing import TypeVar, Generic
+            from pip._vendor.typing_extensions import NamedTuple, TypedDict
+
+            T = TypeVar("T")
+            class Foo(Generic[T]): ...
+            class Bar(Foo[int], float): ...
+            class Baz(list[str]): ...
+            Eggs = NamedTuple("Eggs", [("a", int), ("b", str)])
+            Spam = TypedDict("Spam", {"a": int, "b": str})
+
+            assert get_original_bases(Bar) == (Foo[int], float)
+            assert get_original_bases(Baz) == (list[str],)
+            assert get_original_bases(Eggs) == (NamedTuple,)
+            assert get_original_bases(Spam) == (TypedDict,)
+            assert get_original_bases(int) == (object,)
+        """
+        try:
+            return __cls.__orig_bases__
+        except AttributeError:
+            try:
+                return __cls.__bases__
+            except AttributeError:
+                raise TypeError(
+                    f'Expected an instance of type, not {type(__cls).__name__!r}'
+                ) from None
+
+
+# NewType is a class on Python 3.10+, making it pickleable
+# The error message for subclassing instances of NewType was improved on 3.11+
+if sys.version_info >= (3, 11):
+    NewType = typing.NewType
+else:
+    class NewType:
+        """NewType creates simple unique types with almost zero
+        runtime overhead. NewType(name, tp) is considered a subtype of tp
+        by static type checkers. At runtime, NewType(name, tp) returns
+        a dummy callable that simply returns its argument. Usage::
+            UserId = NewType('UserId', int)
+            def name_by_id(user_id: UserId) -> str:
+                ...
+            UserId('user')          # Fails type check
+            name_by_id(42)          # Fails type check
+            name_by_id(UserId(42))  # OK
+            num = UserId(5) + 1     # type: int
+        """
+
+        def __call__(self, obj):
+            return obj
+
+        def __init__(self, name, tp):
+            self.__qualname__ = name
+            if '.' in name:
+                name = name.rpartition('.')[-1]
+            self.__name__ = name
+            self.__supertype__ = tp
+            def_mod = _caller()
+            if def_mod != 'typing_extensions':
+                self.__module__ = def_mod
+
+        def __mro_entries__(self, bases):
+            # We defined __mro_entries__ to get a better error message
+            # if a user attempts to subclass a NewType instance. bpo-46170
+            supercls_name = self.__name__
+
+            class Dummy:
+                def __init_subclass__(cls):
+                    subcls_name = cls.__name__
+                    raise TypeError(
+                        f"Cannot subclass an instance of NewType. "
+                        f"Perhaps you were looking for: "
+                        f"`{subcls_name} = NewType({subcls_name!r}, {supercls_name})`"
+                    )
+
+            return (Dummy,)
+
+        def __repr__(self):
+            return f'{self.__module__}.{self.__qualname__}'
+
+        def __reduce__(self):
+            return self.__qualname__
+
+        if sys.version_info >= (3, 10):
+            # PEP 604 methods
+            # It doesn't make sense to have these methods on Python <3.10
+
+            def __or__(self, other):
+                return typing.Union[self, other]
+
+            def __ror__(self, other):
+                return typing.Union[other, self]
+
+
+if hasattr(typing, "TypeAliasType"):
+    TypeAliasType = typing.TypeAliasType
+else:
+    def _is_unionable(obj):
+        """Corresponds to is_unionable() in unionobject.c in CPython."""
+        return obj is None or isinstance(obj, (
+            type,
+            _types.GenericAlias,
+            _types.UnionType,
+            TypeAliasType,
+        ))
+
+    class TypeAliasType:
+        """Create named, parameterized type aliases.
+
+        This provides a backport of the new `type` statement in Python 3.12:
+
+            type ListOrSet[T] = list[T] | set[T]
+
+        is equivalent to:
+
+            T = TypeVar("T")
+            ListOrSet = TypeAliasType("ListOrSet", list[T] | set[T], type_params=(T,))
+
+        The name ListOrSet can then be used as an alias for the type it refers to.
+
+        The type_params argument should contain all the type parameters used
+        in the value of the type alias. If the alias is not generic, this
+        argument is omitted.
+
+        Static type checkers should only support type aliases declared using
+        TypeAliasType that follow these rules:
+
+        - The first argument (the name) must be a string literal.
+        - The TypeAliasType instance must be immediately assigned to a variable
+          of the same name. (For example, 'X = TypeAliasType("Y", int)' is invalid,
+          as is 'X, Y = TypeAliasType("X", int), TypeAliasType("Y", int)').
+
+        """
+
+        def __init__(self, name: str, value, *, type_params=()):
+            if not isinstance(name, str):
+                raise TypeError("TypeAliasType name must be a string")
+            self.__value__ = value
+            self.__type_params__ = type_params
+
+            parameters = []
+            for type_param in type_params:
+                if isinstance(type_param, TypeVarTuple):
+                    parameters.extend(type_param)
+                else:
+                    parameters.append(type_param)
+            self.__parameters__ = tuple(parameters)
+            def_mod = _caller()
+            if def_mod != 'typing_extensions':
+                self.__module__ = def_mod
+            # Setting this attribute closes the TypeAliasType from further modification
+            self.__name__ = name
+
+        def __setattr__(self, __name: str, __value: object) -> None:
+            if hasattr(self, "__name__"):
+                self._raise_attribute_error(__name)
+            super().__setattr__(__name, __value)
+
+        def __delattr__(self, __name: str) -> Never:
+            self._raise_attribute_error(__name)
+
+        def _raise_attribute_error(self, name: str) -> Never:
+            # Match the Python 3.12 error messages exactly
+            if name == "__name__":
+                raise AttributeError("readonly attribute")
+            elif name in {"__value__", "__type_params__", "__parameters__", "__module__"}:
+                raise AttributeError(
+                    f"attribute '{name}' of 'typing.TypeAliasType' objects "
+                    "is not writable"
+                )
+            else:
+                raise AttributeError(
+                    f"'typing.TypeAliasType' object has no attribute '{name}'"
+                )
+
+        def __repr__(self) -> str:
+            return self.__name__
+
+        def __getitem__(self, parameters):
+            if not isinstance(parameters, tuple):
+                parameters = (parameters,)
+            parameters = [
+                typing._type_check(
+                    item, f'Subscripting {self.__name__} requires a type.'
+                )
+                for item in parameters
+            ]
+            return typing._GenericAlias(self, tuple(parameters))
+
+        def __reduce__(self):
+            return self.__name__
+
+        def __init_subclass__(cls, *args, **kwargs):
+            raise TypeError(
+                "type 'typing_extensions.TypeAliasType' is not an acceptable base type"
+            )
+
+        # The presence of this method convinces typing._type_check
+        # that TypeAliasTypes are types.
+        def __call__(self):
+            raise TypeError("Type alias is not callable")
+
+        if sys.version_info >= (3, 10):
+            def __or__(self, right):
+                # For forward compatibility with 3.12, reject Unions
+                # that are not accepted by the built-in Union.
+                if not _is_unionable(right):
+                    return NotImplemented
+                return typing.Union[self, right]
+
+            def __ror__(self, left):
+                if not _is_unionable(left):
+                    return NotImplemented
+                return typing.Union[left, self]
+
+
+if hasattr(typing, "is_protocol"):
+    is_protocol = typing.is_protocol
+    get_protocol_members = typing.get_protocol_members
+else:
+    def is_protocol(__tp: type) -> bool:
+        """Return True if the given type is a Protocol.
+
+        Example::
+
+            >>> from typing_extensions import Protocol, is_protocol
+            >>> class P(Protocol):
+            ...     def a(self) -> str: ...
+            ...     b: int
+            >>> is_protocol(P)
+            True
+            >>> is_protocol(int)
+            False
+        """
+        return (
+            isinstance(__tp, type)
+            and getattr(__tp, '_is_protocol', False)
+            and __tp is not Protocol
+            and __tp is not getattr(typing, "Protocol", object())
+        )
+
+    def get_protocol_members(__tp: type) -> typing.FrozenSet[str]:
+        """Return the set of members defined in a Protocol.
+
+        Example::
+
+            >>> from typing_extensions import Protocol, get_protocol_members
+            >>> class P(Protocol):
+            ...     def a(self) -> str: ...
+            ...     b: int
+            >>> get_protocol_members(P)
+            frozenset({'a', 'b'})
+
+        Raise a TypeError for arguments that are not Protocols.
+        """
+        if not is_protocol(__tp):
+            raise TypeError(f'{__tp!r} is not a Protocol')
+        if hasattr(__tp, '__protocol_attrs__'):
+            return frozenset(__tp.__protocol_attrs__)
+        return frozenset(_get_protocol_attrs(__tp))
+
+
+# Aliases for items that have always been in typing.
+# Explicitly assign these (rather than using `from typing import *` at the top),
+# so that we get a CI error if one of these is deleted from typing.py
+# in a future version of Python
+AbstractSet = typing.AbstractSet
+AnyStr = typing.AnyStr
+BinaryIO = typing.BinaryIO
+Callable = typing.Callable
+Collection = typing.Collection
+Container = typing.Container
+Dict = typing.Dict
+ForwardRef = typing.ForwardRef
+FrozenSet = typing.FrozenSet
+Generator = typing.Generator
+Generic = typing.Generic
+Hashable = typing.Hashable
+IO = typing.IO
+ItemsView = typing.ItemsView
+Iterable = typing.Iterable
+Iterator = typing.Iterator
+KeysView = typing.KeysView
+List = typing.List
+Mapping = typing.Mapping
+MappingView = typing.MappingView
+Match = typing.Match
+MutableMapping = typing.MutableMapping
+MutableSequence = typing.MutableSequence
+MutableSet = typing.MutableSet
+Optional = typing.Optional
+Pattern = typing.Pattern
+Reversible = typing.Reversible
+Sequence = typing.Sequence
+Set = typing.Set
+Sized = typing.Sized
+TextIO = typing.TextIO
+Tuple = typing.Tuple
+Union = typing.Union
+ValuesView = typing.ValuesView
+cast = typing.cast
+no_type_check = typing.no_type_check
+no_type_check_decorator = typing.no_type_check_decorator
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/connectionpool.py	2023-07-15 11:52:27.581474434 -0400
@@ -50,6 +50,13 @@
 from .util.url import _normalize_host as normalize_host
 from .util.url import get_host, parse_url
 
+try:  # Platform-specific: Python 3
+    import weakref
+
+    weakref_finalize = weakref.finalize
+except AttributeError:  # Platform-specific: Python 2
+    from .packages.backports.weakref_finalize import weakref_finalize
+
 xrange = six.moves.xrange
 
 log = logging.getLogger(__name__)
@@ -220,6 +227,16 @@
             self.conn_kw["proxy"] = self.proxy
             self.conn_kw["proxy_config"] = self.proxy_config
 
+        # Do not pass 'self' as callback to 'finalize'.
+        # Then the 'finalize' would keep an endless living (leak) to self.
+        # By just passing a reference to the pool allows the garbage collector
+        # to free self if nobody else has a reference to it.
+        pool = self.pool
+
+        # Close all the HTTPConnections in the pool before the
+        # HTTPConnectionPool object is garbage collected.
+        weakref_finalize(self, _close_pool_connections, pool)
+
     def _new_conn(self):
         """
         Return a fresh :class:`HTTPConnection`.
@@ -489,14 +506,8 @@
         # Disable access to the pool
         old_pool, self.pool = self.pool, None
 
-        try:
-            while True:
-                conn = old_pool.get(block=False)
-                if conn:
-                    conn.close()
-
-        except queue.Empty:
-            pass  # Done.
+        # Close all the HTTPConnections in the pool.
+        _close_pool_connections(old_pool)
 
     def is_same_host(self, url):
         """
@@ -1108,3 +1119,14 @@
     if host.startswith("[") and host.endswith("]"):
         host = host[1:-1]
     return host
+
+
+def _close_pool_connections(pool):
+    """Drains a queue of connections and closes each one."""
+    try:
+        while True:
+            conn = pool.get(block=False)
+            if conn:
+                conn.close()
+    except queue.Empty:
+        pass  # Done.
Only in /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/packages/backports: weakref_finalize.py
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/urllib3/poolmanager.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/poolmanager.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/urllib3/poolmanager.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/poolmanager.py	2023-07-15 11:52:27.582474434 -0400
@@ -171,7 +171,7 @@
     def __init__(self, num_pools=10, headers=None, **connection_pool_kw):
         RequestMethods.__init__(self, headers)
         self.connection_pool_kw = connection_pool_kw
-        self.pools = RecentlyUsedContainer(num_pools, dispose_func=lambda p: p.close())
+        self.pools = RecentlyUsedContainer(num_pools)
 
         # Locally set the pool classes and keys so other PoolManagers can
         # override them.
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/urllib3/_version.py /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/_version.py
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/urllib3/_version.py	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/urllib3/_version.py	2023-07-15 11:52:27.581474434 -0400
@@ -1,2 +1,2 @@
 # This file is protected via CODEOWNERS
-__version__ = "1.26.15"
+__version__ = "1.26.16"
diff --suppress-common-lines -u -r -x '*.pyc' -x installed-files.txt /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/vendor.txt /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/vendor.txt
--- /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages/pip/_vendor/vendor.txt	2023-07-14 23:27:47.000000000 -0400
+++ /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages/pip/_vendor/vendor.txt	2023-07-15 11:52:27.564474434 -0400
@@ -4,19 +4,19 @@
 distro==1.8.0
 msgpack==1.0.5
 packaging==21.3
-platformdirs==3.2.0
-pyparsing==3.0.9
+platformdirs==3.8.1
+pyparsing==3.1.0
 pyproject-hooks==1.0.0
-requests==2.28.2
-    certifi==2022.12.7
+requests==2.31.0
+    certifi==2023.5.7
     chardet==5.1.0
     idna==3.4
-    urllib3==1.26.15
-rich==13.3.3
-    pygments==2.14.0
-    typing_extensions==4.5.0
+    urllib3==1.26.16
+rich==13.4.2
+    pygments==2.15.1
+    typing_extensions==4.7.1
 resolvelib==1.0.1
-setuptools==67.7.2
+setuptools==68.0.0
 six==1.16.0
 tenacity==8.2.2
 tomli==2.0.1
Only in /tmp/tmp.SmLjYE4rJE/lib/python3.11/site-packages: pip-23.1.2.dist-info
Only in /tmp/tmp.z7QswUtaKk/lib/python3.11/site-packages: pip-23.2.dist-info
***** END DIFF
Untagged: localhost/devfileregistry:tmp
Deleted: 39402735533d6ad88247f5d40cfee30ecf08226c3e4ac78937a4907dce5da53b
Deleted: 7829e229f05c1b6d74272cafb257def6da57ab35e88a2340bf586a146b899318
Deleted: 79165f4ceed0787c46e4eebe69738676c7ae196ad3d04dc4d7c09d7f26cca20b
Deleted: 1fdbd620353ff83527ccaec16759ef429e72aaacd138c4cc11049f746eb7037b
Deleted: 2d71fa76da3d7b371b02b79972b7f494cd64f4b93eae55a7213c42cc24243f86
Deleted: 9865cb151bf162bf7a2af3b3b6f88296adc6bc7bd24f5f321be6426bee518d50
Deleted: a48ad889a41600046cec55bba89ea45e08eb84f838c8d9f76ea32d41ce5e466a
Deleted: 574f4409c8087f1dbdcbfb10fa5462007b44fe6bce2e01caf985bf18957f5db1
Deleted: 61bb38c17cf5b3b220fe81b86987a917758ca983d450bff024016147140b5d28
Deleted: 67329521057a7127ecdcca574042ed9d6143203997a688b1db4d439636e875d5
Deleted: f3c07c3e21883788369289b32ac12af2a9c910a75f302d448eca12ef3b346ea6
Deleted: a0384dbf96f27af52c1b7f58729087a8f1ca0c7080e2987b29e0c9124dbfc95a
Deleted: a38db4686543a5afe9c552e138910759dc352a1bd6296ec024a9b6eecbf53e63
Deleted: bd75cfad3553a50fd1c88c27d5670d5a40d7b8424246e922e4357c9bb8694d6e
Deleted: 84aad0a854cf687af7ea971f2472a867470f3ce6a68b27d477282cae693ebc6e
Deleted: 07a3b9a39c478d70998bd29ba9662130e4505256e259b00ee56e7ed72034ac8c
Deleted: 182e126974e79cdbd2a1a18e42e211a88781e263bcacc00be71569516c3ab319
Deleted: 1d766367ca1c8470fe79e641d6419b3ca1d98d60fca45a7d5e7300436be0a02a
Deleted: 77df0f7fc36e2e6a52ea2e1349ba21d0b9c58369d56869fdc0980ec0711f89e9
Deleted: 62a5670850b2a3c1949d6a013889fb6c53bf73e48f84267ebbbbe4df723588bb
Deleted: b8986c2fb8c4326aeee82531a811b288315a3a9b0bed80dc8d77c08683f4fcbc
Deleted: 3a8fd864b93a7e1a96912a07b0d6a56bcb21747840271223de5ac0a7db177bfa
Uploading: root-local.tgz
File already uploaded: resources.tgz
Source upload succeeded. Don't forget to commit the sources file
rm 'resources.tgz'
rm 'root-local.tgz'
